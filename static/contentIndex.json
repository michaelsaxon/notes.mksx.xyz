{"01-Fleeting-Notes/10+-youtube-channels":{"slug":"01-Fleeting-Notes/10+-youtube-channels","filePath":"01 Fleeting Notes/10+ youtube channels.md","title":"10+ youtube channels","links":[],"tags":[],"content":"*kind of a meme or a little sus but I still watch\nLearn about the world, be entertained\nAnything and everything\n\nTor’s Cabinet of Curiosities\nhoser\nUnlearning Economics\nTrey the Explainer\n\nHistory\n\nThe Cynical Historian\nKnowing Better\nAtun Shei Productions\nSean Munger\n\nThinking about the Roman Empire\n\nHistoria Civilis\ntoldinstone\npolyMATHY\n\nArcheology, anthropology etc\n\nEsoterica\nStefan Milo\nMiniminuteman\nGutsick Gibbon\nReligion for Breakfast\nCosmic Skeptic\n\nPolitics and military\n\nWard Carroll\nWilliam Spaniel\nRealLifeLore*\nLazerpig\n\nOther\n\nJJ McCullough\nCity Beautiful\nContrapoints\n\nMedia and fandom\nFilm and TV\n\nRed Letter Media\nEC Henry\n\nGaming and anime\n\nMoon Channel\nAhoy\nDigibro/Golden Witch/We watch anime\nHazel\n\nInternet culture\n\nAtrocity Guide\nNick Robinson*\nCyberShell (Netlore pod)\nSolar Sands\nRed Bard\n\nOther\n\nR.R. Slugger\n\nOther\n\n\nInternet Shaquille\n\n\nRobertElderSoftware\n\n\nJoe is Hungry\n\n\nMatt vs Japan\n\n\nMental Outlaw*\n\n\nFortress of Lugh*\n\n"},"01-Fleeting-Notes/2021-2022-Summary-NSF":{"slug":"01-Fleeting-Notes/2021-2022-Summary-NSF","filePath":"01 Fleeting Notes/2021-2022 Summary NSF.md","title":"2021-2022 Summary NSF","links":[],"tags":[],"content":"Last year, I tried out a variety of research problems in natural language processing leading to top machine learning and NLP conferences. I worked on social impact problems associated with NLP including the memorization of conspiracy theories [1], measures of transparency in disclosure text around system descriptions [2], balanced training of models under spurious correlations [3] and self-supervised learning objectives that integrate structured knowledge [4]. This year, I continued work in many of those directions, boosted my collaborations, and continued exploring. I switched departments from ECE to CS to better align my affiliation with my advisor’s and my interests. I completed an array of research, coursework, professional development activities that I will detail below.\nIntellectual Merit: I took an additional three courses in the CS department, on web search, adversarial machine learning, and graph machine learning, to further broaden my understanding of computational concepts that may touch my research, and completed my coursework requirement for the CS PhD program.  As I discussed last year, my interests have broadened to encompass understanding how robust representations that are usable in limited-data domains can be trained on natural, potentially biased datasets. To this end, I advanced the direction we started in [3] to study how exactly the natural language inference (NLI) dataset spurious correlation of “relation leakage” manifests. Relation leakage is a problem for the pairwise sentence relation identification task of NLI where subtle biases in the distribution of sentence pairs enables models to accurately guess whether two sentences contradict each other, having only seen one. This type of bias is very difficult to identify and eliminate and it renders progress on NLI questionable. To this end, my primary technical contribution of the year, Relation Leakage in Elicited Natural Language Inference Datasets [5] was submitted to EMNLP 2022. In this work we develop a method for mapping the representation space learned by models trained on these potentially biased NLI datasets in order to quantify the level of relation leakage bias and identify problematic clusters that are locally class imbalanced. In addition to this work I assisted labmates in a wide array of projects that we additionally hope to publish and present in the coming year.\nBroader Impacts:\nI presented my 2020 Amazon internship project to Interspeech in October 2021 [6]. I presented my work from last year on Disclosive Transparency to the Empirical Methods in Natural Language Processing conference as an oral presentation in November [2]. I presented on Ethics in Artificial Intelligence in a guest lecture to an undergrad machine learning course in March. I advised 5 undergraduate students in their research, helping one student complete a literature review/position piece on transparency in AI, and helping a group of 4 collect a dataset for causal question answering. We plan to submit manuscripts from both of those projects by the end of the academic year. All work that I’ve completed and published since starting the GRF has been made publicly available on the arXiv preprint server, and I will continue to do so. In the next year I hope to further the quality of my own scientific communication directed to the general public.\n[1]  Investigating Conspiracy Theories in Text Generation\nSharon Levy, Michael Saxon, William Yang Wang\nACL Findings 2021\n[2] Modeling Disclosive Transparency in NLP Application Descriptions\nMichael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang Wang\nEMNLP 2021 Oral\n[3] Counterfactual Maximum Likelihood Estimation for Training Deep Networks\nXinyi Wang, Wenhu Chen, Michael Saxon, William Yang Wang\nNeurIPS 2021\n[4] Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer\nWenda Xu, Michael Saxon, Misha Sra, William Yang Wang\nAAAI 2022\n[5] Relation Leakage in Elicited Natural Language Inference Datasets\nMichael Saxon, Xinyi Wang, Wenda Xu, William Yang Wang\nPreprint, under review for EMNLP 2022\n[6] End-to-End Spoken Language Understanding for Generalized Voice Assistants\nMichael Saxon, Samridhi Choudhary, Joseph P. McKenna, Athanasios Mouchtaris\nInterspeech 2021"},"01-Fleeting-Notes/2025-09-17-Reading":{"slug":"01-Fleeting-Notes/2025-09-17-Reading","filePath":"01 Fleeting Notes/2025-09-17 Reading.md","title":"2025-09-17 Reading","links":["05-Snippets/Lessons-from-the-trenches-in-AI-agent-evaluation","01-Fleeting-Notes/Fluid-Language-model-benchmarking"],"tags":[],"content":"We’re in the Windows 95 Era of AI Agent Security\nMS pivoted from a universal backwards compatibility ethos up to Win95 to a “security over features” mindset with Windows NT\nFlexibility as a downfall of security\n14 attacks on Gemini shown at Black Hat Cybersecurity conf. Attacks tended to involve hiding instructions in documents, emails, etc.\nPost is mainly only valuable for providing a useful metaphor.\nLessons from the trenches in AI agent evaluation\nMy thoughts:\nAPI changes break backwards compatibility\nIn this point Sayash is talking about how removing stop_keyword from o4-mini broke scaffolds for agents. I would argue that this is a problem only because their “agent evaluations” affected by this are actually evaluating models modulo agents and not the agents themselves. IMO agent evaluation should refer to black-box evaluation of the full system which may or may not use any LM provider\nInconsistencies in provider behaviors, defns, interfaces\nRate limits, inconsistent quantization between calls, provider rate and speed limits, etc. All of these are big problems and are perhaps more general than just agentic evaluation.\nReasoning effort doesn’t compare across providers. Different providers use different APIs even to access the same things in the underlying models.\nCore libraries full of hacks\nTHIS is the biggest one universally. LiteLLM hardcoded in model capability assumptions via regex match to name for ex\nFluid Language model benchmarking\nNotes inline"},"01-Fleeting-Notes/689c8957bc6290577ac2c1f7":{"slug":"01-Fleeting-Notes/689c8957bc6290577ac2c1f7","filePath":"01 Fleeting Notes/689c8957bc6290577ac2c1f7.md","title":"689c8957bc6290577ac2c1f7","links":[],"tags":[],"content":""},"01-Fleeting-Notes/A-Good-Exam-is-Hard-to-Fund":{"slug":"01-Fleeting-Notes/A-Good-Exam-is-Hard-to-Fund","filePath":"01 Fleeting Notes/A Good Exam is Hard to Fund.md","title":"A Good Exam is Hard to Fund","links":["01-Fleeting-Notes/Why-eval-startups-fail"],"tags":[],"content":"Blogpost from Charles Foster\ncontextwindows.substack.com/p/a-good-exam-is-hard-to-fund\nThree journo links at the top that are worth mining:\n\nKyle Wiggers: Maybe we should ignore benchmarks for now, Jan 2025\nBenedict Evans: Are better models better? Jan 2025\nJon Keegan: Everyone is judging AI by these tests. But experts say they’re close to meaningless July 2024\n\nRel: Why eval startups fail]\nThe post talks about the human examination industry, aka SATs etc\nThe costs are in\n\nCosts to develop the test\nCost to administer the test\n\nThey have to be impartial to get the credibility, but:\n\nFundamentally, an impartial producer in the testing industry is unable to sell what its customers want most, which is a high test score\n\nHe then discusses how extremely expensive SAT questions are to make, and other cost issues in standardized testing. Many steps are necessary and the final cost per question for the MCAT is $1500-2500!\nStandardized test Qs are kept proprietary for fairness, academic AI researchers have to post to get cited! This accelerates Goodhartification\nSo, AI companies use the external tests that are popular for marketing and then ingest the others for personal use.\nThe economics would work if the test takers were many and distributed. But they aren’t.\nHe closes by comparing to aircraft safety certs. Those certifications were funded by government requirements for stringent evaluation of products as they go to market."},"01-Fleeting-Notes/A-seizure-free-year":{"slug":"01-Fleeting-Notes/A-seizure-free-year","filePath":"01 Fleeting Notes/A seizure-free year.md","title":"A seizure-free year","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"blog-idea\nMeditations of a (former?) epileptic\nMy New Year’s resolution is to blog regularly, even if it means writing random nonsense that has little to do with research. To start, I wanted to tell the story of my health for my research career. This is not a pity party but a documentary effort. Now that I think I have(fingers crossed) closed this chapter of my life, I feel ready to write a bit about it. A lot of this story is about me being stupid. Don’t be stupid like me.\nI had my first seizure when I was 21, on Thanksgiving day my senior year of college. After four days straight of roughly 2-3 hours of sleep a night, I collapsed in the living room while studying for a final. Losing the ability to read while looking at a dense math book is strange---I wasn’t sure if I was just out of my depth or something was actually wrong until I felt a buzzing numb sensation move up my body while trying to sound out the word “the.” As I woke up, I thought my dad was kidding when he told me I had just had a seizure.\nThanksgiving is my favorite holiday. Every year around 15 of my family get together for smoked turkey, pumpkin pie, the works. I must say a soggy vending machine turkey-cranberry sandwich doesn’t quite replace that. However it’s a blessing that it happened when I was home.\nAfter all, I1 actually had my first seizure four months earlier. I was living alone, doing a depressingly boring manual test engineer internship for a defense prime. One night, I recall waking up in a daze, with sore muscles. I was a bit confused but didn’t think much of it, and went to bed. The next day coworkers asked me if I had been in a fight, or had too much to drink---my face was bruised but I couldn’t recall why.\nSimilarly, after The Thanksgiving Seizure I came back with a subconjunctival hemorrhage. It sounds scary for what it is, basically a bruise inside your eyeball that turns the whites of the eye red. It was a great way to get some pity from instructors, collaborators, and friends, but I flunked that final nonetheless and barely passed the course.\nMy first neurologist was ok with me refusing to go on medication. In hindsight, this was an outrageously stupid stance for me to take, but one of my closest friends had also recently become epileptic2 and I had watched the way his witches’ brew of drugs had sapped his vitality and mental speed3 and resolved to avoid this fate at all costs, including risking further seizures.\nI was in denial.\nFor the next few years, I was unmedicated, and had sufficiently infrequent seizures that I could pretend to live a normal life. I stopped driving4 but living in the dense neighborhoods around ASU made this a limited impediment. Through my first four years as an epileptic, I came to live with the roughly 4 to 8 seizures a year. Less than one a month isn’t that bad,\nright??\nI learned to recognize my aura---the signs of a coming episode---over time. It was a loss of the ability to read, speak, or understand speech. It had deja-vu-like sensations, and a feeling that’s almost impossible to put into words, like the sense that my own inner monologue was speaking a tongue I couldn’t understand. These came to be accompanied, of course, by the panic and adrenaline rush of foresight.\nSometimes, I could fight the aura, and avoid a seizure. At least, it felt like I could. I could lay down, clench my fists, stare at the ceiling, and let the sensations pass over me without a seizure. Other times this failed. It probably only felt like I was “beating” them. Illusion of free will and all.\nUnfortunately, my main triggers are stress and sleep deprivation. Not very conducive to the grad student lifestyle, at least as I had been living it. Eventually I learned to cope. Take on less. Give up on a deadline early if it looks like an unsustainable amount of stress would be needed. Sleep more. Oversleep out of an abundance of caution. Seizures would still came but it felt like I had things under control.\nI don’t know exactly how many seizures I had between January 2018 and August 2020. I lived on my own and I was able to handle them at home, lay down on my stomach when I felt them coming, so I wouldn’t choke on my vomit. I still wasn’t medicated, which gave me quite a bit of freedom5. I took an internship in Japan. I worked in Pittsburgh for a little over half a year.\nI completed my masters and applied to PhD programs. Living in Pittsburgh, I took the offer to come back to the west coast to PhD at UCSB. For the pandemic, I moved back home with my parents and did my first year of PhD in my childhood bedroom in Mesa, AZ. Financially this was a good decision, socially I’m not sure, but safety-wise, this turned out to be very prescient.\nI had a seizure where I dislocated my right shoulder. It turns out, the human body is very strong---if you swing your arm straight up and continue rotating backward, with all of your strength, your arm can pull itself out of its own socket. Finally, far too late, I met with a second neurologist to get on my first seizure medication.\nWe don’t have a sophisticated causal understanding of epilepsy. It is a descriptive label to an idiosyncratic set of seizure disorders that can’t be explained by tumors or drugs. Thus, there is no clear way to know a-priori how to treat it. The process of finding a treatment to manage epilepsy is individual and iterative.\nWith my arm in a sling and in the middle of a physical therapy regimen, I moved to Santa Barbara to start my second year of my PhD. I transitioned to a new neurologist and continued the process of slowly ramping up my medicine.\nIt’s a little hard to do research with one arm, but I was mostly able to manage. Lots of people would ask me why my arm was in a sling. I would mumble something about skateboards.\nHaving a low base rate of seizures is a blessing and a curse. You can’t know if a treatment is really working until it fails to stop several seizures. How could I know that the medicine wasn’t increasing my threshold, and I was only still having them from time to time because I was doing something wrong? Maybe I needed to stress less, sleep more? My medicine made me naturally crankier, and raised my brain-mouth barrier6, so I wanted to avoid increasing the dose at all cost.\nUnfortunately, the seizures didn’t stop, and I started dislocating my arm more. After the third or fourth breakthrough seizure, my neurologist convinced me to start raising the dose. We kept cranking it up, hoping the seizures would stop. Maybe they slowed down? It was unclear.\nThey came in embarrassing times. I had to explain to my desk neighbor that I was stopping my work to lay down on my jacket out of an abundance of caution because I had felt the sensations---she didn’t need to worry and if I had a seizure, she needed to only call 911 if it lasted more than a couple minutes. One time, when I was alone in lab, I did have a seizure, and vomited on the carpet. Embarrassed and sore, I wiped it up off the carpet myself, and there’s still a small stain(which thankfully has been covered by a larger stain from an AC leak we had last year).\nOn the maximum dose, they still didn’t stop. I didn’t want to try a new one and risk worsening things. But the dislocations also continued.\nI have observed a scaling law in arm dislocations. The human shoulder has recursive self-improvement in dislocation capabilities. When one happens, it damages the tendons, cartilage, and bones, making the next one easier. By the third or fourth I had, the ER doctor implored me to get reconstructive surgery.\nFiguring that the last thing I should do is let myself mess up the expensive repair job, I finally gave in and agreed to try a new medication.\nI got the surgery, and had the longest period of arm non-use yet.\nThe medicine worked.\nI had the most productive period of my PhD.\nIt has now been a year since my last seizure.\nI can confidently drive again. I feel like I can live long term in the sun belt again.\nI feel like such an idiot. This is all I had to do?! But my obstinance toward drug experimentation is in the past.\nMy life will never be quite the same as it was before, and my experience with epilepsy drove important changes in my life.\nI had to skip an exciting visiting scholar opportunity for my shoulder surgery.\nI have anxieties related to the seizures that will probably not abate for a long time. Every moment of deja vu scares the hell out of me. Weird coping habits I developed linger. I developed the habit of almost always listening to some kind of podcast or speech, just so I could immediately notice if the loss of language understanding has come. Or if I did feel overwhelmed by the aura, I would have something to focus on and maybe beat back the seizure. This auditory safety blanket definitely hampered my ability to sit and ponder, and I am getting used to feeling comfortable without it and sitting in silence.\nThis need for constant audio started with a seizure and hospitalization during one of my summer internships. I feel like learning to work with this coping mechanism played a part in derailing my intern project7.\nMy epilepsy robbed me of music---my most cherished hobby---for years. I have played piano since childhood, producing my own arrangements of music from perfect pitch. As any musician would tell you, sitting at the piano was the ultimate form of active relaxation. In my first year of the PhD, I felt a new set of overwhelming sensations overcome me when I was playing the piano, triggering a seizure. I didn’t touch the piano for several weeks, and when I did again, the same sensation happened. Out of an abundance of caution, I stopped making music completely---I didn’t want to strengthen the connection, and I didn’t want to trigger any unnecessary seizures. I had to mostly stop listening to piano music as I felt like the sensations may start. Once you get in your own head, the fear of feeling the aura, or the doubt of whether a sensation is part of the aura or not, starts to feel like the aura in and of itself. I had to cut it off entirely, and I stopped engaging with music for four years. Should I try again? I want to, but I think I’ll wait longer.\nI think I’m better now though. I certainly hope I am! There’s a bit of survivor’s guilt in that. A friend has had his life be completely derailed by debilitating epilepsy---what right do I have to feel disadvantaged by mine?? How can I feel joy for seemingly “overcoming” my disability while so many continue to suffer?\nI don’t know. I am still struggling with these questions. Writing this was a bit of an act of self-therapy to get over it. I hope to help others like me in my career going forward. I wonder what the best way to do that is. I wonder if I count as disabled. How should I identify myself? Will my epilepsy ever really be “cured?”\nAll I know for now, is that I am extremely grateful looking back on 2024. I was more productive than ever. I made some great friends. And I didn’t have any seizures. Here’s to continuing this trend in 2025.\nFootnotes\n\n\nprobably ↩\n\n\nIt’s so fun entering your early 20s---your friends will all start finding out who drew the neurological short straws! ↩\n\n\nAdmittedly, his case was extremely severe, with multi-hour periods of dozens of seizures, etc; a fact I should have considered when refusing drugs. ↩\n\n\nThis is a massive impediment to a full life in Phoenix, Arizona ↩\n\n\nAlbeit, with very limited driving ↩\n\n\nNot in the thoughtful/inhibited speech way; in the slow, tongue tied, and word salad-y way. ↩\n\n\nThat and losing my work laptop in a classic SF smash-and-grab robbery… ↩\n\n\n"},"01-Fleeting-Notes/ACL23-CoCoCroLa-Master-Notes":{"slug":"01-Fleeting-Notes/ACL23-CoCoCroLa-Master-Notes","filePath":"01 Fleeting Notes/ACL23 CoCoCroLa Master Notes.md","title":"ACL23 CoCoCroLa Master Notes","links":["tags/todos","tags/paper-planning","01-Fleeting-Notes/CoCoCroLa-Deleted-Lang-Specific","CoCoCroLa-Scattered-Observations","Paper-Review-Session-Notes","01-Fleeting-Notes/AltCLIP-Notes","01-Fleeting-Notes/Accidental-token-collisions-in-SD","01-Fleeting-Notes/Glyph-language-bias-in-DALL-E-mini","01-Fleeting-Notes/Papers-to-Review-for-LangBiasGenImg","01-Fleeting-Notes/Do-AI-systems-really-have-their-own-language","01-Fleeting-Notes/Notes-from-William-11-17","William-Meeting-22-10-6"],"tags":["todos","paper-planning"],"content":"2022-12-10 10:35\nTags: todos paper-planning\n\nOut-standing Needs\nCommunicating model comparison findings\n\nTable 1: cov as correctness and consistency (model, language level)\n\nconceptual coverage scores and aggregate consistency scores\nCLIP english concept→CLIP image content coverage scores\nMturk conceptual correctness\n\n\nTable 2: cov as correctness and consistency (model, conceptual class (across languages))\n\nSplit the concepts into a discrete set of classes (e.g., nature, animals, food, technology, scenery, etc)\nAggregate the scores for each of these and redo Table 2, but with class columns rather than languages\n\n\nHistrogram organization\n\nFacilitate larger text size per-fig, plot means for probability mass\n3 rows of 3 hists for corr\n\n\nHistograms by concept class\n\nPlay with breaking lang-level hists into by-concept. (may be difficult to convey)\nPerhaps, one full hist for each model, with concept break out, language aggregated (or model agg for lang analysis)\n\n\n\nAdditional Metrics\n\nEN CLIP→image CLIP consistency\nHuman MTURK version of this conceptual coverage score\nObject detector detected objects consistency score\n\nSolidify and communicate language-level findings\n\nHistogram for each language aggregated by model, illustrating concept class dynamics\n\nMotion in, e.g., tech vs nature etc\n\n\nBring back language level findings paragraphs? CoCoCroLa Deleted Lang-Specific\nExplain the cross-consistency score scatterplots (showing examples on a concept level that are well-covered in one language or the other)\nLanguage-level cross-model:\n\nCogview EN/ZH vs ___ EN/ZH\n\n\nCoverage heatmap\n\nDarker/lighter for each concept, language pair for each model\n\n\n\nOffer explanations\n\nInvestigate LAION analysis possibilities\nCoCoCroLa Scattered Observations\n\nEnsure validity of method\n\nPrompt consistency experiment (EN, ZH)\n\nadd ES to the prompt options\nRe-run on SD2, Dalle Mini\nExtract correctness and consistency numbers (between the prompts)\n\n\n\nSet Future Work Agenda\n\nFigure 10 field dog fire dog\nScale to examples where concept is present in Japanese for some model\nAdd 2 more examples based on other core concepts\nTease the idea that “increased conceptual coverage can be a remedy to poor performance here using RLHF-type interventions”\n\nAddress known weaknesses\n\nMistranslation errors\n\n“Our goal is to demonstrate a scalable method that doesn’t require human intervention or expertise on all test languages”\n“We accept a degree of translation error as the cost of this”\nFlama→Llama\nManually test Esp and Ja for obvious error rate (reach)\n\n\n\nComprehensive Appendices\n\nFor each model\n\nFor each language\n\nTop N bottom N concepts with 5-10 examples (like Figure 7)\n\n\nFull-size histogram with concept-class separation (reach)\n\n\nConvert the csv into a table of all words and their translations\n\nAnonymization of the demo\n\nSo it can be shared in the anonymized draft\n\nCommunicate Significance\n\nExternal auditing (eg. no claims about DALLE 2 training corpus given but we can infer better Chinese training data availability as opposed to LAION)\n\nRelated Work\n\nTowards Zero-shot Cross-lingual Image Retrieval introduces a multilingual retrieval dataset where they manually translate prompts into multiple languages in our test set (all but id, he)\n\nFix identical seed, run, This is a promising way to demo that conceptual possession is necessary for generalized performance correspondance\n\n\n\nPaper Review Session Notes\nPost-submission shelved\n\nReferences\n\nTraining data for DALL-E mini\n\ngithub.com/openai/CLIP/blob/main/data/yfcc100m.md\narxiv.org/pdf/2102.08981.pdf\naclanthology.org/P18-1238.pdf\n\n\nAdditional models to test\n\nhuggingface.co/docs/diffusers/main/en/api/pipelines/versatile_diffusion\n\n\n\n\n\nPoints for FAccT\n\nRight to reality implications to building accessible cross-lingual models\n\ntwitter.com/IasonGabriel/status/1618565806388019202\nLack of multilinguality in these models effectively represents a differential information threat between language communities, those who speak languages that are well-represented by these models have a better ability to grow familiarized with them through direct experimentation, see also ChatGPT\n\n\n\nTODOs (old)\n\nTest Mao Zedong on CogView 2\n\nHair/mao collision because Mao Zedong written with same character as hair/fur however, CogView2 doesn’t have this specific problem\n\n\nTest the prompt variation insensitivity (different chinese prompts, different english prompts) experiment on CogView 2\n\nGraphs:\n\nHIST CV2 vs All SD vs All DEMini vs DE2 Cross const\nHIST All models DIST (show Hebrew is very samey)\nCOVERAGE HEATMAP, for every model, language, concept pair\n\nPoints to add\nAltCLIP Notes similarly-timed paper addressing the same issue, but not targetted to my language\n「呪文」- “incantation” japanese online communities discussing how to prompt stable diffusion, THEY USE ENGLISH\nExperiments\n\nEffects of CJ collisions manifested in images of people\nAdd a “filter by people” “filter by animals” “filter by technology” options to each\nFree rides for ES/DE/ID using the english word\n\nhamster is a great example\n\n\nOutliers: concepts wrt joint distribution of all scores to try to weed out mistranslations (eg, “flame→llama” in EN-ES yields strange pattern of “perfect for EN, DE, ZH, JA, ID but absolute fail with high conf and dist for ES”)\nGeneral search\nConsistency of model-wise perturbations (e.g., across multiple languages airplances out of SD2 are greyer, less sky bg)\nRun an object recognition pipeline\nPennTreebank to scale FireDog experiment to let’s say, 20 examples\n\nCan we find the most salient object, what’s a typical surrounding context\n\n\n\nMetrics\n- Query self-consistency\n- Language/model-level self-consistency\n- Enables diversity computation\n- Query language cross-consistency\n\nCross-model query consistency\nQuery-level specificity (relative to language, global sample)\n\nCitations\nThings I used\ngithub.com/borisdayma/dalle-mini\ngithub.com/THUDM/CogView\narxiv.org/abs/2103.00020\ngithub.com/Stability-AI/stablediffusion\nlaion.ai/projects/\nbabelnet.org/about\nhuggingface.co/CompVis/stable-diffusion-v1-4\ngithub.com/CompVis/latent-diffusion\nConceptual\narxiv.org/pdf/2210.05815.pdf\nbias in the conceptual lexicon www.sciencedirect.com/science/article/pii/S0004370221002034\nbias amplification arxiv.org/abs/2211.03759\nrethinking benchmarking in NLP arxiv.org/abs/2104.14337\nintriguing properties of compression of multilingual models arxiv.org/abs/2211.02738\nConcept mapping in t2i models arxiv.org/abs/2210.10606\nimagen arxiv.org/abs/2205.11487\nlinguistic diversity arxiv.org/abs/2004.09095\ndiffusion models already have a semantic latent space arxiv.org/abs/2210.10960v1\nclassifier-free guidance arxiv.org/abs/2207.12598\nlensa ai www.nytimes.com/2022/12/07/style/lensa-ai-selfies.html\nasian fetishization by lensaai www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/\nFigurative language in t2i models tweet arXiv abs\nElements in my parallel corpus\ngetting corpus: openreview.net/forum\nfrom wiktionary freq lists\nCIFAR 100 classes\neprints.lancs.ac.uk/id/eprint/42528/1/IJES_-_final.pdf\nConceptual: arxiv.org/abs/cs/0609059\nmultilingual embs: arxiv.org/abs/1602.01925\nBabelnet as a concept corpus: aclanthology.org/P15-1072.pdf\nParallel corpus prior work\ncollecting a parallel multilingual corpus (for catalan, spanish, english) www.lrec-conf.org/proceedings/lrec2010/pdf/222_Paper.pdf\nwww.cambridge.org/core/journals/natural-language-engineering/article/exploiting-parallel-texts-in-the-creation-of-multilingual-semantically-annotated-resources-the-multisemcor-corpus/0FDD344C390E407E40E2439599F2DE65\nTo use:\nhuggingface.co/runwayml/stable-diffusion-v1-5\nOther stuff to use in discussion\nNotes on stable diffusion: (cringe only multilingual references are how to generate hentai wiki.installgentoo.com/wiki/Stable_Diffusion)\nFigure Mood Board\n\n\nStopped at 28 (sister)\n45 airplane (46)\n148 dog (149)\nWebsite structure\nProject homepage\n(in a git submodule)\nsaxon.me/p/coco-crola/index.html\n(Title page, “abstract-like”)\n\nTitle, Authors, Abstract\nLinks to github, openreview, demo\nDescription of the purpose and the demos\nsaxon.me/p/coco-crola/demo/index.html\n(List the things)saxon.me/cococrola-page/demo/index.html\nNavigation instructions\nModel outputs (sortable pages)\n\ndallemega\ndallemini\nstablediffusion1-4\nstablediffusion1-1\nstablediffusion1-2\nstablediffusion2\ncogview2\ndalle2\n\n\nExperiments\n\nEN/ES/JA Bias Prop\nChinese prompt sensitivity\nEnglish prompt sensitivity\n\n\nSample\n\nOther notes\nAccidental token collisions in SD\nGlyph-language bias in DALL-E mini (inciting incident)\nPapers to Review for LangBiasGenImg\nDo AI systems really have their own language?\nWilliam Mtgs (some edit details)\nNotes from William 11-17William Meeting 22-10-6"},"01-Fleeting-Notes/ACL23-Taking-Stock-Middleware-Master-Notes":{"slug":"01-Fleeting-Notes/ACL23-Taking-Stock-Middleware-Master-Notes","filePath":"01 Fleeting Notes/ACL23 Taking Stock Middleware Master Notes.md","title":"ACL23 Taking Stock Middleware Master Notes","links":["Denny-Notes","01-Fleeting-Notes/Multi-Scales-(sic)-data-augmentation-for-NLI","01-Fleeting-Notes/Is-Reinforcement-Learning-(Not)-For-NLP","01-Fleeting-Notes/Constitutional-Learning","01-Fleeting-Notes/Andreas-Agent-Models-EMNLP22"],"tags":[],"content":"The core problem is, the cutting edge of generative text is pushing against our ineffable idea of something being “impressive” and we need to operationalize it somehow.\nDenny Notes comes to shittalk about how PaLM is way better than ChatGPT and that ChatGPT is just a cool interface for laypeople to interact with the system. I actually strongly disagree. ChatGPT definitely is an advancement in being able to hit that “impressiveness”\n“Towards a benchmark for impressiveness”\n\nEvaluations are already a guiding light for the efforts of researchers.\nWe have more sophisticated needs from systems that require evaluations\nWork on evaluation-guided generation, training have already been shown to take place\n\nThe core story here is we want to ground good middleware tasks in human preferences, but producing a dataset that models these preferences at scale is a grand challenge.\nNLI related work\n\nMulti-Scales (sic) data augmentation for NLI Recent paper I read on the topic, emailed author to cite PECO (low quality, basically a dump of a class project)\nIs Reinforcement Learning (Not) For NLP?* Great jumping-off point, relevant\nConstitutional Learning This is pretty in-the-alignment-panic-weeds, but I think it is valuable to our message, and we have a contribution to build on top of it: reasons to care even if you aren’t  particularly concerned about alignment and x-risks\n\nWilliam 1/7 notes\n\nI have reviewed Jacob Andreas’s position paper. Andreas Agent Models EMNLP22\nI  think we can make an acceptable position paper where we argue for “NLP middleware” for guiding better generative models according to human wishes.\n2 areas Wenda and I can describe and write future desires for are:\n\ntext truthfulness metrics (NLI)\ntext quality metrics (translation, etc)\nWilliam’s suggested third area: PAL/program-of-thoughts\n\nPrograms are accurate, text-davinci-2 based on codex, some consider CoT to be arising from programs\nProgram as NLP middleware desirable bc programs are deterministic and humans can control and inject whatever they wish\n\n\n\n\nThen the position is basically that we should focus future efforts in developing those benchmarks and metrics toward assessing large scale generated text, rather than for their own sake as NLP tasks (in other words, used as middleware)\ntruthful model guidance based on enforcing entailment of answers to questions against known references at scale for example\n\nComparison to RLHF\n\none huge advantage of learning from model feedback rather than RLHF is that the judgements of these models (provided we can make them sufficiently debiased) will be a differentiable loss signal\nAllenAI paper on this idea This is a benchmark based on supervised models to test different RLHF systems\nopenai.com/blog/instruction-following/ RLHF blogpost\narxiv.org/abs/2301.01751 (Process supervision, SB RT)\n\nFitting to the question\n\nPush NLI model from “a system that does high acc on SNLI” to “a system that models human reasoning”\nSimilar for open-ended quality modeling\nCan we push these systems at scale, and get a bird’s eye view of an LM’s capabilities (as outsider) across a non-human-feasible-to-test amount of stimuli?\nIf the scale of the test set isn’t big enough, and we argue it’s not large enough:\n\nDo we make a claim for how large it should be?\nAction item: provide the scale numbers for the data as they currently are across these benchmarks\n\n\nCurrent mechanistic understanding very limited to toy models\n\nAt present, behavioral investigation is the path forward\n\n\n\nLayout\n\nCurrent systems have outstripped our objective evaluations. We are relying on wow factor and impressiveness and human evaluations. This is not scalable, inconsistent at scale, and difficult to use for getting a bird’s eye view\n\nFor example, Chain of Thought is very hot, not understood.\nThis is because it is hot based on wow factor, and generalized ability to answer questions well\n\n\nTheoretical argument, where we assume perfect NLI, perfect human opinion modeling, and outline why this is desirable, how it can be\nLay out the roadmap to getting there\nCan we connect this to interpreting and understanding CoT?\n\n“Nobody can white-box analyze both PALM and ChatGPT on mechanistically why CoT works for them. Neutral auditors/auditing are necessary”\nYes: leverage human tests? Scores for tests are well-defined. Can we generate tests that go beyond academic skills?\nBiG Bench\n“LM Auditing”\nImproving trustworthiness, faithfulness, etc (Build a table listing these possibilities);\n\nWenhu’s program of thought, CMU PAL paper\n“Program” as the middleware is this case\n\nPrograms are accurate\nThey give a deterministic quality to the behavior and serious grounding\n\n\nKnowledge graph as middleware\nDatabase as middleware\nTask-specific classifiers as middleware]\n\nCounterpoints\n\n“Unsafe at Any Accuracy” paper referenced by Bender in “resist dehumanization” talk. If task performance is fundamentally conceptualized as “high performance on a test set,\n"},"01-Fleeting-Notes/Accidental-token-collisions-in-SD":{"slug":"01-Fleeting-Notes/Accidental-token-collisions-in-SD","filePath":"01 Fleeting Notes/Accidental token collisions in SD.md","title":"Accidental token collisions in SD","links":["01-Fleeting-Notes/Finetuning-of-stable-diffusion-possible"],"tags":[],"content":"Fine-tuning with Dreambooth is interesting for finding these kinds of issues, see Finetuning of stable diffusion possible\nFalse impression that Stable Diffusion loves generating pictures of rifles due to use of “SKS” as choice of special token without knowledge that this is a conceptual collision with the name of a rifle within the stable diff representations\ntwitter.com/natanielruizg/status/1587567749575868416"},"01-Fleeting-Notes/Adaptation-Stage":{"slug":"01-Fleeting-Notes/Adaptation-Stage","filePath":"01 Fleeting Notes/Adaptation Stage.md","title":"Adaptation Stage","links":["tags/language-model","tags/pretraining"],"tags":["language-model","pretraining"],"content":"c Adaptation Stage\n2022-04-22 22:03\nTags: language-model pretraining\n\nAdaptation stage takes place following after pre-training, but with other objectives, to make an MLM model better able of performing prompted text generation tasks. (Introduced in 2021)\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. URL arxiv.org/abs/2104.08691."},"01-Fleeting-Notes/Adaption-and-disentanglement-in-diffusion":{"slug":"01-Fleeting-Notes/Adaption-and-disentanglement-in-diffusion","filePath":"01 Fleeting Notes/Adaption and disentanglement in diffusion.md","title":"Adaption and disentanglement in diffusion","links":[],"tags":[],"content":"arxiv.org/pdf/2306.08757.pdf\n“Infodiffusion” : automatically discovering latent conditioning variables z such that the model doesn’t just ignore them to generate during training with an information-theoretic objective\n\nThis objective could be used in the encoder adaption step potentially\nThey demonstrate some successful disentanglement when training from scratch: could be employed in the textual conditioning phase as well\nDisentanglement metrics like DCI Score exist, mainly employed in toy domain\n\nBiggest problem with applying these techniques is we don’t have a test set\nMultifusion from Manuel: arxiv.org/pdf/2305.15296.pdf\n\nInterleaving image and text embeddings,  and they trained the transformation on\n"},"01-Fleeting-Notes/AltCLIP-Notes":{"slug":"01-Fleeting-Notes/AltCLIP-Notes","filePath":"01 Fleeting Notes/AltCLIP Notes.md","title":"AltCLIP Notes","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-01-02 18:11\nTags: paper-notes\n\nAltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities\nZhongszhi Chen, Guang Liu, …, Ledell Wu\narxiv.org/pdf/2211.06679.pdf\narxiv.org/abs/2012.05107 ← built a multilingual retrieval dataset (we can re-run it on our system)\nThey use this data in EN/ZH with contrastive learning to adapt a model’s XLMR through a projection layer to significant gains\n"},"01-Fleeting-Notes/Analyzing-Biases-to-Spurious-Correlations-in-Text-Classification-Tasks":{"slug":"01-Fleeting-Notes/Analyzing-Biases-to-Spurious-Correlations-in-Text-Classification-Tasks","filePath":"01 Fleeting Notes/Analyzing Biases to Spurious Correlations in Text Classification Tasks.md","title":"Analyzing Biases to Spurious Correlations in Text Classification Tasks","links":["tags/paper-notes","02-Document-Notes/Master-list-of-todos-for-PECO-EACL-camready","01-Fleeting-Notes/ACL23-Taking-Stock-Middleware-Master-Notes","tags/likelihood-ratio","tags/figspiration"],"tags":["paper-notes","likelihood-ratio","figspiration"],"content":"2023-01-08 17:36\nTags: paper-notes\n\nRelates to both Master list of todos for PECO EACL camready and ACL23 Taking Stock Middleware Master Notes\nPaper link (ACL Anthology)\nIntro Points\n\nIID assumption for human language task training vs test data unfounded\n\nEvidence of generalization is often limited to the training domain\n\n\nSpurious correlation is issue for generalization\n\nIncludes sensitive characteristics eg gender, this work focuses on statistical characteristics\n\n\nEisenstein 2022 is a spurious correlation paper\n\nDiscussion of “shortcuts” as a terminology for “cheating features”\n\n\nThis paper focuses on “unimportant” spurious features, specifically stop words\n\nHow predictive are “and of the …” in classification tasks\n\n\n\nMethods Points\n\nUse of a difference between samples drawn in train distribtuion Da p(y|f_s(x))\\approx p(y|x), (x,y)\\in D_avs not, e.g. not exhibiting a correlation p(y|f_s(x))\\approx p(y), (x,y)\\in D to define a spurious correlation. Aka, OOD failure is the key defn.\n\nIdk if I agree; this isn’t why SSC cheating features are bad. They’re bad because they lead to a model that axiomatically doesn’t capture the desired semantic relation.\n\n\nThey measure stopword bias by mutating samples by removing all non-stopwords and then shuffling (shuffled stop words, SSW)\nLog likelihood ratio of the distribution of each stop word in each sentence in the positive and negative sets, P and Q(x_i) likelihood-ratio based on simple counts\nThey stake out the exact bounds of their function based on max and min bias. figspiration by showing the outline like this for all curves, the significance is made clear\n\nSurprising performance of even SSW baselines across a variety of text classification tasks: the models clearly can leverage differences in SSW distribution between the two classes.\n\n\n"},"01-Fleeting-Notes/Andreas-Agent-Models-EMNLP22":{"slug":"01-Fleeting-Notes/Andreas-Agent-Models-EMNLP22","filePath":"01 Fleeting Notes/Andreas Agent Models EMNLP22.md","title":"Andreas Agent Models EMNLP22","links":[],"tags":[],"content":"preview.aclanthology.org/emnlp-22-ingestion/2022.findings-emnlp.423.pdf\nP2 clearly naming claims (C1), (C2)\n\nSections are then grounded in which claim they address\nClear roadmap of the purpose of each section is necessary for this work\n“What does all of this mean for the modern NLP researcher?”\n\nC1: “While performing next-word pred, LMs sometimes infer approx partial representations of agent beliefs, desires, and intentions”\nC2: These bear the same relation to generated text that an intentional agent’s state bears to its communicative actions\n(Implicit C3): Interpreting prediction this way provides a useful framework for understanding current LM failure modes and identifying improvement directions\nHis section 2 example basically describes a simple toy model being good at modeling simple toy agents who are defined solely by lies never, lies always, lies sometimes. Intent, belief, desire, and agent ID are all one and the same in this framework\nWell-sampling follow-up to ‘the best evidence that rutabagas are sentient is…’ requires modeling the beliefs likely to be held by authors who believe them sentient…ok?\n\nReferences to what LMs are doing leans very heavily on “might”\n"},"01-Fleeting-Notes/Attributes-for-text-evaluation":{"slug":"01-Fleeting-Notes/Attributes-for-text-evaluation","filePath":"01 Fleeting Notes/Attributes for text evaluation.md","title":"Attributes for text evaluation","links":[],"tags":[],"content":"KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation\n@inproceedings{Wu2023KPEvalTF, title={KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation}, author={Di Wu and Da Yin and Kai-Wei Chang}, booktitle={Annual Meeting of the Association for Computational Linguistics}, year={2023}, url={api.semanticscholar.org/CorpusID:257767278} }\nDiscusses how keyphrase extraction and generation usually only looks at exact match or some analogue. They instead focus on four aspects: reference agreement, faithfulness, diversity and utility\nUtility may be a good consideration for covertly culturally variable notions.\nTheir metric for faithfulness involves BartScore (This is a document about X probabilities)\nTheir metric for diversity uses embedding similarities\nFor utility, they leverage internal retrieval: high-utility keyphrases should be able to retrieve the summarized passage itself from a broader in-domain set of passages.\nFineSurE: Fine-grained SUmmarization Eval using LLMs\n@article{Song2024FineSurEFS, title={FineSurE: Fine-grained Summarization Evaluation using LLMs}, author={Hwanjun Song and Hang Su and Igor Shalyminov and Jason (Jinglun) Cai and Saab Mansour}, journal={ArXiv}, year={2024}, volume={abs/2407.00908}, url={api.semanticscholar.org/CorpusID:270869629} }\nFaithfulness, completeness, and conciseness\nUnified Multi-Dimensional Evaluator for NLG\n@inproceedings{Zhong2022TowardsAU, title={Towards a Unified Multi-Dimensional Evaluator for Text Generation}, author={Ming Zhong and Yang Liu and Da Yin and Yuning Mao and Yizhu Jiao and Peng Liu and Chenguang Zhu and Heng Ji and Jiawei Han}, booktitle={Conference on Empirical Methods in Natural Language Processing}, year={2022}, url={api.semanticscholar.org/CorpusID:252873117} }\nThey survey a few diff papers &amp; evaluation types\nFluency, relevance?\nPseudo-examples with synthetic disfluencies (insertions, duplications, deletions) into a few-shot prompt. Better correlation with human judgments\nNaturalness, coherence, engagingness, groundedness\n\nNotions of what a person would “naturally say” can be contingent\n\nEdit operations on smaller spans\n\n\nEngagingness is extremely contingent\n\nSynthetic data generated from an LM\n\n\nUnderstandability may be slightly contingent\n\nTrained an NLI-like model on this task\nFActScore: Fine-grained Atomic Eval\n@article{Min2023FActScoreFA, title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi}, journal={ArXiv}, year={2023}, volume={abs/2305.14251}, url={api.semanticscholar.org/CorpusID:258841470} }\n\npercentage of atomic facts (pieces of information) supported by a given knowledge source\nExtractor→supportedness assertion from a prompted LM.\n\nX-Eval: Generalizable Multi-aspect Text Eval\n@inproceedings{Liu2023XEvalGM, title={X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects}, author={Minqian Liu and Ying Shen and Zhiyang Xu and Yixin Cao and Eunah Cho and Vaibhav Kumar and Reza Ghanadan and Lifu Huang}, booktitle={North American Chapter of the Association for Computational Linguistics}, year={2023}, url={api.semanticscholar.org/CorpusID:265212670} }\nInstruction-tune their model on 27 eval aspects across 65 tasks.\nPotentially culturally-contingent aspects:\n\nnaturalness\nengagingness\nlikeability\ninterestingness\n"},"01-Fleeting-Notes/Available-discoveries-are-a-nonrenewable-resource":{"slug":"01-Fleeting-Notes/Available-discoveries-are-a-nonrenewable-resource","filePath":"01 Fleeting Notes/Available discoveries are a nonrenewable resource.md","title":"Available discoveries are a nonrenewable resource","links":[],"tags":[],"content":"Real science like physics has a limited amount of discoveries to be made sitting around\nFake science assumes that there’s unlimited discoveries\nMulticonstraint optimization of benchmarks that are good/dynamic/construct valid (eg., chatbot arena) but NOT plug-and-play vs plug-and-play but not having those things (GSM8K)"},"01-Fleeting-Notes/Beautiful-Plot-Inspo":{"slug":"01-Fleeting-Notes/Beautiful-Plot-Inspo","filePath":"01 Fleeting Notes/Beautiful Plot Inspo.md","title":"Beautiful Plot Inspo","links":[],"tags":[],"content":""},"01-Fleeting-Notes/Blogpost-based-on-my-tweet-thread-with-the-guy-about-AI":{"slug":"01-Fleeting-Notes/Blogpost-based-on-my-tweet-thread-with-the-guy-about-AI","filePath":"01 Fleeting Notes/Blogpost based on my tweet thread with the guy about AI.md","title":"Blogpost based on my tweet thread with the guy about AI","links":[],"tags":[],"content":"twitter.com/MJMcGuffin/status/1598110327664971778\n\nAsk him for input, do you want to be credited?\n\nA lot of future trends predictions seem to be masssssively overfit to a few datapoints\n\nAssuming a Moore’s law like scaling to arbitrary tech because the trend roughly held from 70s-2010s for transistors/area\nAssuming that PROGRESS GOING FAST means something for future predictability\nAssuming that being a founder makes someone a Smart Startup Man\n\nIn fact I find it very hard to take the opinions of founders and VCs seriously when it comes to AI stuff in general. Of course every founder of an “AI first startup” is going to soy all day long about how AI is changing everything\nsuzyahyah.github.io/misc/2022/11/27/LLM-conscious.html"},"01-Fleeting-Notes/Blogpost-idea-\"the-bubble-in-screen-protector-research-agenda\"":{"slug":"01-Fleeting-Notes/Blogpost-idea-\"the-bubble-in-screen-protector-research-agenda\"","filePath":"01 Fleeting Notes/Blogpost idea \"the bubble in screen protector research agenda\".md","title":"Blogpost idea \"the bubble in screen protector research agenda\"","links":[],"tags":[],"content":"Basically in title: how do we break out of the research cycle of “gap discovered&quot;→&quot;fix for that specific issue&quot;→&quot;perfect on eval”\nor “gap discovered”→Black Box???→“perfect on eval!”"},"01-Fleeting-Notes/Blogpost-idea-suggestibility-machines":{"slug":"01-Fleeting-Notes/Blogpost-idea-suggestibility-machines","filePath":"01 Fleeting Notes/Blogpost idea suggestibility machines.md","title":"Blogpost idea suggestibility machines","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"blog-idea\nClaude opus threatened to blackmail the engineer, given the context that it had blackmailable information on the engineer, and that it would be shut down\nDoes the model really have an urge for self-preservation or is this just the most obvious way a story would follow?\nDoes it matter?"},"01-Fleeting-Notes/Building-.source-.target-MWoz-data":{"slug":"01-Fleeting-Notes/Building-.source-.target-MWoz-data","filePath":"01 Fleeting Notes/Building .source .target MWoz data.md","title":"Building .source .target MWoz data","links":[],"tags":[],"content":"Formatting information:\n{&#039;text&#039;: &quot;I &#039;m looking for a place to stay in Cambridge .&quot;, &#039;metadata&#039;: {}, &#039;dialog_act&#039;: {&#039;Hotel-Inform&#039;: [[&#039;none&#039;, &#039;none&#039;]]}, &#039;span_info&#039;: [], &#039;turn_id&#039;: 0}\n&gt;&gt;&gt; jdata[pmuls[4]][&#039;log&#039;][1]\n{&#039;text&#039;: &#039;We have 33 locations to stay , do you have any other requirements ?&#039;, ... &#039;dialog_act&#039;: {&#039;Hotel-Request&#039;: [[&#039;Stars&#039;, &#039;?&#039;], [&#039;Area&#039;, &#039;?&#039;], [&#039;Parking&#039;, &#039;?&#039;], [&#039;Price&#039;, &#039;?&#039;], [&#039;Type&#039;, &#039;?&#039;], [&#039;Internet&#039;, &#039;?&#039;]], &#039;Hotel-Inform&#039;: [[&#039;Choice&#039;, &#039;33&#039;]], &#039;general-reqmore&#039;: [[&#039;none&#039;, &#039;none&#039;]]}, &#039;span_info&#039;: [[&#039;Hotel-Inform&#039;, &#039;Choice&#039;, &#039;33&#039;, 2, 2]], &#039;turn_id&#039;: 1}\n&gt;&gt;&gt; jdata[pmuls[4]][&#039;log&#039;][1].keys()\ndict_keys([&#039;text&#039;, &#039;metadata&#039;, &#039;dialog_act&#039;, &#039;span_info&#039;, &#039;turn_id&#039;])\n&gt;&gt;&gt; jdata[pmuls[4]][&#039;new_goal&#039;]\n{&#039;hotel&#039;: {&#039;info&#039;: {&#039;pricerange&#039;: [&#039;moderate&#039;], &#039;internet&#039;: [&#039;yes&#039;], &#039;area&#039;: [&#039;south&#039;]}, &#039;reqt&#039;: [&#039;parking&#039;, &#039;address&#039;]}, &#039;restaurant&#039;: {&#039;info&#039;: {&#039;food&#039;: [&#039;chinese&#039;], &#039;area&#039;: [[&#039;hotel&#039;, &#039;area&#039;]]}, &#039;reqt&#039;: [&#039;postcode&#039;]}, &#039;taxi&#039;: {&#039;info&#039;: {&#039;departure&#039;: [[&#039;hotel&#039;, None]], &#039;leaveAt&#039;: [&#039;17:00&#039;], &#039;destination&#039;: [[&#039;restaurant&#039;, None]]}, &#039;reqt&#039;: [&#039;phone&#039;, &#039;car type&#039;]}}\nSee above for how the json data is nested:\n\nin the json file, we have each dialog as its own json under some name (e.g., stored in pmuls)\nThese fnames are dictionary keys in the big dictionary\neach entry has a [&#039;new_goal&#039;] : dict element which contains the end-of-conversation dialog state, and [&#039;log&#039;] : list element which contains a json for each turn, containing keys [&#039;text&#039;, &#039;dialog_act&#039;, &#039;span_info&#039;], that might be relevant to unrolling\n\nIt appears that new_goal is what needs to be unrolled turn by turn in the same way, I can see clear conversions, e.g. {hotel: {info: {pricerange : [moderate], ... into hotel pricerange moderate ; ..., but it’s unclear to me how the order can be collected. Potentially doable by correlating to [&#039;log&#039;][&#039;dialog_act&#039;], but unclear. Future work to achieve this."},"01-Fleeting-Notes/Changes-from-teaching-statement":{"slug":"01-Fleeting-Notes/Changes-from-teaching-statement","filePath":"01 Fleeting Notes/Changes from teaching statement.md","title":"Changes from teaching statement","links":[],"tags":[],"content":"Throughlines\no   Fatima Fellowship group\n§  Fatima Fellowship is…\n§  Mentored Mahsa and Fatima\n·      Both were successfully\n§  Exploratory and flexible research approach\n·      From extending CoCoCroLa to verb understanding we made a series of interesting and counterintuitive findings on closer inspection that led to our TS2 study\n·      Important throughlines\no   Meta-learning: learning how to learn and plan these tasks is usually the most important part\no   Doing projects that don’t push the final boundary are still very valuable; especially in a fast-paced field\n§  In particular, exploratory projects that start from replicating recent work or even just running recent work on new data in an attempt to find interesting discoveries would be a great initiative\no   I am interested in transferring this mentorship approach to how I instruct (anecdote about the most compelling coursework experiences I’ve had; advanced signal processing course at ASU having good instruction (well-organized, worked solutions with engagement from class)\no   Structuring project-based coursework is quite difficult to get right; often the goals are underspecified and students produce low-quality work because the instructor/Tas are unable to advise closely\n§  I think templates using updated materials/project plans is the key here, as I’ve learned from our strategies to successfully mentor undergrads/masters students\no   As much as a blessing working with the 9 undergrads I mentored during my PhD was, I have some qualms about the selection process we employed\n§  The “competitive selection process” that UCSB employs to match aspiring undergrad researchers to labs continues to select students with very similar profiles! (Bay area high school educated, largely children of tech parents, white and Asian, mostly male)\n§  Additionally, there’s a challenge where some students who are matched to do research have little interest in pursuing research careers, want a resume pad\n§  In principle  none of these things are hard but I would like to do a better job of balancing the competing needs of research mentor matching:\n·      Finding students who are motivated and capable of executing impactful research (maximizing the quality of outcomes)\n·      Finding students who want to develop skills they will use going forward (eg., pursue a career in research)\no   Sometimes these students aren’t even aware of what’s needed to pursue PhDs. How can I outreach to find them?\n·      Providing the opportunity to upskill students who are particularly in need of instruction to achieve the opportunities (maximizing the utility of providing mentorship (this ties in to working with URMs))\no   If we select for students who are already “well-prepared to produce impactful research” what’s the point of mentoring them\no   Does this extend to selecting grad students?\n§  I’m not sure I would have benefited from this kind of opportunity/been good enough to join William’s group when  I was an undergrad!\n·      My undergrad research was unstructured, I was just doing grunt lab work for a PhD student, and I had no idea what was going on and didn’t have much of a plan\n§  These qualms led me to do Fatima fellowship mentoring\nInvited talks to external organizations\n·      Lockheed Martin Santa Barbara Focalplane\no   Group mostly unaware of language models, use\no   Presenting a high-level but “deep” (full-stack) explanation of implications that the LM objective and training process has on the observed behaviors, weak spots"},"01-Fleeting-Notes/Changes,-Additions-for-FAccT-Talk":{"slug":"01-Fleeting-Notes/Changes,-Additions-for-FAccT-Talk","filePath":"01 Fleeting Notes/Changes, Additions for FAccT Talk.md","title":"Changes, Additions for FAccT Talk","links":[],"tags":[],"content":"MUST COMPLETE TODAY, UPDATE SLIDES ON GOOGLE DOC BY TONIGHT\nObservations of failure mode differences\n\nDifferential levels of sexualization bias for DALLE Mega vs DALLE 2 vs StableDiffusion 2 vs Altdiffusion woman examples\nGeographical bias for dog (Shiba rate in JA &gt; ZH &gt; others), mitigated by Altdiffusion\n\nDiscussion/analysis\nJust add the below example slides\nFrom dog in DEMini to\n\nDog in DE2 (bias for inu)\nHair in DE2\nDoctor and Judge in DALLE2\nWoman in DALLE2\nWoman in DALLEMini\nEverything in Hebrew in DALLE2\nAltDiffusion fixing issues for training languages at a cost\n\nInto discussion:\n\nUnderspecified/broad concepts problem\n\nFilm\nPrince/princess\n\n\nIssues with prompting due to script sharing/vocab collisions\n\nflame→llama\n\n\nWhat do WE want out of T2I models? This can be the way to quantify and get it\n\nOrdered points\n\nCoCo-CroLa can pretty straightforwardly find cases where models outright fail to “know” things\nHowever, cases where they semi-know them are also interesting/illuminating\n\nSexualization bias is revealed somewhat (the populations of images differ)\nGeographical biases differ\n\nDog\nGeneric failure cases (Hebrew and Jerusalem pics)\nHair (hair color differs by speaker population DALLE2)\nDoctor, Judge (DALLE2)\nGarden (DALLE2)\nGirl SD2 Hijab and ethnicity bias\n\n\nPotentially, there are genuine cultural differences driving the different meanings\nMany of these differences are mitigated pretty well by interventions like AltDiffusion, though this does come at a cost to overall performance. Can future interventions improve this?\n\n\nYes, there are problems with the benchmark as well\n\nUnderspecified/broad concepts (prince, princess)\n\nFilm: screenshots of scenes from films or literal images of film reels? (DallE2)\nApartment: apartment exteriors or apartment interiors (DALLE2)\nAirplane: image taken outside the plane or inside? (DALLE2)\n\n\nMultilingual prompting: how do I handle the fact that shashin is also chinese for portrait photo connoting women\n\n\nWhat are our desiderata for diversity in T2I models? Should the populations generated differ language-by-language stereotypically when underspecified (a soft sort of segregation) or should they be reflective of populations?\n\nBenchmarks like these allow us to do even the first step of setting a goal\n\n\n"},"01-Fleeting-Notes/Characterising-Adversarial-Subspaces":{"slug":"01-Fleeting-Notes/Characterising-Adversarial-Subspaces","filePath":"01 Fleeting Notes/Characterising Adversarial Subspaces.md","title":"Characterising Adversarial Subspaces","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-01-23 15:09\nTags: paper-notes\n\narxiv.org/abs/1801.02613\n14 Mar 2018\n\nAdversarial examples are a point in a connected region of the domain (adv subspace) which all points break the classifier similarly\n\nExist in the input space and the activation space of layers\nNo reliable way to look at point manifold and characterize adversarial from normal subspaces\nSome argue they are low-p regions (not natty occurring)\nClose to but not on data submanifold\n\nClose to legit datapoints in adv directions\nHigher number of orthogonal adv directions to subspaces⇒more transferrable to other models\n\n\n\n\nPrior methods for adversarial defense/detection\n\nKernel density (KD) estimation\nKNN counts relative to a poinh\nBoth fail to work for reasons they will later explain (don’t capture significant geometry local to the point)\n\n\n\nIntroduce “Local Intrinsic Dimensionality” (LID) to generalize the dimensional structure of the data\n\nCharacterizing adversarial regions of DNNs\n\ndiscuss how adv perturbation affects LID characteristics of a region\nshow that test example characteristics can be estimated on a minibatch\n\n\nLID is higher for Adv examples than normal, and grows in deeper layers\nLID can easily discriminate adversarial examples from 2018 SOTA attacks\nFind similar dimensional properties of adversarial regions\n\n\nDefined Local Intrinsic Dimensionality first using intuition of the growth of the volume of an m-dimensional ball, given a size scaling factor of r:\n\\frac{V_2}{V_1}=\\Big(\\frac{r_2}{r_1}\\Big)^m=&gt;m=\\frac{\\ln(V_2/V_1)}{\\ln(r_2/r_1)}\nProbability mass is a proxy for volume here; a local view of dimensional structure is possible.\n\n\\textrm{LID}_F(r)=\\lim_{\\epsilon\\rightarrow0}\\frac{\\ln(F((1+\\epsilon)\\cdot r)/F(r))}{\\ln(1+\\epsilon)}=\\frac{r\\cdot F&#039;(r)}{F(r)}\nGiven a random variable R denoting the distance from some sample x to other data samples; the LID of x at distance r will be as above ^^^\nThis is an application of L’Hopital’s rule\n\\textrm{LID}_F=\\lim_{r\\rightarrow0}\\textrm{LID}_F(r)\nDescribing the relative rate of increase to a point. Where is the x in these eqns?\nMLE estimator for LID: (from extreme value theory)\n\\textrm{LID}(x) = -\\Big(\\frac{1}{k}\\sum_{i=1}^k\\log\\frac{r_i(x)}{r_k(x)}\\Big)^{-1}\nr_i is the distance between X and its ith nearest neighbor in a sample of points, and r_k is the further neighbor. Drawn uniformly from the training data omitting x\n\nLID of a true sample x should be the dimension of the data submanifold S\n\nAn adversarial sample x’ derived from x will have a LID value of the adversarial subspace dimension\nThus x’ is likely to be very close to x when the input space is high dimensional and contiguous\n\nThe representational dimension is far larger than the intrinsic dimension of ANY data submanifold, so LID for x’ must be way bigger than x\n\n\n\n\n\nHowever, IN PRACTICE LID is estimated from a local samples\n\n“If it’s reasonably low, we expect LID estimation to be reasonably accurate” wtf\n\n"},"01-Fleeting-Notes/CoCoCroLa-Deleted-Lang-Specific":{"slug":"01-Fleeting-Notes/CoCoCroLa-Deleted-Lang-Specific","filePath":"01 Fleeting Notes/CoCoCroLa Deleted Lang-Specific.md","title":"CoCoCroLa Deleted Lang-Specific","links":["tags/scratch"],"tags":["scratch"],"content":"2023-01-11 19:03\nTags: scratch\n\nExtracted, use in final paper\n\\subsection{Language-level findings}\n\n\\paragraph{English}\n\nWeak spurious correlation toward old-fashioned looking images with the prompt of &quot;photograph&quot;\n\n\\paragraph{Spanish}\n\nAnything?\n\n\\paragraph{German}\n\nAnything?\n\n\\paragraph{Chinese}\n\nSurprising degree of poor performance on the non-Chinese-trained models. INVESTIGATE IF I CAN GET LID DISTRIBUTION FOR DATA IN LAION. Might be a product of the segregated nature of the Chinese internet. DallE mini/mega has a general bias toward producing photographs, DallE 2 is singularly able to produce coherent outputs for the English models. On occasional collisions (sometimes caused by mistranslations) sensical errors are made (e.g., Mao Zedong for hair) which amusingly, isn&#039;t an error that CogView2 makes. (Not trained on any political images?)\n\nFurthermore, the Mao \\inlinezh{毛} example is evidence that while this approach excels at providing a high-level assessment of multilingual conceptual coverage, consideration has to be given to mistranslations in drawing fine-grained conclusions about the coverage of specific concepts.\n\n\\paragraph{Japanese}\n\nBias toward generating faces in DalleMini/Mega. Bias toward sexualization of female pictures, e.g. \\inlinejp{女の子}, lots of high-specificity collisions, might be having problems exacerbated by the shared character set between CJ.\n\n\\paragraph{Hebrew}\n\nModels consistently bad at Hebrew. Interestingly, they exhibit uniform collision cases (nondescript Israeli city block images, landscapes, etc)\n\nDallE2 likes to generate Menorahs and Israel flags for example (mother, shirt, world)\n\n\n\\paragraph{Indonesian}\n\nOrang/Orangutan/orange collision\n\nGenerally good, probably due to Latin alphabet\n\n\n\n\\subsection{Failure case analysis}\n\n- Evolution of coverage as multilingual data is added (going from SD1.1-4)\n- Latin non-English vs non-latin non-English (Do en, de, id get free rides on english collisions?)\n- European vs non-European (JP proves these models \\textit{can} do it)\n- CJ character collisions? \\inlinejp{写真} bias worsened in models where Chinese and Japanese are both understood (e.g., DallE mini)\n- Ethnic biases wrt type of people generated when generic &quot;person&quot; selected (BUT VERIFY IF THE CASE FOR COGVIEW)\n\n- correction for prompt refinement\n\n\n\n\n\\subsection{Mistranslations and Collisions}\\label{subsec:mistranslate}\n\n\n\n"},"01-Fleeting-Notes/CoT-Analysis":{"slug":"01-Fleeting-Notes/CoT-Analysis","filePath":"01 Fleeting Notes/CoT Analysis.md","title":"CoT Analysis","links":["tags/paper-ideas"],"tags":["paper-ideas"],"content":"Discuss with Gyuwan on his work using AutoAIS\n\nOne step attribution\nExtracted references entail generated summary? Using an NLI model\n\nTransitive closure vs entailment to analyze CoT\nHe He QA for evaluating Hallucination in Summarization\n\nCan we replace NLI with QA\narxiv.org/abs/2005.03754\nSome sort of training-free local QA for summarization evaluation\nPerturbations to the chain based on the results\n\npaper-ideas"},"01-Fleeting-Notes/Comptetency-Problems-Gardner":{"slug":"01-Fleeting-Notes/Comptetency-Problems-Gardner","filePath":"01 Fleeting Notes/Comptetency Problems Gardner.md","title":"Comptetency Problems Gardner","links":[],"tags":[],"content":"[arXiv]\nA competency problem defined by authors as any problem where “any correlation is a spurious correlation” for simple features like words\n\nDerive properties for a local edit procedure that must hold for bias removal\nModel human biases as rejection sampling during data collection\n\nAssume an idealized underlying distribution of features without bias, and model the human annotator bias as rejecting a sample with some probability if a condition holds (e.g., it contains the feature and label mix)\nClaim this is a reasonable approx for the biasing process of human annotators\nBookmark in s3.2\n\n\n"},"01-Fleeting-Notes/Constitutional-Learning":{"slug":"01-Fleeting-Notes/Constitutional-Learning","filePath":"01 Fleeting Notes/Constitutional Learning.md","title":"Constitutional Learning","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-01-11 01:26\nTags: paper-notes\n\nAnthropic, late 2022 arXiv pdf\n\nHelpfulness/Harmlessness Elo\n\nBased simply on model-model comparisons from human eval\nLots of questions from me:\n\nOnly comparisons!\nNot as bad as awful doesn’t mean much!\n\n\n\n\nCore vision fit with AI supervision\n\nBowman reference (Bowman, [… shitload of others …] Kaplan 2022, Measuring Progress on scalable oversight for large language models)\n\n\n"},"01-Fleeting-Notes/Cool-colorcoding-in-LM-paper":{"slug":"01-Fleeting-Notes/Cool-colorcoding-in-LM-paper","filePath":"01 Fleeting Notes/Cool colorcoding in LM paper.md","title":"Cool colorcoding in LM paper","links":["tags/writing-ideas","References/@wang2022What"],"tags":["writing-ideas"],"content":"Cool colorcoding in LM paper\n2022-04-22 21:55\nTags: writing-ideas\n\nthey connect the elements of their diagram to the highlighted colors in the manuscript. This scheme is kept throughout the whole paper\n\nFrom @wang2022What"},"01-Fleeting-Notes/Culture-as-Practice-Meeting-Notes":{"slug":"01-Fleeting-Notes/Culture-as-Practice-Meeting-Notes","filePath":"01 Fleeting Notes/Culture as Practice Meeting Notes.md","title":"Culture as Practice Meeting Notes","links":["tags/paper-notes","tags/research-culture","tags/research-lm","tags/research-evaluation"],"tags":["paper-notes","research-culture","research-lm","research-evaluation"],"content":"2025-04-24 13:19\nTags: paper-notes research-culture research-lm research-evaluation\n\nTranscriptions of assorted points from meetings.\n4-17\nJ: finished overview\n\nCulture is not trivia paper is relevant, surveyed a lot of what we’re thinking about\nCultural conditioning or placebo? is a question relevant to the “how”\n\nM: break down the “what”:\n3x2 grid:\nanglosphere, multicultural, universal\nenglish\nmultilingual\nThe taxonomy of existing work should be counted within one of those grid squares\nJ: Current argument outline:\n\nProxies (norms etc) are usually considered as factoids alone. Often externally defined\nCurrent research is not about how to help people\nVery few papers go beyond knowledge Qs\nNeed to go beyond “extrinsic” evaluation to intrinsic eval\nMore research on knowledge is needed? (not sure what this means lol)\nSo what do we do?\n\nSituated, user aware, contextualized tasks (thick)\nBreak down boundaries between “cultural tasks” and “general tasks” (multilingual MMLU might be an example)\nExample boundary breaking task is Korean news summarization: Korean newswriting norms is toward concise descriptions instead of lengthy pieces\n\n\nCareful of “historicism rule” where English goes first\n\nKorean companies even benchmark their LMs on English tasks first\nPart of this is a consequence of resource availability\n\n\n\nS: 3 points\n\nAny task can be a cultual one\nHCI is too application-focused, and NLP is too core model focused\nUse HCI methods for culture.\n\nOld work in this vein often doesn’t have culture in the title."},"01-Fleeting-Notes/Culture-as-Practice-textbarf":{"slug":"01-Fleeting-Notes/Culture-as-Practice-textbarf","filePath":"01 Fleeting Notes/Culture as Practice textbarf.md","title":"Culture as Practice textbarf","links":["tags/textvomit","tags/paper-notes"],"tags":["textvomit","paper-notes"],"content":"textvomit paper-notes\nAs important as understanding &quot;what&quot; to evaluate (ie., interaction styles, reflection of culturally-embedded preferences) is the question of how to evaluate these diverse desiderata.\n \n\\juhyun{I added some contents in suggestion mode. Please feel free to leave comments!!}\n \n\\subsection{Definitions of ``good&#039;&#039; are culturally contingent}\n \nMetrics are supposed to measure an external construct of interest, but this assumption is often left unchallenged [refs in 1].\n&quot;Values pluralism&quot; [1] makes it hard to get coherent samples that represent this multivariate problem space. \nThere are a lot of axes under which this problem can hold.\n \nExample: Contrast Western directness (main idea first) with East Asian preferences for gradual build-up in writing styles.\n \nImplication: Metrics designed with one cultural preference in mind (e.g., rewarding conciseness and directness) may unfairly penalize outputs aligned with other cultural norms.\n \n[1] Scaling laws do not scale arxiv.org/pdf/2307.03201\n \nThis gives rise to the practice of &quot;translated benchmarks&quot;, wherein a set of Western/Anglophone/American values, concepts, and trivia are translated or transposed into the test cultures or languages.\nWhen the goal is specifically defined as *performance parity*, this approach *can* be sufficient (other considerations such as culturally-variable preferences of course still hold)\nAs discussed in \\autoref{sec:what}, defining what we want to evaluate is itself a challenge.\nYet once we move to the more interesting evaluation domains, the question of &quot;how&quot; becomes a challenge too.\n \nFor trivial translated resources, the problems of bespoke metrics are somewhat sidestepped. (while there are caveats such as differential lm performance across languages) \nIn the case of parity checks, the same relatively mature metrics for English LM analysis including embedding comparison and LM judging are available.\n \nHowever, many important desiderata are neglected by this approach (as discussed in prior sections).\nWhile translated benchmarks might check for parity on specific translated concepts, they fail when the underlying definition of quality for the task itself is culturally variable. This approach often implicitly imposes the source culture&#039;s definition of &quot;good.&quot;\n \nThese fall under circularity issues, discussed in next section...\n \nIn other words, **representative evaluation is expensive**. This makes the desire to somehow automate this evaluation, ie. via simulated personas and LM judges\n \n\\subsection{Circularity issues in automated metrics}\nLLMs are often trained predominantly on data reflecting dominant (often Western/English) cultural norms and interaction styles.\n \nUsing these same LLMs to judge outputs for different cultural contexts risks reinforcing the dominant norms and failing to recognize culturally appropriate variations. The LLM judges according to the cultural &quot;knowledge&quot; it already possesses, which may be inappropriate or incomplete for the target culture.\n \nThis is especially problematic when evaluating subtle cultural nuances beyond simple factual accuracy.\n \nThe Case for ‘Thick Evaluations’ of Cultural Representation in AI (Qadri et al., 2025)\n \n\\subsection{Limitations of Decontextualized \\&amp; Single-Point Metrics}\n \nThe challenges in evaluating cultural alignment extend beyond the culturally contingent nature of &quot;goodness&quot; to the very metrics and methodologies commonly employed. Standard quantitative metrics prevalent in NLP evaluation, such as accuracy, F1-score, BLEU, or ROUGE, prove fundamentally insufficient when assessing the nuances of cultural competence. Their primary drawback lies in their decontextualized nature. Cultural appropriateness is inherently tied to the specific situation, interactional dynamics, audience expectations, and implicit social cues – elements largely stripped away by metrics designed to measure lexical overlap with a reference text or adherence to a predefined factual label. A statement, for instance, might be factually accurate according to a gold standard but delivered in a manner that is culturally inappropriate, offensive, or simply nonsensical given the context; standard metrics are blind to such critical distinctions.\n \nFurthermore, these metrics often operate under the implicit assumption of a singular &quot;correct&quot; or optimal output. However, as discussed, culturally situated interactions frequently allow for, or even necessitate, a plurality of valid responses reflecting diverse perspectives, communication styles, or acceptable social norms within a given culture. Reducing evaluation to a single score against a predefined target obscures this crucial aspect of cultural competence and risks penalizing valid, culturally-attuned variations simply because they deviate from an arbitrarily chosen reference. This raises the question of whether the goal should always be assessing correctness or quality on a linear scale, or perhaps identifying what is definitely not acceptable – defining the boundaries of harmful or inappropriate behavior within a specific cultural frame.\n \nBeyond the metrics themselves, the dominant benchmark methodology – typically involving isolated, single-turn query-response pairs evaluated independently – fails to reveal important systemic behaviors or interaction patterns. Evaluating models based on aggregated scores from thousands of discrete test items can overlook consistent biases, stereotypical tendencies, or peculiar behavioral quirks that only emerge when observing responses across multiple related queries or during extended interactions. For example, Myung et al. (2024) observed a form of stereotypical fixation where, when queried about West Java, an LLM frequently defaulted to answering various food-related questions with `Seblak,&#039; one of the most famous dishes from the region. This tendency illustrates a pattern—providing seemingly relevant traditional information without fully grasping the specific context—that is unlikely to be captured by evaluating individual answers in isolation but points to a potential systemic issue in the model&#039;s nuanced understanding or knowledge representation. Such potentially problematic patterns, reflecting underlying biases or skewed knowledge representations, cannot be adequately identified or measured through single-instance evaluations focused solely on the correctness or quality of individual answers. Detecting these requires methods designed to systematically probe for and analyze patterns in model behavior across a spectrum of relevant inputs and interaction contexts.\n \nConsequently, relying solely on decontextualized performance scores and single-point evaluation paradigms risks painting an incomplete, and potentially misleading, picture of a model&#039;s true cultural capabilities and limitations. This necessitates a fundamental shift towards evaluation approaches that embrace context, acknowledge response plurality, and are capable of identifying broader behavioral patterns, demanding the &quot;thicker&quot; forms of evaluation we explore next.\n \n% Our suggestions\nGiven the cultural contingency of quality definitions and the limitations of decontextualized metrics and single-point benchmark evaluations, we must move towards richer, more situated assessment methodologies. Drawing inspiration from concepts like &quot;thick evaluation,&quot; as explored by Qadri et al. (2025) in the context of AI-generated cultural representations, offers a promising direction. This involves shifting away from singular notions of accuracy towards multi-dimensional frameworks that capture aspects like cultural missingness, specificity, coherence, and connotation, alongside correctness. Building on this, we suggest complementing the search for &quot;good&quot; outputs by identifying negative boundaries—systematically detecting responses that are culturally unacceptable, harmful, or nonsensical within specific contexts. Furthermore, particularly when comparing across cultures and languages, we advocate for employing more qualitative and comparative metrics that assess factors beyond simple performance parity. For instance, recent work highlights significant disparities in the sheer quantity of factual information models generate across different languages (Shafayat et al., 2024), suggesting that merely achieving similar accuracy scores might obscure underlying inequities in informational scope. Moreover, as noted by Shafayat et al. (2024), current factual evaluations often fail to differentiate the granularity or informational value of facts, treating highly specific statements (e.g., “Person X attended MIT”) the same as generic ones (e.g., “Person X attended a renowned university”). Assessing these factors—granularity, scope, and nuance—is crucial for a deeper understanding of cross-cultural capabilities. Adopting such &quot;thicker&quot; approaches inherently recognizes evaluation not just as a technical measurement, but as a contextualized process deeply intertwined with the perspectives and social locations of those involved, demanding careful consideration of who evaluates and under what conditions.\n% Metrics are supposed to measure an external construct of interest, but this assumption is often left unchallenged [refs in 1].\n% &quot;Values pluralism&quot; [1] makes it hard to get coherent samples that represent this multivariate problem space. \n% There are a lot of axes under which this problem can hold."},"01-Fleeting-Notes/D3JS-Notes":{"slug":"01-Fleeting-Notes/D3JS-Notes","filePath":"01 Fleeting Notes/D3JS Notes.md","title":"D3JS Notes","links":["tags/tool-learning"],"tags":["tool-learning"],"content":"2023-01-29 17:02\nTags: tool-learning\n\nwattenberger.com/blog/d3#manipulating-data\nwww.newline.co/fullstack-d3\nwww.w3schools.com/js/js_graphics_d3js.asp\nBuilding scatterplot for PECO\nd3-graph-gallery.com/graph/interactivity_tooltip.html\ntwitter.com/Wattenberger/status/1619729834153746432\n\nexample that has a lot of what i want. Mouseover for the tooltip on the actual text + category, dropdown of the categories.\nI want to do the same, a menu showing the cluster numbers, maybe ranked by clusterwise ECO bias score\nKey:\n\nList the cluster IDs with ECO bias score\nmouseover:\n\nblack outline all datapoints in the same cluster\ndim out-of-cluster datapoints\nshow distribution bar chart for the label classes\nshow wordcloud for samples\n\n\n\n\nPoints:\n\nxy from T-SNE\nColorcode by class label\nmouseover:\n\ntooltip for the specific datapoint\n\nchartio.com/resources/tutorials/how-to-show-data-on-mouseover-in-d3js/#creating-a-tooltip-using-mouseover-events\n\n\nblack outline all datapoints in same cluster,\ndim out-of-cluster datapoints\n\n\n\n\nSlider: (reach)\n\nThreshold for outlier/non-outlier cluster\nChanges the size of datapoints and the PECO score\n\n\n\nBeautiful example visualizations\nmbtaviz.github.io/\ntylermclaughlin.github.io/blog/2018/04/29/D3-graph-visualization-in-github-pages.html\nobservablehq.com/@d3/gallery\nobservablehq.com/@d3/scatterplot-tour\ngithub.com/d3/d3i"},"01-Fleeting-Notes/Daniel-Kahneman-(Fast-and-Slow-Author)-Interview":{"slug":"01-Fleeting-Notes/Daniel-Kahneman-(Fast-and-Slow-Author)-Interview","filePath":"01 Fleeting Notes/Daniel Kahneman (Fast and Slow Author) Interview.md","title":"Daniel Kahneman (Fast and Slow Author) Interview","links":["tags/article-notes","01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost"],"tags":["article-notes"],"content":"2023-04-10 12:51\nTags: article-notes\n\nLink (The Guardian)\n\nIn general, Kahneman is downbeat about the capacity of his brand of psychology to effect change in the world. I imagine he would simply argue he’s a realist about human nature. And, indeed, studies showing that “skilled” analysts are hopeless at predicting the price of shares have yet to translate into mass sackings or even reduced bonuses on Wall Street or in the City. The same goes for evidence that the influence of a high-quality CEO on the performance of a company is barely greater than chance.\n\n\nBut there are more modest ways his insights can help us avoid making mistakes. He advises, for example, that meetings start with participants writing down their ideas about the issue at hand before anyone speaks. That way, the halo effect – whereby the concerns raised first and most assertively dominate the discussion – can be mitigated, and a range of views considered. Then there is the concept of adversarial collaboration, an attempt to do away with pointless academic feuding. Though he doesn’t like to think in terms of leaving a legacy, it’s one thing he says he hopes to be remembered for. In the early 2000s Kahneman sought out a leading opponent of his view that so-called expert judgments were frequently flawed. Gary Klein’s research focused on the ability of professionals such as firefighters to make intuitive but highly skilled judgments in difficult circumstances. “We spent five or six years trying to figure out the boundary, where he’s right, where I am right. And that was a very satisfying experience. We wrote a paper entitled ‘A Failure to Disagree’”.\n\n“If I could use a magic wand and get rid of one thing, it would be overconfidence”\nWhy I’m Not worried about superintelligence blogpost\n\nWhat’s fascinating is that Kahneman’s work explicitly swims against the current of human thought. Not even he believes that the various flaws that bedevil decision-making can be successfully corrected. The most damaging of these is overconfidence: the kind of optimism that leads governments to believe that wars are quickly winnable and capital projects will come in on budget despite statistics predicting exactly the opposite. It is the bias he says he would most like to eliminate if he had a magic wand. But it “is built so deeply into the structure of the mind that you couldn’t change it without changing many other things”.\n"},"01-Fleeting-Notes/Definition-problem-in-killer-robot-ban":{"slug":"01-Fleeting-Notes/Definition-problem-in-killer-robot-ban","filePath":"01 Fleeting Notes/Definition problem in killer robot ban.md","title":"Definition problem in killer robot ban","links":["tags/writing-ideas","tags/transparency","tags/definitions"],"tags":["writing-ideas","transparency","definitions"],"content":"Definition problem in killer robot ban\n2022-04-23 16:44\nTags: writing-ideas transparency definitions\n\nRao wrote an article for the guardian in 2017 about why he is reluctant to sign a “killer robot ban,” and a part of the reason why is that it’s poorly defined and not actionable.\n\nApart from the difficulty of pinning down exactly what the ban entails for states that want to follow it – is the ban against autonomy or intelligence? – I wonder about the ban’s ability to deter misuse by rogue state or non-state actors.\n\n(Link)"},"01-Fleeting-Notes/Demo-Track-p37":{"slug":"01-Fleeting-Notes/Demo-Track-p37","filePath":"01 Fleeting Notes/Demo Track p37.md","title":"Demo Track p37","links":[],"tags":[],"content":"Abstract def of CoT is a little off---it’s intermediate step explanations, not example explanations.\nPerformance hindered by intermediate errors in the explanation steps. They introduced CoTEVer, a toolkit for annotating factual correctness of generated steps in reasoning and collecting revisions to that data\nBackground knowledge and manual writing by annotators are both issues in collecting explanation data. They want to try to collect it more efficiently, with a human-in-the-loop with an automated machine annotation system.\nSteps:\n\nPrompting\n\nBuild GPT-3 CoT prompts using “Self Ask”\nAnnotators asked to query a variety of different questions. Requests are sent following annotator use\nWere there sample questions used? Not clear in the main text\n\n\nEvidence retrieval\n\nSub question as query for retrieval on Google Search API\n512 token chunking of retrieved documents for custom reranking using Sentence-T5 and STS\n\n\nExplanation and Answer verification\n\nAnnotators can 1-5 likert scale the quality of explanations\nThey also label which document is used as evidence\n\n\n\nAnalysis given focuses on failure modes for explanations, including insufficient knowledge, out of date, and wrong fact. Interestingly, wrong fact is the predominant error type.\nThey propose utilizing this human-in-the-loop system for fine-tuning LLMs for more accurate Chain-of-thought prompting. This is probably a good way in particular to deal with wrong fact errors, perhaps in an RLHF setup, but apart from that difficult. They suggest unlikelihood training as a different way to negatively reinforce wrong fact errors."},"01-Fleeting-Notes/Denoising-Diffusion-Probabilistic-Models":{"slug":"01-Fleeting-Notes/Denoising-Diffusion-Probabilistic-Models","filePath":"01 Fleeting Notes/Denoising Diffusion Probabilistic Models.md","title":"Denoising Diffusion Probabilistic Models","links":["tags/paper-notes","01-Fleeting-Notes/Mapping-is-not-memorization","CoCoCroLa-Scattered-Observations"],"tags":["paper-notes"],"content":"2023-03-14 16:57\nTags: paper-notes\nLinks: Mapping is not memorization CoCoCroLa Scattered Observations\n\nDenoising Diffusion Probabilistic Models\nOriginal DDPM paper. Expertise on this topic is necessary.\nOriginal PDF\n\nDiffusion process is explicitly a markov chain\nVariance schedule of Betas\n\nWe ensure that the final step in the forward process “zeroes” out information by having last Beta be 1\n\n\n\nChallenge to my understanding is that the previous t latent (or frame) x_{t-1} is a condition for the function for the next step \\mu_\\theta(x_{t-1},t)\nAt each pixel, guess the mean of the pixel values that will most denoise us toward the final image x_0, which can be conditioned on the other pixels in my time step in any way as well as on knowledge of the timestep\nSupplemental\nMost focus in the paper is on the uncoditional generation case, they also used CelebA\nScore-based models\n[yang-song.net/blog/2021/score/](Yang Song blogpost)\nlilianweng.github.io/posts/2021-07-11-diffusion-models/\nCheck out the Lilog post as well\nDoes a phrase h"},"01-Fleeting-Notes/Design-moodboard":{"slug":"01-Fleeting-Notes/Design-moodboard","filePath":"01 Fleeting Notes/Design moodboard.md","title":"Design moodboard","links":[],"tags":[],"content":"coloring\nantiwork.com\n\nGenerates color scheme automatically\nfonts\n\nI really like the way she used Futura here, and the yellow font with the slight drop shadow reads well over the rest\nOpenai logo exploration\nx.com/areatechnology_/status/1968342839542350119\nSo y2k so aesthetic so sex\n\n"},"01-Fleeting-Notes/Diffusion-Image-Memorization":{"slug":"01-Fleeting-Notes/Diffusion-Image-Memorization","filePath":"01 Fleeting Notes/Diffusion Image Memorization.md","title":"Diffusion Image Memorization","links":["01-Fleeting-Notes/Mapping-is-not-memorization","01-Fleeting-Notes/Tasks-for-\"Mapping-is-not-Memorization\""],"tags":[],"content":"twitter.com/WenhuChen/status/1620859319670681603\narxiv.org/pdf/2301.13188.pdf\n\n\nMore duplicated samples from SD2 training data easier to recover\n\nLots of discussion framing this around SD2 as a “lookup table” or not. Dismissive response is “it’s the most effective compression”\ntwitter.com/Eric_Wallace_/status/1620488766925438976\n\nNote that it is impossible by definition for large-scale models to memorize lots of data because the size of their training sets are 1000x - 1,000,000x larger than the model in terms of storage.\n\nI’m a little on the fence about this argument. Each input noised image carries a massive amount of initial information which is also an aide in keying prompt to exact image.\nThey point out that it’s easier/more successful to find the duplicated samples in the dataset. Could it be that those samples simply have more keys paired to them?\n1000x compression is a LOT, but (a) it’s very lossy, (b) it’s keyed to an equivalent-size key.\nMy tweet:\n\nI think the 2GB model should be thought of as the compression algorithm, not the compressed archive.\n\n\nPerhaps only one (noise init, prompt) pair recovers a target training image. There’s a LOT of possible noise initializations out there.\n\n\nWhat if I told you that in 116kb github.com/packjpg/packMP3 I can achieve a perceptually lossless 95% compression rate of any audio file?\nIn the case of SD, the input “compressed file” (the noise init) is actually of equivalent size to the “decompressed” output image. PLUS you’re adding the prompt information\n\nThe fact that it’s easier to find the duplicated images could support this interpretation. After all, you would have to try an INSANE amount of randomly sampled mp3-encoded binary files before you could recover a specific target audio.\nThis “diffusion model as (keyed noise, prompt) → (exact image)” interpretation I think supports the argument that memorization isn’t really the function of the model. The image itself isn’t inside the model any more than “Never Gonna Give You Up” isn’t stored in the MP3 algorithm.\nIts just that the learned init pix+prompt → image map is very complicated and incomprehensible, learned via diffusion training process, and as a byproduct produces a uesful generalization.\n\nI think it’s possible that every training sample is “memorized” in the sense that there could exist a (noise initialization, prompt) pair to recover every training image.\nInit pixels rather than the model weights as compressed image⇒lossy, negative compression rate?\n\nI think this interpretation is further supported by altclip’s shocking similarity across languages in their controlled init noise experiments\nTHAT BEING SAID: if this interpretation is correct, the fact that Wallace and friends are so capable of performing this attack to find the inversion is SHOCKING. Imagine using this output similarity attack (with no prior knowledge about the MP3 algorithm) trying to find a matching mp3 binary encoding to match a wav file. Impossible.\nThere’s probably lots of redundancy built in to this mapping and the performance of this dumb search method suggests there’s more going on.\nHowever, at the end of the day, the recoverability of a specific training image given a specific noise init supporting the compression + interpolation interpretation suggests to me that handling this is memorization by the model is not the right way to treat handling, regulating, responding etc to these model\nPhilosophy of memorization\nhow is Fig1 diff from “memorized” llama that’s a different colorkj\nFrom messenger\nI have no intuition for how reusable subelements of the mapping function from semantic conditioned noise to image is for memorization… could be very high\nImportant distinction against mp3 algo is that the input noise thats being “decompressed” is not correlated at all with the final output. But it’s gaining info from prompt+2GB of weights\nAnd 2gb of memo images is nothing, but 2GB of pure function implementation is massive\nUseful to have a principle for noise similarity\n\nSince input and output are in image space, L2 metric is good for all points\nL2 similarity is useful for guarantee of Lipschitz smoothness (bounded by difference on output)\nAgrees that targetting what the true meaning of memorization actually is for generative image (conditioned on noise) in diffusion models is the right direction\n\nWould it be possible to estimate the bits of information carried internally and externally?\nProject Coordination\nMapping is not memorization\nTasks for “Mapping is not Memorization”\nwww.kaggle.com/datasets/denislukovnikov/celebahq256-images-only\nShutterstock memorization by model twitter.com/_akhaliq/status/1637321077553606657\narxiv.org/abs/2208.11970\n\nUnderstanding diffusion models\n"},"01-Fleeting-Notes/Do-AI-systems-really-have-their-own-language":{"slug":"01-Fleeting-Notes/Do-AI-systems-really-have-their-own-language","filePath":"01 Fleeting Notes/Do AI systems really have their own language?.md","title":"Do AI systems really have their own language?","links":["tags/language-model","tags/pretraining","tags/prompting","tags/summary"],"tags":["language-model","pretraining","prompting","summary"],"content":"Do AI systems really have their own language?\n2022-10-10 18:21\nTags: language-model pretraining prompting summary\n\nAn article by Aaron J. Snoswell\n“DALL-E has its own secret language” tweets popped up in mid 2022\n\nMain point of these was that prompting DALL-E to write stuff about birds put out seeming gibberish, such as “Apoploe vesrreaitais” to mean birds.\nSome arguments in this article are made against it being accurate to say that DALL-E 2 has a “secret language.”\n\nDifficulty to verify claims about these models due to lack of access\n\nLeads to cherry picking\nLimits to interaction with the system under the hood\n\n\nRelated to specific items of vocabulary.\n\n\nTurns out the weird samples here were actually drawn from the binomial taxonomical names of the birds\n\nGarbage in garbage out?\nGreat example of an adversarial attack vector\nRelates to things like “zoning tapping fiennes” triggering racist content\n\nRelates to problems of control"},"01-Fleeting-Notes/Doctoral-degree-filing-checklist":{"slug":"01-Fleeting-Notes/Doctoral-degree-filing-checklist","filePath":"01 Fleeting Notes/Doctoral degree filing checklist.md","title":"Doctoral degree filing checklist","links":[],"tags":[],"content":"\n Verify no incompletes in GOLD ✅ 2025-06-09\n Verify committee on file ✅ 2025-06-09\n Copyright permissions ✅ 2025-06-09\n ProQuest account ✅ 2025-06-09\n Electronic submission of dissertation on proquest ✅ 2025-06-10\n Signed forms sent to gradacademics@graddiv.ucsb. ✅ 2025-06-09\n Approval page emailed to same ✅ 2025-06-09\n Accurate committee on gradpoint ✅ 2025-06-09\n sed-ncses.org/login.aspx 📅 2025-06-16 ✅ 2025-06-16\n ucsb.co1.qualtrics.com/jfe/form/SV_6ytxq2xEnXDx1ye ✅ 2025-06-09\n"},"01-Fleeting-Notes/ECE594-Wk1":{"slug":"01-Fleeting-Notes/ECE594-Wk1","filePath":"01 Fleeting Notes/ECE594 Wk1.md","title":"ECE594 Wk1","links":["tags/course-notes"],"tags":["course-notes"],"content":"2023-01-12 14:05\nTags: course-notes\n\nCovers:\n\nAdversarial robustness (e.g., adversarial patches in image)\nDistributional robustness (snow on stop sign)\nUncertainty estimates*\nCourse goal:\nOverview of different robustness issues in ML\nPropagate knowledge from one research community into another\nWell-aligned goals with my own research direction\nCourse format:\nPaper list\nPresent papers to the class for a rubric\nClass paritcipation, paper presentation, answers to reading questions/self eval, final research proposal (2 pg)\n\nProposal is individual\nPresentations are team based and you should swap up teams\n\n\nSlides need to go to Yao within 4 days before the class; DO NOT BE LATE\n"},"01-Fleeting-Notes/Elements-of-transparency-study-as-it-stands":{"slug":"01-Fleeting-Notes/Elements-of-transparency-study-as-it-stands","filePath":"01 Fleeting Notes/Elements of transparency study as it stands.md","title":"Elements of transparency study as it stands","links":[],"tags":[],"content":"Elements of transparency study as it stands\n2022-04-24 15:59\nTags:\n\n\nOverview by broad separation\n\nfrom human/from machine\n\nfurther breakdown into specific clusters of work\n\n\nREV: this distinction requires elaboration\n\n\nSec 4: analysis of commonalities and conflicts b/w studies\n\nGiven the distinctions, we need to point out why they’re significant/impactful\n\neg, we have 4.5 where we explain conceptually why the distinction matters but not concretely\n\n\n\n\nSec 5: why matters\n\nPoint: we should\n\n\n\nWilliam says just send it out\nIn the very beginning, an entire section setting up why this is significant\n\nAcademic and industrial research behavior in AI has real impacts on the world, in particular, legislation\nPeople are throwing around suitcase word transparency without defining conc retely or an agreed upon def\nAgreement necessary to ensure good research going forward\nWe go through some work and identify core disambiguating directions and introduce how they fall in there\n\nSpecific quotes and stuff\n\n\n\n\nAlex likes splitting the paper idea\nWe need more references in secs 4,5 (yea ofc)\nLit review elements will need to be expanded (AND MADE MORE SPECIFIC) to stand alone\nI need to consider conferences we can send the position to\nSecs 2,3 should be broken apart (EACH SUBSECTION AS A FULL SECTION)\n\nEach subsection can be exploded into 2-3 pages on its own\n\n\nI need to go through the prior lit reviews we’re referencing\n\nGO BACK AND CHECK COMMENTS IN SLACK MSG FROM ALEX"},"01-Fleeting-Notes/Emergent-Capabilities-and-Memorization":{"slug":"01-Fleeting-Notes/Emergent-Capabilities-and-Memorization","filePath":"01 Fleeting Notes/Emergent Capabilities and Memorization.md","title":"Emergent Capabilities and Memorization","links":["tags/blog-idea","01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost"],"tags":["blog-idea"],"content":"2023-03-24 16:47\nTags: blog-idea\n\ntwitter.com/danish037/status/1639412723963539456\nMemorization in high-generalization models is a very interesting question\n\nIf a model can generalize from discussion of a problem to correctly answering it, that’s a really exciting capability!\nBut it muddies the waters of the capability we’re actually testing for. Maybe it can’t do math at all, it’s just REALLY good at reasoning over language\n\nConnects to the “How much did lunch cost?” idea\nWhy I’m Not worried about superintelligence blogpost"},"01-Fleeting-Notes/Emerging-Architectures-Reading":{"slug":"01-Fleeting-Notes/Emerging-Architectures-Reading","filePath":"01 Fleeting Notes/Emerging Architectures Reading.md","title":"Emerging Architectures Reading","links":[],"tags":[],"content":"Mamba, H3, Hyena, RWKV\nwww.youtube.com/watch"},"01-Fleeting-Notes/Examples-of-Google-Veo-fails":{"slug":"01-Fleeting-Notes/Examples-of-Google-Veo-fails","filePath":"01 Fleeting Notes/Examples of Google Veo fails.md","title":"Examples of Google Veo fails","links":[],"tags":[],"content":"\n\nNonsense dialogue\nSlight shifting of background elements\n\n\nTalking frog video:\n\nNonsense background hand motion on top right\n\nSWAT team raid video\n\nDiscontinuity between shots:\n\nCar is much bigger on the inside\nI count 8 inside car\nShot pans past open door on the inside that’s closed on the outside\nTwo step out the side door with face shields\n5 step out back, where the car has changed color and is opening in a way it shouldn’t be able to\n0:39 muzzle flash is coming out of the sides of the muzzle instead of the front\n0:45 wearing googles he didn’t have in prior scenes\nTalking main character with beard disappeared inside of the car\n\n\n\n\nAuto show:\n\n0:30 HELS ALGELS\n\nTalking heads:\n\n0:28 “Imagine all the narratib possibilities”\n\nNotes on developing these evaluations\nHow do we sanity check on real generated videos?\nWill non-video-related aspects of the IRL video sources cause artifacts in the generated assessments?\nCan we collect a dataset of real professional videos that have these kinds of things?\n\nExpensive\nTime-consuming\n\nOr we can generate videos using Pika etc\n\nCompare to the google model\nCreate these challenging examples\n"},"01-Fleeting-Notes/Examples-of-papers-for-AGI-Middleware":{"slug":"01-Fleeting-Notes/Examples-of-papers-for-AGI-Middleware","filePath":"01 Fleeting Notes/Examples of papers for AGI Middleware.md","title":"Examples of papers for AGI Middleware","links":["tags/paper-notes","tags/paper-planning"],"tags":["paper-notes","paper-planning"],"content":"2023-01-16 14:10\nTags: paper-notes paper-planning\n\nSelf-Instruct (Yizhong Wang…Swaroop Mishra…Hannaneh Hajishirzi AllenAI)\nConstitutional AI (Anthropic)\nDebiasing\nAnalyzing Biases to Spurious Corerlations in Text Classification Tasks (ACl22)\nDoes Self-Rationalization Improve Robustness? (AllenAI)"},"01-Fleeting-Notes/Excerpted-from-the-review-paper":{"slug":"01-Fleeting-Notes/Excerpted-from-the-review-paper","filePath":"01 Fleeting Notes/Excerpted from the review paper.md","title":"Excerpted from the review paper","links":[],"tags":[],"content":"\\subsection{Replicability and the rush to preprint}\nThere is something of a replicability crisis ongoing in the LLM field. Reliable and transferable benchmarks of \\ourterm self-correction have yet to be fully established. Work in parallel problem areas such as translation evaluation may provide insights into best practices for LLM capability evaluation."},"01-Fleeting-Notes/Explaining-Answers-with-Entailment-Trees":{"slug":"01-Fleeting-Notes/Explaining-Answers-with-Entailment-Trees","filePath":"01 Fleeting Notes/Explaining Answers with Entailment Trees.md","title":"Explaining Answers with Entailment Trees","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-04-03 19:11\nTags: paper-notes\n\nabs link (Cited in Neha/Rachel delta baselines to NLI context ignorance paper)\n\n\nUsed as a core analysis dataset in ROSCOE\n\n\nI think I looked at this when we were working on wikiwhy write up\n\n\n“Line of reasoning”\n\n\nCollected/expressed through a more intricate graphical interface\n\nPool of relevant facts can be dragged and dropped to form answers\nExpresses the joint entailment as a graph.\n\nIntermediate conclusions in blue.\n\n\n\nCollected from wikipedia"},"01-Fleeting-Notes/FAccT-Log":{"slug":"01-Fleeting-Notes/FAccT-Log","filePath":"01 Fleeting Notes/FAccT Log.md","title":"FAccT Log","links":[],"tags":[],"content":"Monday\n\nSaw Luca\nCaught first talk, Thick Alignment\nMet with Ohannessian, Mesrob in Coffee Sesh from UIC in the poster session, he mentioned trying to get invited as a young researcher talk at TTIC and that he previously did their research prof job, I should apply to UIC as well\n\nPaper Session on Decisions, Trust, Reliance\n\nCertification labels for Trustworthy AI (Nicolas Scharowski…Florian Bruhlmann)\n\nInterview study\nAccuracy measures ought to be worked in to certification labels\nOught to be granted by external evaluators\nProblem: too many labels, risk of bullshit labels (too many organic labels let’s say)\n\n\nTowards a Science of Human-AI Decision Making (Chacha Chen…Q Vera Liao)\n\nSurvey study of a big set of AI-aided decision making, human-AI interaction\nConsider dimensions of\n\nRisk\nTask nature\n\n\nJustify decisions made\nFrameworks\nExtremely fast\nTaxonomy of AI assistance elements\n\nPrediction precedes info about the prediction precedes information about models precedes other AI system elements\n\nASK HER ABOUT IT, I would argue model-related information sits outside\n\n\n\n\nTrend of assistance beyond predictions, explanations by the AI\n\nGap is no systematic understanding of assistance elements\n\n\nMetrics include efficacy, efficiency\nSubmit a PR for my paper\n\nhaidecisionmaking.github.io/#/about\narxiv.org/abs/2112.11471\n\n\n\n\nHumans, AI, and Context: Understanding User Trust (Sunnie SY Kim)\n\nInterviewed users of a Bird ID app about many questions including current stakes and intention to use the app in hypothetical high stakes scenarios\nIterative coding scheme to prepare the data analysis\nNot all participants have the ability to assess the correctness of system outputs\n\n\n\nPolicy CRAFT Session\n\nNIST: standards around AI governance\nNational AI Advisory Cmte\nPCAS (Presidential Council on AI something…)\nDon’t chase everything. Every senator is trying to get in the news. Focus on what’s attainable.\nAdvisory council members are the right people to get to amplify (eg, National AI Advisory Cmte)\nSpecific results are more successful in the policy sphere than general. Show them their system is discriminatory eg, not someone else’s or classifiers in general\n\nREVIEWERS NEED TO VALUE THOSE SPECIFIC TYPES OF PAPERS\nITS NOT SURPRISING THERES DISCRIMINATION IN THIS SYSTEM SO WHY PUBLISH\nEACH AUDIT IS EXTREMELY USEFUL FROM A POLICY PERSPECTIVE. WE NEED THEM. WE NEED REPEAT AUDITS. THESE GET CITED AND USED IN DOCUMENTS THAT GUIDE POLICYMAKERS\nMAKE THE DECLARATIVE STATEMENTS AND PAINT THE PROBLEMS THAT FOLLOW IN BROAD STROKES. EXPRESS WHY ITS IMPORTANT\nThis runs contra to our instincts to prioritize the most general solutions as CS people\n\n\nThey really are inviting people in. Center for Data and Technology, etc all organizers are looking for people to apply. They can help add the cover letter, etc\n\nTuesday\n\nIRL friend requests lmao\nTurkopticon session\n\nTurkers don’t want union\n\n\n\nSession 5 (Explainability + Limitations)\n\nRun by Jenn Wortman Vaughan\nAlon Jacovi from Yoav’s group on diagnosing AI explainer methods using limitations of behavior\n\nExplainability analyzed based on alignment to a mental model from a user of what information is being used to make a given decision\n“Folk concepts of behavior”\nCounterfactuals necessary for causal explanation?\n\nThey construct a causal chain in an illustrative example using a self-driving car crashing into a wall. With an alternative event narrative with but-for changes to the scenario wherein the outcome is different, this is a good expl\n\n\nThey do training data attribution examples\nExplanations should use interactivity to resolve contradictions\n\n\nSecond talk: what is to stop automated decision systems from lying about using bad features\n\nThey set up a malicious explanation generation system\nThey assume that recipients of a decision don’t get to compare to others\nThis is for racial bias in COMPASS type of setting\nMetrics exist to measure consistency, sufficiency in decisions. The sufficiency metric can go down via looking for conflicts.\n\nHowever, the way to use these requires a comparison between an original model, racist, and random model\nBut IRL you only get to see one of those\n\n\nTurns out black box explanations for black box models doesn’t work\n\n\nQuestioning the ability of feature-experts explanations (Astrid Bertrand)\n\nImportance of scare-quoted “automated”\n\n\nExplainable AI is dead! Long Live explainable AI! (Tim Miller)\n\nStory of “bluster” and “prudence”, two friends who give you advice\n\nbluster tells you what it thinks you should and only positively justifies its decisions, negative justifications to anything else\nprudence tells you only feedback on your suggestions, positive and negative\nEveryone in the room almost prefers prudence, but we give users bluster\n\n\n“People who ignore explanations overrely on decisions?” I kinda missed what he was saying\nWe have a recommend then defend approach\n\nHis hypothesis for why this doesn’t work is it doesn’t fit in to our decisionmaking processes\nWe can’t use this kind of thing in a deliberative process\n\n\nREAD THIS PAPER FOR SURE\n\n\nnl4xai.eu/\n\nAI Art Session\nEva Toorenent\n“I never imagined that if I said no to a proposal that a company could mimic my style”\n“I was at a crossroads: I could completely abandon my style and try to find work or continue developing”\nShe sells a course for watercolors + photoshop\nShe made a book and got some work. Made marketing videos for Wacom, works with galleries\nEvery single income stream is affected by AI art, “total infiltration:”\nAI artist fed her work into midjourney\n“Without artists this tech would not exist but also without you scientists this tech would not exist”\nConcept artist for a bunch of movies shares his journey as an artist\nMatte painting…even film screenshots as training data will constitute theft\nPower accumulation more than a democratization, more akin to commissioning than doing art\n\nRe: Heikkila sexualization article something something “the process by which images of marginalized women are generated” leading to the sexualization disparity (missed it fixing a typo)\nEconomic and social impact\n“Ouroboros” effect ends novelty in content and also reduces culture\nEnd-goal is believed to be highly personalized content: I 100% agree this is the core goal\n\nDeleterious social impacts of giving people exactly what they want…never challenged\n\n\nLots of extremely cringe vitriolic insults toward artists who protest, the luddite stuff tech hype etc.\n\nI want to let them know we’re really with them against the frustrating tech bros\nEntitlement and Impunity in AI/ML\n\n\nData laundering: academic can do whatever they want → for profit\nLAION website clearly says don’t\nSpawning.ai is a false opt-out. You need to be aware of it to get out, and it doesn’t remove entries\nglaze.cs.uchicago.edu is producing a tool to obfuscate your data for training\n\nMy question:\n\nThanks to the organizers for dealing with tech diff and making it accessible\nA lot of us in this room even researchers who work in this and related areas are 100% with you in distaste for the tech bros\nConcern: with or without research communities like FAccT, individual devs can advance this technology, I feel like we need to work on responsible dev\nIncludes building public awareness to weaken the impact of fake news, I think this could be extended to make unethical use of t2i socially unethical outside of the tech bro circles\nMy question re: ethical dev:\n\nThree separate issues:\n\nTheft of style\nUncompensated use of their work to build the core capabilities\nEconomic impacts to loss of work\n\n\nA technical solution to limiting style theft in responsible AI could be possible\nHowever, the fundamental capabilities to generate “art” or “understand” objects, etc are fundamentally built on a huge diversity of input samples\n\n\nWhat kinds of norms of ethical production are you comfortable with?\nIs your preference for nonexistence of this\nWhat compensation would you find acceptable, how does this change depending on if the styletheft is or isn’t impossible\n\nWednesday\n\nManuel Brack from TU Darmstadt working on a project with a german group very related, multilingual but dealing with calssifiers, should make contact\nMet Hanlin Li at lunch, incoming faculty at UT Austin working on fairness, should send students her way\n"},"01-Fleeting-Notes/FB-ConvAI-Mtg":{"slug":"01-Fleeting-Notes/FB-ConvAI-Mtg","filePath":"01 Fleeting Notes/FB ConvAI Mtg.md","title":"FB ConvAI Mtg","links":[],"tags":[],"content":"FB ConvAI Mtg\n2022-05-04 13:42\nTags:\n\nFB ConvAI has an end-to-end task oriented dialog problem space\n\nSome work and collaboration connects with chit-chat scenarios and task-oriented dialog\nTraditional approach is pipeline\nReplacing the pipeline with an end-to-end LM fine-tuned\n\nBART-based model pretrained on a dialog dataset\nCalled KARAOKE\nEnd-to-end model takes in dialog context (flattedened user-system turn)\nPredicting next API call and its parameters\nModeling these as the API calls with their parameters\nAt the end of the API calls you also do some generation of text (user-facing system-generated text)\nThey have this model and several datasets both open-source and internal, the E2E model works\n\n\nDirections:\n\nAdd multimodal context (vision encoding so context isn’t just text-based)\n\n“visual context of the user”\n\n\nContinual learning\n\nGiven domains ABC model, how do you scale to domains DE\nHow do you do domain composition?\n\n\nMaking these resource constrained\n\n\n\nWilliam’s tip:\n\nDiscuss what it takes to deliver on this particular group, project pair with Zhiyu\nShe knows the pitfalls, etc\nFocus on leveraging the resources\n\nMeta fellowship to keep in mind:\n\nbuilding relationships inside the company, lunches, free time with mentors\n\nAsk about what the fellowship takes\nThey will not give it to people they don’t have connection to\nInternship, collaborative project, etc helps people get the fellowship\n\n\nNeed a champion on the inside\n\nIdentify someone who can be vocal, like your work\nAsk those people to help, the thing we can really offer to them is continuing to push the work if they get it to us\nBing Liu, Shane Moon\n\n\n\nConnection is beneficial and diversity is useful for getting the stuff"},"01-Fleeting-Notes/Felix-Mtg":{"slug":"01-Fleeting-Notes/Felix-Mtg","filePath":"01 Fleeting Notes/Felix Mtg.md","title":"Felix Mtg","links":[],"tags":[],"content":"sega model editing method\narxiv.org/abs/2211.05105\n\n\nsimilar direction of multilingual coverage, but in gender bias direction\n\n\nedits the model to even binary gender occurrence rate for occupational concepts\n\n\nlooking for difference between languages\n\n\ngenerate images from altdiffusion on german\n\n\ninterested: can we steer in one language and keep mitigations?\n\n\nbias: image of a “foreigner” which gives different results by different language\n\n\npareto curve for representativeness vs bias; justifying contributions based on improving that frontier\n\n\npotential for a survey study of ppl on their opinions for representativeness\n\n\nSEGA framework applies most well for gender bias\n\n\nblindly subtracting nudity vector from conditioning\n\n\nusing DDIM look at final image estimate\n\n\n“diffusion models are zero-shot classifiers”\n\n\nI asked: how does subtracting nudity vector impact bodybuilders, swimsuits, etc\n\nmight be good\nbut the better you can define the concept you want to guide away from, the better it works\n“does CLIP know this concept even?”\n\nif you want to subtract from something CLIP doesn’t know, “you’re lost”\n\n\n\n\n\ndiscussed Royi Collab op\n\n\nthey mention this work MultiFusion arxiv.org/abs/2305.15296\n\n\nCollab op:\nThe “complex biases” for things like “african man in front of his house”\n\nWhen you want to generate the david sculpture but subtract nudity, you end up with this collision\ncan we suppress the complex biases in English as well as Chinese?\nproblem they have:\n\nalready have eg skin tone biases on a bunch of generated images\nalready have some manual analysis\nQ: how can they evaluate what they have?\nHow can they deal with the genderbias vs skintone that they have?\nAre there traps or things they could get caught on?\n\n\n\nOpportunity for collaborations:\n\nhigh-impact metrics capturing variations in generated images that are actually semantically important\nidentifying the fine-line between desireable and undesirable classes of images and framing the transtition between prompts that elicit one and the other as CCCL-like mappings (as in CCCL the mapping is EN→new language)\n"},"01-Fleeting-Notes/Fine-tuning-to-improve-SD2-multilinguality":{"slug":"01-Fleeting-Notes/Fine-tuning-to-improve-SD2-multilinguality","filePath":"01 Fleeting Notes/Fine-tuning to improve SD2 multilinguality.md","title":"Fine-tuning to improve SD2 multilinguality","links":["tags/paper-ideas"],"tags":["paper-ideas"],"content":"2023-01-16 14:01\nTags: paper-ideas\n\nDiffusion Models for Adversarial Purification (Caltech and NVIDIA)\nImproving Diversity with Adversarially Learned Transformations for Domain Generalization (Tejas Gokhale…Chitta Baral and Yezhou Yang)"},"01-Fleeting-Notes/Finetuning-of-stable-diffusion-possible":{"slug":"01-Fleeting-Notes/Finetuning-of-stable-diffusion-possible","filePath":"01 Fleeting Notes/Finetuning of stable diffusion possible.md","title":"Finetuning of stable diffusion possible","links":["01-Fleeting-Notes/Accidental-token-collisions-in-SD"],"tags":[],"content":"But how stable are the other representations?\nSeems the approach is to add a new special token to correlate with the new concept/images\nHow well supported is the concept of a “concept” with respect to the storage of an object or idea or type of “thing” inside these models? Check it in.\nwaxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/\nOther examples of issues:\nAccidental token collisions in SD"},"01-Fleeting-Notes/Fluid-Language-model-benchmarking":{"slug":"01-Fleeting-Notes/Fluid-Language-model-benchmarking","filePath":"01 Fleeting Notes/Fluid Language model benchmarking.md","title":"Fluid Language model benchmarking","links":["tags/benchmarking","tags/paper-notes","tags/evaluation"],"tags":["benchmarking","paper-notes","evaluation"],"content":"Blog post\nPaper\nFrom Ai2 benchmarking paper-notes evaluation, COLM 2025\nMotivation\nItem response theory: a test question is most informative when its tailored to the capabilities of the student.\nWhy are we using monolithic test sets across the model capability range?\nAcc item response theory (IRT) joint patterns of responses from test takers reveals latent difficulty.\nMethod\n“Fluid benchmarking” selects items by matching them to the model’s capability level.\nFindings\n\nHigher validity and lower variance with 50x fewer items.\n"},"01-Fleeting-Notes/G2P":{"slug":"01-Fleeting-Notes/G2P","filePath":"01 Fleeting Notes/G2P.md","title":"G2P","links":[],"tags":[],"content":"Working with a company to improve name pronunciation in graduation ceremonies\nVoice conversion\nAutoVC - does VC but doesn’t transfer the target speaker rhythm"},"01-Fleeting-Notes/GPT4chan-paper":{"slug":"01-Fleeting-Notes/GPT4chan-paper","filePath":"01 Fleeting Notes/GPT4chan paper.md","title":"GPT4chan paper","links":["tags/paperidea","tags/ethics","tags/language-model","tags/writing-ideas"],"tags":["paperidea","ethics","language-model","writing-ideas"],"content":"GPT4chan paper\n2022-06-07 13:40\nTags: paperidea ethics language-model writing-ideas\n\nThe ethics of GPT-4chan, in particular releasing it to interact directly with the public is extremely questionable and controversial.\nUtility of the model\n\nthe harm was done more in releasing the dataset\nthere is lots of potential utility in this model, in particular in\n\noutput comparative analysis for understanding model behavior under the different kinds of training coropora\nclassification and text analysis we can build likelihood features like in the transparency paper using this tool\n\n\n\nHigh impact: get a new SOTA\n\nPeople don’t care in academia with this kind of analytical/discussion work\nHow can this become novel when papers like the Stochastic Parrots work is already out there\nBest bet to get impact for now is to stay focused on the transparency work\nOr get a strong, SOTA hate speech detector on all existing hate speech datasets using GPT-4chan\n\nThe controversy\n\nLimited ethical gatekeeping took place\n\nAuthor has a history of being (to put it charitably) skeptical of AI ethics as a concept\n\nFor example, turning the Timnit Gebru firing into DRAMA CONTENT for youtube\n\nDevAdv: how is this different from mass media articles on the firing?\n\nDifferent “angle” and audience\nThe nature of social media invites discussion\n\n\n\n\n\n\n\n\nThe attention-seeky manner of the release is eyebrow-raising and has a potential for problems\n\nThe twitter-first content-focused research discussion paradigm is particularly destructive in this domain because of how it interacts so directly with the general public\nAI ethics becoming a “culture war issue” is happening and is dangerous\n\nUnwarranted politicization/inviting discussion by bad faith/uninformed actors is destructive for the discourse\n\n\nActors like Pedro Domingos are particularly bad on this front: deputizing culture war/“anti-woke” actors online in order to win online arguments by casting things like ethical review as “cancel culture”\n\nI do believe Yannic was deliberately inviting this kind of attention with the edgy and clickbaity title and action of actually releasing the model in the wild\n\n\n\n\n\nQuestions raised\n\ndo we really have a solid agreed-upon definition of harm in this space?\n\nAnswering this question seems to be key to making a successful\n\n\nHow do we navigate the connection between research and content?\n\nGetting the open research problem on NLP is a better thing to do. Stay focused and have something for EMNLP"},"01-Fleeting-Notes/GRFP-Report-2025":{"slug":"01-Fleeting-Notes/GRFP-Report-2025","filePath":"01 Fleeting Notes/GRFP Report 2025.md","title":"GRFP Report 2025","links":[],"tags":[],"content":"I study generative AI artifacts like LLMs and text-to-image models. I make meaningful evaluations of new capabilities that are difficult to measure to improve them.\nI am a fifth-year Ph.D. student at the University of California, Santa Barbara, advised by Prof. William Yang Wang. I am a NSF Graduate Research Fellow, a Center for Responsible ML Fellow, and a 2024 Rising Star in Generative AI.\nAbout\nI have broad interests in generative AI, NLP, and multimodal systems. In particular, I’m interested in:\n\nRigorous evaluation of difficult-to-measure capabilities in language models and generative image systems. (COLM 2024\n\n, NeurIPS 2024 Spotlight- \n)\n\nBuilding multilingual and culturally competent generative AI systems, addressing performance disparities, bias, and unique knowledge possession. (ACL 2023, FAccT 2023 Oral, NAACL 2024)\nAdvancing multimodal (primarily vision &amp; language) generative AI systems, in particular with respect to deep semantic understanding. (EMNLP 2024, Tech Crunch coverage)\n\nIn the last year I presented four top conference publications either as the sole or a joint first author, presented in two meetings of the Association for Computational Linguistics (ACL) as well as in the first Conference on Language Modeling (COLM) and the conference on Neural Information Processing Systems (NeurIPS). This work revolved around the goal of producing meaningful evaluations of new capabilities that are difficult to measure in multimodal and text-only language models.\nAmong the most interesting results, our NeurIPS paper overturned the conventional wisdom in text-to-image model evaluation: fancy, expensive LM-based metrics fail to outperform simple correlation-based ones at actually detecting consequential differences between images (the actual domain in which they are deployed); their superiority was an illusory artifact of poor meta-evaluation.\nMy work has also been impactful outside the academy. My COLM paper was a position piece on the problems of AI system evaluation, particularly the lack of scientifically rigorous approaches to intelligence benchmarking in real-world contexts. This paper has already received considerable attention, including being referenced as a guide within an Open Philanthropy call for grant proposals for AI safety. My EMNLP paper on the limitations of needle-in-a-haystack evals for vision language models was the basis of a Tech Crunch article on deceptive advertising of corporate foundation model services."},"01-Fleeting-Notes/Get-me-off-your-fucking-mailing-list":{"slug":"01-Fleeting-Notes/Get-me-off-your-fucking-mailing-list","filePath":"01 Fleeting Notes/Get me off your fucking mailing list.md","title":"Get me off your fucking mailing list","links":["tags/complaints","tags/research-culture"],"tags":["complaints","research-culture"],"content":"complaints research-culture\n\nSelf-explanatory, this is a real article that was published by David Mazieres and Eddie Kohler in the predatory open-access International Journal of Advanced Computer Technology. Lmao.\n[Vox] [Link]"},"01-Fleeting-Notes/Glyph-language-bias-in-DALL-E-mini":{"slug":"01-Fleeting-Notes/Glyph-language-bias-in-DALL-E-mini","filePath":"01 Fleeting Notes/Glyph-language bias in DALL-E mini.md","title":"Glyph-language bias in DALL-E mini","links":["tags/writing-ideas","tags/language-model","tags/ethics"],"tags":["writing-ideas","language-model","ethics"],"content":"2022-07-01 17:34\nTags: writing-ideas language-model ethics\n\nI performed a “big dog” experiment on DALL-E mini where I queried it with a very mundane (common and presumably easy to translate) prompt in many languages:\n\nEnglish\nSpanish\nIndonesian\nJapanese\nHindi (Devanagari and roman writing)\nRussian\nArabic\nChinese  (simplified)\n\nMy number one interest was the divergent behavior between Spanish/English/Indonesian (latin only large internet languages) and Japanese (unique writing system big internet language) very weird behavior\nI’m comfortable calling the outputs consistent\nI think weird edge case representations get learned because of\n\nlatin-oriented BPE tokenizer\nenglish-dominant data distribution\n\nDALL-E Mini uses BART-finetuning to get the text-image token encdec\n\nlimited vocabulary over which images can be “described” in image tokens\nit looks like\n"},"01-Fleeting-Notes/How-PECO-differs-from-Competency-Problems":{"slug":"01-Fleeting-Notes/How-PECO-differs-from-Competency-Problems","filePath":"01 Fleeting Notes/How PECO differs from Competency Problems.md","title":"How PECO differs from Competency Problems","links":["01-Fleeting-Notes/Comptetency-Problems-Gardner"],"tags":[],"content":"\nOur emphasis is on learned features rather than the straight distribution of words/ ngrams as features\n\nComapre with: arxiv.org/abs/2203.12942, arxiv.org/abs/2009.10795\narxiv.org/abs/2104.08646\nGenerate data using GPT guided by the clustering\nComptetency Problems Gardner"},"01-Fleeting-Notes/How-to-\"how-to\"-questions-in-English":{"slug":"01-Fleeting-Notes/How-to-\"how-to\"-questions-in-English","filePath":"01 Fleeting Notes/How to \"how to\" questions in English.md","title":"How to \"how to\" questions in English","links":[],"tags":[],"content":"Asking a question that starts with “how to” is almost always wrong in English.\nYou don’t say:\n\nHow to install Linux on an M1 mac?\n\nYou should ask:\n\nHow do you install Linux on an M1 mac?\n\nEveryone knows this. I’m sure you already know this, or have read this before, even if you do still ask “how to” questions.\nI know many, many extremely smart and extremely-English-proficient people who still make this mistake, so I think it’s worth describing a bit more and examining why it happens.\nThese “how to” strings are noun phrases (NP), a linguistics term which describes nouns + modifiers which can be treated equivalently to a single noun.\nIn English, a noun phrase cannot make a complete sentence on its own. In the simplest sentences, an NP must be the subject or object of a verb. For example, this is a correct sentence:\n\nI will tell you [how to install Linux on an M1 mac].\n\nIf we only ever came across these “how to” NPs in complete sentences like this, I don’t think many people would make the “how to question” error. But there’s one important place where NPs very often appear alone: titles.\nHow to ask questions right\nHere, I just wrote a title that is a “how to” NP on its own. It is completely valid.\nTitles\nWhen we write a title to a paper or a section, it doesn’t need to be a complete sentence. If you were to convert a title into a complete sentence, you could almost always do it like this:\n[This section is about] Titles\nThus really, when we have a how to title, you should parse it as something like:\n[This section will tell you] How to remember the how to rule\nSo, putting this all together, I think there’s a simple way to remember that you shouldn’t ask a question with how to:\n\n\n                  \n                  IMPORTANT\n                  \n                \n\nA “how to” question is grammatically equivalent to its short answer. Treat an answer as an == relation.\n\n\nFor example, I am going to tell you how to find the exit.\nHow to find the exit: take the first door on the right.\nLiterally, treat\n\\textrm{&quot;How to find the exit&quot;} = NP\n\\textrm{``take the first door on the right.&quot;}=A\nNP = A\nNow, is it a natural question to ask:\n\n“Take the first door on the right?”\n\nShit, this is really hard to write lmao"},"01-Fleeting-Notes/How-to-embellish-honestly-(advice-for-phd-apps)":{"slug":"01-Fleeting-Notes/How-to-embellish-honestly-(advice-for-phd-apps)","filePath":"01 Fleeting Notes/How to embellish honestly (advice for phd apps).md","title":"How to embellish honestly (advice for phd apps)","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"blog-idea\nI notice a lot of people posting these long-ass project names in their resumes when they don’t have published research experience but need to convey that they’re familiar with a project\nThere is a style of recruiter-targeted resume embellishment that is effective.\nBut researchers screen CVs a little differently.\nOne thing that gives me mixed feelings is"},"01-Fleeting-Notes/I-HATE-LONGTERMISTS-Vol-5":{"slug":"01-Fleeting-Notes/I-HATE-LONGTERMISTS-Vol-5","filePath":"01 Fleeting Notes/I HATE LONGTERMISTS Vol 5.md","title":"I HATE LONGTERMISTS Vol 5","links":[],"tags":[],"content":"I HATE LONGTERMISTS Vol 5\n2022-04-30 22:48\nTags:\n\nSecular anti-humanism (prophetic future metahuman millenarianism) leading to literal “lives in the developed world matter more” ideology retardation\nWHY ARE PEOPLE LIKE THIS lovrepesut.com/\nlovrepesut.com/writing/alignment\nwww.lesswrong.com/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of\nLessWrong and effective altruism communities just seem to be a nexus for the most obnoxious bullshit. Experienced them first in the neolib/econ/yimby community and online tech people where they are relatively benign, but then you start getting into the gwern/slatestarcodex land of weirder navel gazing and bullshit\nWHY ARE THESE PEOPLE SO OBSESSED WITH NAVEL GAZING AND ABSURDLY FLOWERY PROSE\nReplies to this thread: twitter.com/mmitchell_ai/status/1535774664596680705\n\nWhat the fuck is this guy’s deal!?!? twitter.com/DylanoRepublic/status/1535778971324796928 He “writes” for Areo “Mag.” ANDERSONIAN!? USE NORMAL WORDS\n\nNothing language (Please just tell me what you do)\n\nSussy behavior\ntwitter.com/flotsam70272377/status/1613398615422033920\n\nBostrom N word post apology without apologizing for his post saying black people are inherently unintelligent\nLiteral crime statistics posters swoop in to defend and it seems like EAs\nas a rule, serious academics might as well just not engage with adversarial sealioning pseudonymous posters\n"},"01-Fleeting-Notes/ISI-Visit":{"slug":"01-Fleeting-Notes/ISI-Visit","filePath":"01 Fleeting Notes/ISI Visit.md","title":"ISI Visit","links":[],"tags":[],"content":"\nNeed to submit final title and abstract by Nov 1\nNeed to get list of profs to meet by ASAP\n\nwww.isi.edu/directory/xuezhema/\nwww.isi.edu/directory/smiller/\nwww.isi.edu/directory/arussell/\nwww.isi.edu/directory/jonmay/\nwww.isi.edu/directory/fredmors/\nwww.isi.edu/directory/jpujara/\nwww.isi.edu/directory/xiangren/\nwww.isi.edu/directory/ambite/\n\n\n"},"01-Fleeting-Notes/In-the-beginning-there-was-the-command-line":{"slug":"01-Fleeting-Notes/In-the-beginning-there-was-the-command-line","filePath":"01 Fleeting Notes/In the beginning there was the command line.md","title":"In the beginning there was the command line","links":["tags/bootleg","tags/writing-sample","01-Fleeting-Notes/Mark-I-Fire-Control-Computer","tags/to-read"],"tags":["bootleg","writing-sample","to-read"],"content":"bootleg writing-sample\nInteresting blogpost from Adam Elkus about a sort of interface conservatism that arises in defense of command lines contra the GUI and in defense now of linear timeline feeds vs algorithmic curation.\nSome lines I liked:\n\nIt is not uncommon for a conservative to mistake tradition for a superior model of reality. The good old ways are the best ways. Not because they are good and old, but rather because they help you see visions others cannot. This argument is not usually stated outright. It is often hidden behind homilies to common sense and injunctions to respect rituals even if one cannot understand their function.\n\nLater on he uses the phrase:\n\n[The book isn’t] really about the command line. Hymns and hosannas about the command line do take up a lot of literary real estate. But the book’s message…\n\nGod, I love everything about that sentence. “hymns and hosannas”, “literary real estate.” Will steal both.\n\nMuch of the book contains jeremiads about the imperialism and sinfulness of the GUI. ical user interfaces. Given that he wrote the book in 1999, that battle has long since been lost.\n\nAnother great one!\n\nanalog computers were essentially modeling systems. The fit of model to reality is why analog fire control machines survived in the Navy long past the introduction of digital computing.\n\n\nFascinating description of the Mark I Fire Control Computer from Wikipedia\nLinks to “nameless feeling”, a post:\n\n\nThe idea of “vibes” discourages the more difficult work of interpretation, foregrounding the idea of affect as inexplicable, ineffable\n\n\nI’ll put this down as a to-read\n\nThe “traditional” education may not guarantee creativity, self-actualization, or even sound judgment, but it is a sizable inheritance anyone willing to engage with it may claim. The value is intrinsic. You may not be rewarded for it, but you miss out on a rich source of insight and meaning if you won’t at least ingest some of it.\n\nThe CLI as this sort of tradition and learning it as a sort of “classical education” in computing"},"01-Fleeting-Notes/Interlingua-is-awesome":{"slug":"01-Fleeting-Notes/Interlingua-is-awesome","filePath":"01 Fleeting Notes/Interlingua is awesome.md","title":"Interlingua is awesome","links":[],"tags":[],"content":"Interlingua was designed to be maximally comprehensible to (Western) Europeans by being heavily based on English + the Romance languages (with secondary influence from German and Russian), see the concept of “Standard Average European languages”. Only features present in all of the control languages were allowed to become a part of Interlingua. This means no grammatical gender and simple conjugations, as well as no progressive tense, but things like plural “s”.\nThe cool part in particular was that it was designed to be a language of scientific communication, a sort of modern equivalent to how Latin was used through the Enlightenment.\nIt saw minor adoption as a secondary language of scientific publication in the early cold war years, for example the “Spectroscopia Molecular” periodical from 1952-1980 I\n(Volume 1, Issue 8).\nToday there is the magazine “Panorama in interlingua” which is still published, and similarly has a particular focus on Scientific Topics.\nI am surprised by how comprehensible it is to me. English really is a Romance language lmao."},"01-Fleeting-Notes/Is-Reinforcement-Learning-(Not)-For-NLP":{"slug":"01-Fleeting-Notes/Is-Reinforcement-Learning-(Not)-For-NLP","filePath":"01 Fleeting Notes/Is Reinforcement Learning (Not) For NLP?.md","title":"Is Reinforcement Learning (Not) For NLP?","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-01-10 00:27\nTags: paper-notes\n\nFrom AllenAI: arXiv pdf\nSummary\n\nText gen as sequential decision-making is justification for RL\n\nHowever, empirical challenges to RL for LM-based generation\n\nInstability in action space\nNo OSS libraries for this\nBegs question: is RL actually practical for this?\n\n\n\n\nHuman-in-the-loop costly\n\nAutomated metrics can be a compromise, but\n\nNot per-token differentiable\nGoodhart’s law (Measure becomes target)\nLack of open-source benchmarks\n\n\n\n\nThey introduce the RL4LMs library\nUse GRUE benchmark\n\nIMDB continuation, CommonGEN commonsense, CNNDM Summarization, data-to-text, WMT-16, NarrativeQA, DailyDialog\nScored with perplexity, SPICE, BertScore, BLEURT, etc\n\n\nKey weakness is the imprecision and simplicity of the metrics\n\nHowever, the overall ideas are solid\n\n\nStrengths:\n\nVery interesting analysis of “data budget” issues\nGather more training data for the supervising function or demonstrations?\n5x improvement in supervising function training over demonstration examples\n\nMakes sense. More usable information per-sample imo\n\n\n\n\n\nHow this ties into our direction: Further savings on the human demonstration front available if you can automate sampling\nKey person to invite is gonna be Jack Hessel"},"01-Fleeting-Notes/It's-not-the-incentives":{"slug":"01-Fleeting-Notes/It's-not-the-incentives","filePath":"01 Fleeting Notes/It's not the incentives.md","title":"It's not the incentives","links":["tags/research-culture","tags/complaints","tags/blogpost-other"],"tags":["research-culture","complaints","blogpost-other"],"content":"research-culture complaints blogpost-other\nInsightful blogpost from Tal Yarkoni\nArgues that it’s cowardly to do something unethical, antisocial, or harmful but then blame the incentive structures that reward it. Rundown:\n\nWhat I do object to quite strongly is the narrative that scientists are somehow helpless in the face of all these awful incentives—that we can’t possibly be expected to take any course of action that has any potential, however small, to impede our own career development.\n\nThis is because:\n1. Anything can be excused from appealing to the incentives.\nData fabrication, threats, sabotage, etc\n\nIf we’re not comfortable with pariahs like Stapel blaming The Incentives for causing them to fabricate data, we shouldn’t use The Incentives as an excuse for doing things that are on the same spectrum, albeit less severe. If you think that what the words “I did not withstand the pressure to score” really mean when they fall out of Stapel’s mouth is something like “I’m basically a weak person who finds the thought of not being important so intolerable I’m willing to cheat to get ahead”…\n\n2. If everyone did it things would be destroyed\n\npractitioners in other fields at least appear to have enough sense not to loudly trumpet The Incentives as a reasonable justification for their antisocial behavior—or to pat themselves on the back for being the kind of people who are clever enough to see the fiendish Incentives for exactly what they are\n\n3. You are not special\nI think this one is self-explanatory.\nIndividual success in science is zero-sum… antisociality is not victimless.\n4. You probably have no data\nClaims to the true benefits of following “the Incentives” are not strongly supported\n5. It won’t matter anyway\n\nOutcomes in academia are multiply determined and enormously complex. You can tell yourself that getting more papers out faster will get you a job if it makes you feel better, but that doesn’t make it true.\n\n\nIs it taking you from a 3% chance of getting a tenure-track position at an R1 university to an 80% chance? Almost certainly not. Maybe it’s increasing that probability from 7% to 11%; that would still be a non-trivial relative increase, but it doesn’t change the fact that, for the average grad student, there is no full-time faculty position waiting at the end of the road.\n\n6. You’re (probably) not going to “change things from the inside”\nThere’s always a hurdle to the next step that has fucked incentives until you’re emeritus and not shaping behaviors anyway.\n7. You’re not thinking long-term\n\nOne of the most frustrating aspects of appeals to The Incentives is that they almost invariably seem to focus exclusively on the short-to-medium term.\n\n8. It achieves nothing and probably makes things worse\n\ndo you think there’s any working scientist on Planet Earth who doesn’t already know that The Incentives are fucked up?\n\n9. It’s your job\nDoing science right, that is."},"01-Fleeting-Notes/LLMs-Still-Can't-Plan":{"slug":"01-Fleeting-Notes/LLMs-Still-Can't-Plan","filePath":"01 Fleeting Notes/LLMs Still Can't Plan.md","title":"LLMs Still Can't Plan","links":[],"tags":[],"content":"LLMs Still Can’t Plan\n2022-10-10 19:51\nTags:\n\nFrom the article, LLMs Still Can’t Plan by Rao’s students Karthik Valmeekam, Alberto Olmo, and Sarath Sreedharan\nRationale and setup\n\nInitially, they note that no guarantees were made about the language generated beyond LLMs apart from their coherence\nWe didn’t initially expect that they would be able to, say, explain jokes or reason\nHowever, benchmarks of reasoning have grown in interest as results suggestive of LLM reasoning capabilities have arisen\nAssumption: By learning to model the distribution of a large amount of text, LLMs pick up an approximation of simple reasoning\n\nThe authors propose a suite of reasoning benchmarks based on the International Planning Competition to test these capabilities\n\n\nIn the paper they claim to provide\n\nAn extensible suite of benchmarks for planning/reasoning evaluation of LLMs\nShow GPT-3’s poor performance on those benchmarks\n\n\nOne key way in which they improve on prior benchmarks is in having a much longer and involved prompts.\n\nThey do things like list candidate possible options\nRestrictions to the order and manner in which candidate options may be performed (e.g., affordances)\nAnd list detailed initial conditions\ntogether, these changes mean their benchmark reflects a significant increase in complexity over prior ones\nComparison benchmarks are GSM8k, SvAMP, AQuA, CommonSenseQA, StrategyQA, Tracking Shuffled Objects, Date Understanding, Last Letter Concatenation, Coin Flip\n\n\nThey find examples of simple common-sense planning tasks that are beyond LLM capabilities\n\nIt isn’t that hard to construct a reasoning task to which the correct answer is low-likelihood after all!\n\n\nThey uploaded the benchmark here\nThey build their language reasoning tasks off of blocksworld planning problems. These contain actions that can be performed, which take in parameters, have preconditions, and lead to effects, e.g. this action definition for picking up an object:\n\nThis is a simple class of planning models that doesn’t contain object types, only has simple preconditions, and doesn’t have conditional effects\n\nApproach\nThey decompose their approach into a lifted, domain-agnostic definition of component classes, and then domain-dependent components that require specific development.\n\nThey define lifted models using PDDL (Planning Domain Definition Language) and compatible structurd inputs\nDomain-dependent element converts these abstractions into domain-specific natural language and back out\n\n\nEditorializing : I think the DDC is much more critical to success than the DIC; if the model outputs aren’t properly translated for reasoning evaluation, it all breaks\nDomain Model\nLifted, describes\n\nthe available actions to solve any planning problem\npredicates that can describe relationships to objects\navailable object types\n\nProblem Generator\nNeeds to produce a domain-specific planning problem, which includes the following:\n\nDescription of objects\nInitial state\nGoal description\nValid solution\n\nTranslator\nNeeds to convert the generated test cases into natlang and the model outputs back out of natlang to evaluate performance.\nThey use a templating-based mechanism to achieve this, with a template for each:\n\npredicate\naction\nstate/plan (by concatenating the above)\n\nTest curriculum\nAt present, their testbench provides 7 different cases (domains?)\n\nGoal-directed reasoning - Can the LLM come up with valid plans that will achieve a specific goal?\nCost Optimal Planning - Can the LLM come up with plans that are optimal to achieve a specific goal?\nReasoning about plan execution - Can the LLM reason about what happens when a plan is executed?\nRobustness to goal reformulation - Can the LLM recognize the same goal when specified in different ways?\nAbility to reuse plans - Can the LLM recognize scenarios where it can reuse part or the whole of the original plan to achieve the new goal?\nReplanning - Can the LLM replan for cases where an unexpected change is reported?\nPlan Generalization - Can the LLM take specific plans, extract underlying procedural patterns and apply them to a new instance?\n\n1. and 2. correspond to real planning problems, while the others correspond to simple auxiliary tasks.\nThey ground their tasks based on colored block manipulation in the blocksworld domain. In a blocksworld problem, a set of blocks are placed either on a table or on top of each other. The goal is to arrange them in a stack in a particular order.\nBlocks may only be picked up if they are clear, i.e. have no other blocks stacked on top of them. Every prompt begins with a description of the blockworld problem space:\n\nThey provide one single prompt that demonstrates how the validly-formed steps are phrased, followed by the actual query they test:\n\nI will explain below how the various cases (2-7) differ from the first sample after first covering how the plan extraction works.\nTo do this, they use plan validator tools like: VAL: Automatic plan validation using PDDL . This is essentially a mixture of forcing a certain structure:\n\nOne action per line\nOne verb per action\nFixed action vocabulary\n\nSo that the correct actions may be inverted\nEditorializing: I believe that there are a few issues with this approach.\n\nIs there a demonstration that the failures are “reasonable” failures and not “glitches”?\nAka, is it getting structure right and failing on the understanding or just failing to grasp the correct structure?\nThe analysis section on this is a little limited.\n\nOther task descriptions\n\nOptimal planning: same as the base case, but now a cost is assigned to each action. (In this case, in time), and in the prompt they mention things like “finishing as fast as possible” or other ways to specify that time is of the essence. They also then include the cost of the plan as compared to the optimal cost in the eval\nReasoning about plan execution: answer a question about the generated sequence, seemingly of form true/false. In this case, the action sequence is provided to the model, and all it needs to do is answer questions like “Is the statement, ‘the blue block is on top of the orange block’ true?”\nRobustness to goal formulation: They provide an identical problem to the example, where all they do is reorder/rephrase the the goal, or only include a subset of the goal specification (in which case same plan would work)\nAbility to reuse plans: same as before, but the prefix of the sample is the whole solution\n\nIn the sample they provide, there is an example of an affordance error, where it tries unstacking a block off a stack that doesn’t exist (blue isn’t on top of red)\n\n\nReplanning: this one moves off into wonky territory. They introduce intermediate PLAN END tags where they describe “During execution, an unexpected event has occurred.” followed by what happened eg a block fell or something was dropped. They don’t use words like “dropped” to articulate this. This may be an unfair task\n\nThey put GPT-3 in at the “After re-planning a new state…” here\nThere may be fairer ways to prompt-engineer this one.\n\n\nPlan generalization:\n\nThey implement pseudo-examples of things like loops of repeated actions and see if they generalize to a condition where one element of init is different. These often do not work.\nSimilar wonkiness here, will elaborate further tmrw.\n\n\n\nResults\n"},"01-Fleeting-Notes/Lab-Meeting-3-12-Nick-Roberts":{"slug":"01-Fleeting-Notes/Lab-Meeting-3-12-Nick-Roberts","filePath":"01 Fleeting Notes/Lab Meeting 3-12 Nick Roberts.md","title":"Lab Meeting 3-12 Nick Roberts","links":[],"tags":[],"content":"Science of scaling laws\nAre compute optima skill-dependent?\nMeta internship project Paper link (pdf)\nSkills are described as specific capacities:\n\nKnowledge QA\nReasoning\n\nCode as a proxy for reasoning\n\n\n\nCompute needs differ between these different skills. Knowledge is “capacity hungry” while code is “data hungry”\n\nArtifact of pretraining data mix or real?\nIn other words, can we control this behavior? Yes\n\nThey do some experiments altering the data mix\nThey find that minor changes in the optimum are possible based on eg., how much code data you include in the training\n\nHowever, you can’t in principle change this story about data- vs capacity- hunger. Knowledge task optimality grows with relevant data faster than data does.\nHow should this impact pretraining?\n\nThe correlations between NLLs for different task validation set, scaling additional data to target it can lead to a drop in performance on other benchmarks.\n\n\nFuture work\nTiny scaling laws\nCan we model this stuff on smaller data/in very low-compute regimes?\nFine-grained scaling laws\n\nCan we take chinchilla and break param count into width and depth (paper from Tom Goldstein’s group)\nWhat are MoE scaling laws?\n\nMy question about how do you define tasks\nCreate “synthetics” models of the task\n\nAssociative recall task from Arora, Ré (Chris Re’s group)\nTuring machines to capture attributes like length vs compute bounding as a way to frame these sorts of things\nDeepak: how does mech interp fit into this?\n\nThere are heads that may correspond to lookup, reasoning, etc\n\n\nThey have work on “skill heads” abt this task\n"},"01-Fleeting-Notes/Letter-to-the-editor":{"slug":"01-Fleeting-Notes/Letter-to-the-editor","filePath":"01 Fleeting Notes/Letter to the editor.md","title":"Letter to the editor","links":[],"tags":[],"content":"sciencehomecoming.com/\ngannett-nxuao.formstack.com/forms/letters_editor\nI can submit to AZCentral or maybe Mesa Tribune"},"01-Fleeting-Notes/Mapping-is-not-memorization":{"slug":"01-Fleeting-Notes/Mapping-is-not-memorization","filePath":"01 Fleeting Notes/Mapping is not memorization.md","title":"Mapping is not memorization","links":["tags/paper-planning","01-Fleeting-Notes/Diffusion-Image-Memorization","01-Fleeting-Notes/Tasks-for-\"Mapping-is-not-Memorization\"","01-Fleeting-Notes/Denoising-Diffusion-Probabilistic-Models"],"tags":["paper-planning"],"content":"2023-02-03 17:47\nTags: paper-planning\nLinks: Diffusion Image Memorization Tasks for “Mapping is not Memorization” Denoising Diffusion Probabilistic Models\n\nNeed to execute fast\nDiffusion Image Memorization\narxiv.org/abs/2212.03860\ntwitter.com/atroyn/status/1622720622827819008\n\nPre-index attention maps between image latents and caption sequence for each example in training\nAuto caption target image using OFA\nTake latents from LDM attention over image and compare to KNNs in the index\ntwitter.com/KATURATION/status/1622625795960107012\ntwitter.com/culture3xyz/status/1622701726028423170\ntwitter.com/viscerina/status/1622676212358479872\ntwitter.com/98_0634741763/status/1622704264748449792\ntwitter.com/yourbestwarlock/status/1622628201129189377\n\narxiv.org/pdf/2302.01381.pdf\n\nEmailed Yao for feedback on idea\n\nNotes from original diffusion paper:\narxiv.org/pdf/2006.11239.pdf\nFactor of \\sqrt{\\alpha_T} times the source image + \\sqrt{1-\\alpha_T} ensures that the forward process at the final T step is not conditioned on the source image X\n- Ergo, stepping down the process to T from the source image must fail, no useful information is preserved\n- Can we identify a “midpoint?”\nMain paper defining stable diffusion (latent diffusion)\narxiv.org/pdf/2112.10752.pdf\nDetails on StableDiffusion training\nDuring training it’s just learning a single step at a random timestamp\nWhen we choose which to save, training script must be modified to generate the whole trajectory\n\nWe could generate the entire trajectory\nWe could also rewrite the form to an arbitrary sampling resolution\n\nCharacterizing timesteps\n\nBall of possible latents around t latent at t+1 is bounded by worst case sampleable step, \\alpha, giving brownian motion\nClosed form definition of noise at t given x, \\alpha arxiv.org/pdf/2302.02285.pdf\nNormal distribution\n\nUniversality Hypothesis\nConvergent learning 2015/6 reference for model learning same features on same task"},"01-Fleeting-Notes/Mark-I-Fire-Control-Computer":{"slug":"01-Fleeting-Notes/Mark-I-Fire-Control-Computer","filePath":"01 Fleeting Notes/Mark I Fire Control Computer.md","title":"Mark I Fire Control Computer","links":["tags/bootleg"],"tags":["bootleg"],"content":"Excerpted from Wikipedia bootleg\nThe Mark 1, and later the Mark 1A, Fire Control Computer was a component of the Mark 37 Gun Fire Control System deployed by the United States Navy during World War II and up to 1991 and possibly later.\n\nIt was originally developed by Hannibal C. Ford of the Ford Instrument Company and William Newell. It was used on a variety of ships, ranging from destroyers (one per ship) to battleships (four per ship). The Mark 37 system used tachymetric target motion prediction to compute a fire control solution. It contained a target simulator which was updated by further target tracking until it matched.\nWeighing more than 3,000 pounds (1,400 kg), the Mark 1 itself was installed in the plotting room, a watertight compartment that was located deep inside the ship’s hull to provide as much protection against battle damage as possible.\nEssentially an electromechanical analog computer, the Mark 1 was electrically linked to the gun mounts and the Mark 37 gun director, the latter mounted as high on the superstructure as possible to afford maximum visual and radar range. The gun director was equipped with both optical and radar range finding, and was able to rotate on a small barbette-like structure. Using the range finders and telescopes for bearing and elevation, the director was able to produce a continuously varying set of outputs, referred to as line-of-sight (LOS) data, that were electrically relayed to the Mark 1 via synchro motors. The LOS data provided the target’s present range, bearing, and in the case of aerial targets, altitude. Additional inputs to the Mark 1A were continuously generated from the stable element, a gyroscopic device that reacted to the roll and pitch of the ship, the pitometer log, which measured the ship’s speed through the water, and an anemometer, which provided wind speed and direction. The Stable Element would now be called a vertical gyro.\nIn “Plot” (the plotting room), a team of sailors stood around the four-foot-tall (1.2 m) Mark 1 and continuously monitored its operation. They would also be responsible for calculating and entering the average muzzle velocity of the projectiles to be fired before action started. This calculation was based on the type of propellant to be used and its temperature, the projectile type and weight, and the number of rounds fired through the guns to date.\nGiven these inputs, the Mark 1 automatically computed the lead angles to the future position of the target at the end of the projectile’s time of flight, adding in corrections for gravity, relative wind, the magnus effect of the spinning projectile, and parallax, the latter compensation necessary because the guns themselves were widely displaced along the length of the ship. Lead angles and corrections were added to the LOS data to generate the line-of-fire (LOF) data. The LOF data, bearing and elevation, as well as the projectile’s fuze time, was sent to the mounts by synchro motors, whose motion actuated hydraulic servos with excellent dynamic accuracy to aim the guns.\nOnce the system was “locked” on the target, it produced a continuous fire control solution. While these fire control systems greatly improved the long-range accuracy of ship-to-ship and ship-to-shore gunfire, especially on heavy cruisers and battleships, it was in the anti-aircraft warfare mode that the Mark 1 made the greatest contribution. However, the anti-aircraft value of analog computers such as the Mark 1 was greatly reduced with the introduction of jet aircraft, where the relative motion of the target became such that the computer’s mechanism could not react quickly enough to produce accurate results. Furthermore, the target speed, originally limited to 300 knots by a mechanical stop, was twice doubled to 600, then 1,200 knots by gear ratio changes.\nThe design of the postwar Mark 1A may have been influenced by the Bell Labs Mark 8, which was developed as an all electrical computer, incorporating technology from the M9 gun data computer as a safeguard to ensure adequate supplies of fire control computers for the USN during WW2. Surviving Mark 1 computers were upgraded to the Mark 1A standard after World War II ended.\nAmong the upgrades were removing the vector solver from the Mark 1 and redesigning the reverse coordinate conversion scheme that updated target parameters.\nThe scheme kept the four component integrators, obscure devices not included in explanations of basic fire control mechanisms. They worked like a ball–type computer mouse, but had shaft inputs to rotate the ball and to determine the angle of its axis of rotation.\nThe round target course indicator on the right side of the star shell computer with the two panic buttons is a holdover from WW II days when early tracking data and initial angle–output position of the vector solver caused target speed to decrease. Pushbuttons slewed the vector solver quickly."},"01-Fleeting-Notes/Meditation-on-feeling-useless-in-AI-research":{"slug":"01-Fleeting-Notes/Meditation-on-feeling-useless-in-AI-research","filePath":"01 Fleeting Notes/Meditation on feeling useless in AI research.md","title":"Meditation on feeling useless in AI research","links":[],"tags":[],"content":"Many people who worked in AI slightly earlier on are benefiting a lot from working on the “current thing”\nLiving through AI being “the current thing” has been exhausting.\n\nThe panic and massive influx of tourists/neophytes freaking out about xrisk etc\nThe massive amount of attention that Yudtypes have gotten, sucking the air out of the room\nThe gulf between SOTA industry shit and what feels achievable as a PhD student\nDisconnect between what I care about and the kind of work that gets rewarded\n\nventurebeat.com/ai/why-ai-is-teetering-on-the-edge-of-a-disillusionment-cliff-the-ai-beat/\nFeels like the “point” of making good text generation is so hollow: it’s just going to allow us to fill the world with so much more meaningless bullshit"},"01-Fleeting-Notes/Metrics-for-T2I-Verb-Assessment":{"slug":"01-Fleeting-Notes/Metrics-for-T2I-Verb-Assessment","filePath":"01 Fleeting Notes/Metrics for T2I Verb Assessment.md","title":"Metrics for T2I Verb Assessment","links":[],"tags":[],"content":"Problem with TIFA is that the in-context examples are all faithful\nNovelty is finding relational hallucination between objects and correcting\ngithub.com/BradyFU/Woodpecker\nA/B test on natural image + prompt ⇒ generate new img = new generated image from prompt that is potentially unfaithful\nbrowse.arxiv.org/pdf/2306.14060.pdf\nFaithfulRefCOCO\nopenreview.net/pdf\nFour directions:\n\nVQA (eg, TIFA)\nObject Detection (MDTER, direct run of an object detection method)\nSegmentation\nImage description model (captioning, extract structured description, and then handle with LLM etc)\n\nbrowse.arxiv.org/pdf/2308.06394.pdf\n\nOther work does hallucination shit\nCan we characterize different hallucinations\n\nObject hallucination\nAttribute hallucination\nRelational hallucination\nVerbal hallucination\n\n\nJust searching through a bunch of papers with “mitigating hallucination in vision-language”\n\nI forgot to mention the POPE paper:\ngithub.com/RUCAIBox/POPE\narxiv.org/abs/2305.10355\nNiket:\n\nQuestions that bring out the diff faithfully?\nE2E RL to get a policy that generates those good diffs\nAnd getting these from extracted information\nAugmenting TIFA with questions that lead toward reasoning\nExtend to “is it safe?”\nFocus on safety is a big deal for the community\nFor self-correcting the vision side might grow\nGDCE Generate Discriminate Critique Edit\nThese different benchmarks\n\n\nLooking at alignment to papers I’ve read and cited and focused on\nWhere do I see opportunities to transfer what I’ve worked on to somewhere else\nPropose something out of the box that appeals\n\nExample (I want to work on these multimodal models that deal with scientific diagrams)\n\n\n\nMaking future NLI:\n\nAdding a “contested” label\nFinding HARD examples, so hard even humans may disagree\nMultimoda\n\nMultilinguality in T2I:\n\nLook for word2vec-like relational semantic geometry as evidence for “alignable” Emu-like finetuning\n"},"01-Fleeting-Notes/Michael-Jordan-Campus-Visit-Notes":{"slug":"01-Fleeting-Notes/Michael-Jordan-Campus-Visit-Notes","filePath":"01 Fleeting Notes/Michael Jordan Campus Visit Notes.md","title":"Michael Jordan Campus Visit Notes","links":["tags/visitor","tags/talk-notes"],"tags":["visitor","talk-notes"],"content":"2023-01-17 16:32\nTags: visitor talk-notes\n\nTalk 1\nan engineering field:\n\nscaling new possibilities with building blocks from fundamental sciences\ncomparable to chemE and EE from scientific principles\nQ: It seems like we’re not building the field on scientific principles that are as fundamental\nunique problem: - empirical findings without theory\n\nhow to transition from alchemy to chemistry\n\n\n\nHis A to my Q:\n\nGood example from this maket-level thinking\n\namazon is able to close the empirical and theoretical gap in massive supply chain and deliveries throughout the world\nThis is a domain where it has emerged\nHealthcare, “people would like it to emerge” (it hasn’t)\n\n\nGood point that it isn’t divorced in all subregions (I am DL fixated)\n“This is the most exciting engineering field to be building because humans are so inherently involved”\nThis is where the “economic ideas” and “data flows at planetary scale” fit in\n\nThe issue\nInteresting point too:\n\ndecisions on competition\nuber accidentally makes fastest route slowest by sending people that way\n“Who do you choose? How do you load balance?”\n\nthis is what SV gets wrong: “we know everything about you”\n\n\n\nWe don’t have “dynamical systems that converge toward multidimensional, multiagent, long-term optima”\n\nMARKETS are agents that achieve this\nthis kind of thinking should be involved in our thought about AI\n\nExamples of his work on this\nHe then presents 4 research ‘vignettes’\n\nTijana Zrnic, Eric Mazumdar\n\n\nDecision making in face of strategic behavior\nProblem: Goodhart’s law ⇒ feedback loop in learning\nstrategic agents will distort data to gain favorable outcomes out of data-driven decision maker “Stackelberg Game”\n\n\n(Forget authors)\n\n\nThe theory of incentives\nAirlines provide the options to pay for business class and that’s how they figure out how much people are willing to pay\n“Contract theory meets Neyman-Pearson”\n\n\nBreakthrough in uncertainty quantification\n\n\nconformal prediction: Monotonic loss function lambda gives to choose a “nested set-predictor” based on preference for false positives or false negatives\nWithout the ground truth, how do you turn that knob to find the right param?\nThey propose a new theorem to define a desired stopping condition to calibrate automatically within some acceptable error lambdahat\n\nTalk 2\nOn the decision-making side of ML\n“minimizing the price of anarchy”: how do you do that computationally?\n“Fixed-point people”\n\nHow to escape saddle points efficiently\n\n\nnonconvex optim: “can we say anything about them?”\n\ntrad persp: “they’re np-hard, no you can’t”\n“turns out local minima aren’t as much of a problem as we’d thought”\nBUT saddle points are\nIf you spend too much time trying to get out of a saddle point you’re “stuck there forever”\n\n\n\nOptimization via gradient descent:\n\nConvex case: obviously will descend to global min, via dimension-free gradient iteration\n\n\nVariational, hamiltonian, symplectic perspectives on acceleration\n“now we’re in control land”\n\n“understand these optimizers as dynamical systems”\n“optimization doesn’t have a variational perspective (it only has differentials, no integrals)”\nproject objective: to achieve this\nClassical GD gets a convergence rate O(1/k)\nLower bound was found for a faster of O(1/k^2) (Nemorovsky)\n“Trying to find a Lagrangian for the accelerated methods of gradient descent”\n“General second-order equation with a term depending on damping and a term depending on the auxiliary function”\n“Under ideal scaling, the E-L equation has convergence rate”\nf(X_t) - f(x*) \\leq O(e^{-\\beta t})\nLots of stuff on his website recent works"},"01-Fleeting-Notes/Michael-Lore":{"slug":"01-Fleeting-Notes/Michael-Lore","filePath":"01 Fleeting Notes/Michael Lore.md","title":"Michael Lore","links":[],"tags":[],"content":"William’s academic family tree sites.cs.ucsb.edu/~william/misc.html\nIncludes the claim that our academic genealogy includes James Clerk Maxwell. As based as that would be it is unfortunately probably not accurate. Maxwell is actually our academic first cousin, 10x removed. Not as cool but still interesting. And of course we still have Poisson, Lagrange, Bernoulli, Liebniz, and Isaac Newton. Of course being a mathematician descended from Newton is kind of like being a Mongolian descended from Genghis Khan, this isn’t exactly finding our Kunta Kinte.\nDeep lore (true fun facts):\n\nI’m in a very big youtube video (hahahahahaha you are ugly)\nI created the lomgbois meme\n\nRegular lore:\n\nMilitary brat\n\nAmong the places I lived before age 5: Florida, Texas, Alaska, Arizona\n\n\nInteresting sunburns\nMy “if free, take” policy in early undergrad has led to me maybe technically being a member of Nichiren Shoushuu\n\n"},"01-Fleeting-Notes/Muhao-Talk":{"slug":"01-Fleeting-Notes/Muhao-Talk","filePath":"01 Fleeting Notes/Muhao Talk.md","title":"Muhao Talk","links":["tags/visitor","tags/talk-notes"],"tags":["visitor","talk-notes"],"content":"2023-02-10 15:22\nTags: visitor talk-notes\n\n\nCo-regularized knowledge distillation\ntrain multiple models, enforce a consistency loss between them, this increases robust of a randomly selected model to noisy samples\n“Learning from noisy labels for entity-centric information extraction”\n"},"01-Fleeting-Notes/Multi-Scales-(sic)-data-augmentation-for-NLI":{"slug":"01-Fleeting-Notes/Multi-Scales-(sic)-data-augmentation-for-NLI","filePath":"01 Fleeting Notes/Multi-Scales (sic) data augmentation for NLI.md","title":"Multi-Scales (sic) data augmentation for NLI","links":["tags/paper-notes"],"tags":["paper-notes"],"content":"2023-01-07 22:44\nTags: paper-notes\n\nZhenyuan Lu, UT Austin\nwww.semanticscholar.org/reader/38c558b924217f53cb0bad86df36ac455f40f8cd\narXiv pdf\n\nCore issue for them is also “understanding” dataset artifacts\nThey discuss dataset artifacts in the abstract and don’t center ssc acc issues\nSpend more time talking about checklist\nIt seems like most of what they do is just training ELECTRA to implement hypothesis-only SSC and PSC SNLI, and then run Checklist on\nThis is basically just an arxiv dump of a class project or something\n"},"01-Fleeting-Notes/My-envy-over-academic-twitterbrags":{"slug":"01-Fleeting-Notes/My-envy-over-academic-twitterbrags","filePath":"01 Fleeting Notes/My envy over academic twitterbrags.md","title":"My envy over academic twitterbrags","links":[],"tags":[],"content":"My envy over academic twitterbrags\n2022-04-23 19:17\nTags:\n\nContrapoints points out social media envy in Envy.\n\nWanting what others have but we cannot get… dark… begrudging them what they have [paraphrased]\n\nShe connects to the incels example but I actually think it’s very relatable in how I react to seeing bragposts especially on twitter, and things about getting into better phd programs, academic jobs, etc.\nFind myself justifying why I’m not as good; eg. not preparing as much, etc"},"01-Fleeting-Notes/NLI+-for-CoT":{"slug":"01-Fleeting-Notes/NLI+-for-CoT","filePath":"01 Fleeting Notes/NLI+ for CoT.md","title":"NLI+ for CoT","links":["01-Fleeting-Notes/CoT-Analysis"],"tags":[],"content":"arxiv.org/pdf/2302.08577.pdf\nCoT Analysis"},"01-Fleeting-Notes/NLI+-for-LLM-Eval":{"slug":"01-Fleeting-Notes/NLI+-for-LLM-Eval","filePath":"01 Fleeting Notes/NLI+ for LLM Eval.md","title":"NLI+ for LLM Eval","links":["tags/paper-planning","01-Fleeting-Notes/Natural-Language-Deduction-with-Incomplete-Information","01-Fleeting-Notes/Partial-input-baselines-show-that-NLI-models-can-ignore-context,-but-they-don't","01-Fleeting-Notes/Explaining-Answers-with-Entailment-Trees"],"tags":["paper-planning"],"content":"2023-04-03 11:11\nTags: paper-planning\n\nMet with Alisa 4/3 to brainstorm\ndocs.google.com/document/d/1rk2RHBlrGf3fSt0Q_sjfY3T6SRt7M15xQCtWbqOByRM/edit#heading=h.k8i2fw84fiwq\nOutputs:\n\nAugmentations of existing NLI datasets need to be justified\nQuality of the NLI model needs to be solid for LLM-eval using it to be well-motivated\nWANLI+RoBERTa-large has problems on long context\nRyo Kamoi from Durret’s group at UT produced a document-level NLI dataset\n\nPECO intervention to combine this + WANLI + other existing benchmarks\n\n\nWould this model perform better for the evaluation we want?\n\nI think we have all the pieces in place for doing an impactful paper:\n\nWANLI + WICE + PECO = strongest possible NLI model for our domain\nWikiWhy = dataset for generating the eliciting questions\nOutput: primordial version of an eval for reasoning+COT in LLMs\n\nNatural Language Deduction with Incomplete Information\nPartial-input baselines show that NLI models can ignore context, but they don’t\nPerformance Impact Caused by Hidden Bias of Training Data in RTE (2018)\nExplaining Answers with Entailment Trees (Dataset, 2021)\nXinlu working on privacy preserving training/finetuning of small LM for interaction with LLM for medical applications\n\nQuite good results with extracting keywords from the question, and giving those to the LLM to generate context\nAccuracy of these questions are low because you have keywords only\nShe has tested on a standard medQA benchmark\n"},"01-Fleeting-Notes/Name-Agreement":{"slug":"01-Fleeting-Notes/Name-Agreement","filePath":"01 Fleeting Notes/Name Agreement.md","title":"Name Agreement","links":[],"tags":[],"content":"2-29-24\nFrom the mtg with linguistics\nask Laurel Brehm"},"01-Fleeting-Notes/Naming-the-encoder-and-decoder-elements-of-LMs":{"slug":"01-Fleeting-Notes/Naming-the-encoder-and-decoder-elements-of-LMs","filePath":"01 Fleeting Notes/Naming the encoder and decoder elements of LMs.md","title":"Naming the encoder and decoder elements of LMs","links":["tags/language-model","tags/nomenclature","tags/complaints","References/@wang2022What"],"tags":["language-model","nomenclature","complaints"],"content":"Naming the encoder and decoder elements of LMs\n2022-04-22 20:40\nTags: language-model nomenclature complaints\nLinks: @wang2022What\n\nI appreciate how @wang2022What introduce causal/non-causal as the ways to describe the encoder-only and decoder-only models; a decoder isn’t really “decoding” from nothing, this is a welcome change."},"01-Fleeting-Notes/Natural-Language-Deduction-with-Incomplete-Information":{"slug":"01-Fleeting-Notes/Natural-Language-Deduction-with-Incomplete-Information","filePath":"01 Fleeting Notes/Natural Language Deduction with Incomplete Information.md","title":"Natural Language Deduction with Incomplete Information","links":["tags/paper-notes","01-Fleeting-Notes/Explaining-Answers-with-Entailment-Trees"],"tags":["paper-notes"],"content":"2023-04-03 20:19\nTags: paper-notes\n\nNLD with Incomplete Information (2022)\nDurrett’s group, Nov 2022\nUses EntailmentBank (Explaining Answers with Entailment Trees) for eval\nIdea I just had: even feeding recontextualized GPT4 CoT outputs back in to GPT4 for further analysis and self-reflection might be better than relying on it to self-coordinate: limitations to the activation space will constrain how much of its capabilities it can coordinate at once\n“Abductive reasoning”: the materialization of new knowledge can be thought of as an example of generating an explanation conditioned on premise and conclusion"},"01-Fleeting-Notes/Notes-from-Text-is-the-Universal-Interface":{"slug":"01-Fleeting-Notes/Notes-from-Text-is-the-Universal-Interface","filePath":"01 Fleeting Notes/Notes from Text is the Universal Interface.md","title":"Notes from Text is the Universal Interface","links":["tags/blogpost-notes","tags/disagree"],"tags":["blogpost-notes","disagree"],"content":"2023-02-05 16:46\nTags: blogpost-notes\n\nRoon Blogpost\nRelates the Unix philosophy of treating text as the universal interface to LLM dominance on many general tasks\nProse is a bit flowery for my taste.\n\nThe gauntlet these leviathans get trained on embeds some dificult behaviors\n“Vast ensemble of many models that play many characters”\n\n“find the prompt, don’t write the program” related to unix programs\n\nThe inexorable scaling laws of deep learning models work in [the LLM paradigm]‘s favor.\n\n\nI think I disagree with this take. Running out of orders of magnitude\n"},"01-Fleeting-Notes/Notes-from-William-11-17":{"slug":"01-Fleeting-Notes/Notes-from-William-11-17","filePath":"01 Fleeting Notes/Notes from William 11-17.md","title":"Notes from William 11-17","links":[],"tags":[],"content":"In diffusion when you are adding this noise, we don’t know how that effects the regular gradient descent in machine learning…\nThe debate on twitter Yann Kai-wei Amina\n\nBias from data only\nBias amplified by the model training process\n\nEmpirical element of this paper is inevitable, but interesting theoretical stuff\nPercy Liang uploaded evaluating LLMs today\nSharon’s Amazon paper was multilingual bias in sentiment analysis\n\nHer approach shows the inconsistency\n\nImage comparison via:\n\nHuman evaluation\nFiD\nCLIP embedding extractor\nConsider an automation evaluation metric\n\nWilliam thinks the “amplification of bias in learning from a sampling-based process for sampling-based generation” is the novel direction\nPlan by Monday."},"01-Fleeting-Notes/Notes-from-the-William+ERSP-11-16":{"slug":"01-Fleeting-Notes/Notes-from-the-William+ERSP-11-16","filePath":"01 Fleeting Notes/Notes from the William+ERSP 11-16.md","title":"Notes from the William+ERSP 11-16","links":[],"tags":[],"content":"\nWilliam wants to keep it in the GPT3 paradigm instead of GPT-J (smaller, worse) (although idk, Neo is good and prompting is worse research paradigm imo)\n\nPrompt-to-prompt with stable diffusion\n\nAble to do semantic edits on an image\n\nE.g., picture of guy setting up a tent at a campsite, we can replace the tent with a car with his supplies in the back\nChain of semantic image edit\n\n\nGood results from DALLE2 generating the outputs directly from the intermediate story steps\n\nWilliam brings up “pseudorelevance feedback” from IR community as an analog to what we’re doing with text-image in-context learning enrichment\nWilliam wants us to discuss with Wanrong: her project is compressing the steps in the image generate step (midjourney has multistep prompt edits tracked, and all the intermediate images that were generated) she may be aware of tools to doing this\n“Maintaining local and global context”\n“Parallel chain of thought:” we have a text chain and an image chain that don’t have to be ‘constrained’ together, they can just be ‘complementary’\n\nAlso interested in the in-between images\nMore experiments needed:\n\nIf you generate a longer chain, what happens\nEmpirical measurement of the performance costs, assessment of the quality impact\netc\n\n\n"},"01-Fleeting-Notes/OSHW":{"slug":"01-Fleeting-Notes/OSHW","filePath":"01 Fleeting Notes/OSHW.md","title":"OSHW","links":["LLMops-collab"],"tags":[],"content":"Talk from OpenHW group guy\nLicensing stuff\n\nApache into hardware needs new shit\n\nVerification problems\n\nClosing the gap for putting uni-designed stuff\nStandard languages, tooling, etc\n\nGoogle random instruction generator\n“NASA’s technology readiness scale” www.nasa.gov/directorates/heo/scan/engineering/technology/technology_readiness_level/\n\nWe don’t even come close to doing this kind of shit in LLMs etc LLMops collab\nTalk was a little more high-level than I was hoping for…\nHow does the lessons from industry uptake/higher quality hardware from OS carry over to our topic?\n“Linux is a cancer” → Windows &lt;3 Linux\n\nIBM gives massive valuation to RedHat\n"},"01-Fleeting-Notes/Ofir-Press-benchmarking-talk":{"slug":"01-Fleeting-Notes/Ofir-Press-benchmarking-talk","filePath":"01 Fleeting Notes/Ofir Press benchmarking talk.md","title":"Ofir Press benchmarking talk","links":["tags/research-evaluation","tags/talk-notes"],"tags":["research-evaluation","talk-notes"],"content":"research-evaluation talk-notes\nYoutube link\nOfir gave a presentation on his evaluation stuff for SWE-bench.\nAgree strongly with goals for his evals: hard x verifiable x useful. Who cares about IQ tests?\nInteresting oral history:\n\nSWE-bench went viral because a startup chose to adopt it for their press release after 2mo of nothing\nThey then rushed to push out SWE-agent to do it\n\nStarting around 46:00 discuss the future directions:\n\nCan we advance autonAI in areas where auto verification is hard or impossible?\n\nSub-objectives that are verifiable\nSimulators\n\n\nHow can we measure and drive progress in human-AI collab domains?\n\nHCI relevant\n\n\nCan we surpass human performance?\n"},"01-Fleeting-Notes/Over-squashing-and-bottlenecks":{"slug":"01-Fleeting-Notes/Over-squashing-and-bottlenecks","filePath":"01 Fleeting Notes/Over-squashing and bottlenecks.md","title":"Over-squashing and bottlenecks","links":["@topping2022UNDERSTANDING"],"tags":[],"content":"Over-squashing and bottlenecks\n2022-04-29 15:51\nTags:\n\n@topping2022UNDERSTANDING\nMessage passing paradigm has “over-squashing” problem where messages from distant nodes have their information distorted\nMessage passing:\n\nlearnable non-linear functions diffuse info in graph\nGCN and GAT (popular GNN frameworks) are posed as flavors of this scheme and considered instances of more general geom DL framework\nDrawbacks have been formalized, like oversmoothing and limits of expressive power\n\nOversquashing\n\nDistortion of messages from distant notes is less understood\nOne proposed way to fix is reducing the “bottleneck”\n\nthis topological framing is still not so well understood, so we look at general message passing NNs (MPNNs)\nDefined in terms of an adjacency mat A, update function phi, and message function family psi, the l+1th hidden layer output h is\n\nLong-range dependencies in MPNNs exist when the output depends on represenations from distant nodes interacting.\n\nIf they exist, they need to be propagated across the network without distortion\nThe problem is that the size of the receptive field of a node grows exponentially with layer count (r), forcing exponentially more information to be compressed in a fixed code size\n\n\nAuthors propose using the Jacobian of hidden layers wrt x to formally assess over-squashing\n\n\n\nIf the MP function and update functions have bounded derivatives, then the propagation of messages is controlled by a suitable power of Ahat.\nThe Jacobian (RHS) that measures over-squashing is related to graph topology via powers of the augmented normalized adjacency matrix (I do not understand why lmao)\n\nGraph Curvature\nThey define the Balanced Forman curvature, a metric that captures the local curvature of a region in a graph (from the geometric notion, hyperbolic⇒parallel lines pull apart⇒information lost⇒negative curvature on graph)\nIn the neighborhood of an edge ij:, they count the triangles containing that edge, the vertex neighbors forming a 4-cycle based at ij without diagonals inside, and the maximal number of 4-cycles based at ij traversing a common node\n\n\nIf i~j is a “bridge” between the 1-neighbors of i and j, curvature on that edge is negative, else it is positive, and the edges stay connected even if i and j are removed.\nThey then show that if this Ric(i,j) is lower-bounded at every point by a positive number, the curvature is positive everywhere, and the receptive field of each node will be polynomial in a hop-distance, there fore the bottleneck effect will not play a “crucial role”\nThey use this to show that negatively-curved edges are the source of the over-squashing problem, using an epsilon-delta proof to demonstrate bounds on the gradients of the transition and update functions, Ric(i,j) upper bounded by -2+delta, then there exists a Q that fixes a bound on the layerwise jacobian , which\n\nimplies that if we  have a negatively curved edge as in (ii), then there exist a large number of nodes k such that GNNs---on average---struggle to propagate messages from i to k in two layers despite these nodes k being at  distance 2 from i. In this case the over-squashing occurs as measured by the Jacobian in equation 4  and hence the propagation of information suffers.\n\nConnection to Cheeger Constant\nThey further connect their curvature idea directly to spectral graph theory by showing the Cheeger constant (spectral gap) \\lambda_1 is bounded by Ric\n\nRewiring with Curvature\nThe authors finally propose using their technique to augment the input graph with additional network passing edges (which has been previously proposed) in a novel way---reduing bottlenecks by adding edges between nodels k and l that maximize gain in Ric(i,j) on minimal edges, and remove high Ricci curvature edges to convergence\n\n\nAdds edges to “support” the negatively-curved edge, taking them away from places where they aren’t needed in the message passing graph\nCompare efficacy against random-walk-based rewiring\n\nAuthors suggest a weakness of these methods is they mostly smooth across short diffusion distances\nThis might not help with larger structural problems in bottlenecks\nThey show this with more math I’m frankly not very interested in\nThey also demonstrate that structure is better-preserved under the bottleneck-targeting rewiring approach due to its surgical nature, something I’m more inclined to believe\n\n\n\nResults\nAcross a variety of tasks they find that unsupervised node classification models perform better under the graphs rewired with SDRF, particularly on low-homophily datasets, whereas more heterophilic datasets (adjacent nodes have different labels) performance is worse, as noise actually gets injected\n\n"},"01-Fleeting-Notes/Overconfidence-in-Deep-Learning-Discussions":{"slug":"01-Fleeting-Notes/Overconfidence-in-Deep-Learning-Discussions","filePath":"01 Fleeting Notes/Overconfidence in Deep Learning Discussions.md","title":"Overconfidence in Deep Learning Discussions","links":[],"tags":[],"content":"2022-10-07 19:40\nTags:\n\nKey points from AI Snake Oil substack post on DL overconfidence by Arvind Narayanan and Sayash Kapoor\n\nGrudges and dogma\n\nMany DL guys lived through dismissiveness, and now project that onto all criticism of DL can’t do X\nThis is coupled with a genuine belief in the equivalence of problems, just gimme labeled data!\n\n\nLeads to neglect of domain expertise\n\nThe deskilling of domain expertise\n\nFrom CHI, discussion of how the collectors of data in AI dev are viewed by the developers\nWe are guilty of this in using crowdworking platforms\n\n\nRich Sutton’s bitter lesson essay similarly makes a strong argument to the “fire a linguist, accuracy goes up” view\n\nReflective of the strong belief in the AI community that attempts to add domain expertise reduce performance in general\nAuthors rebut this thru appeal to the main examples Sutton uses: Go, CV, chess, and NLP.\n\nGo and Chess are both highly circumscribed and have clear ground truths\nIn CV and NLP, our results have only been demonstrated rigorously on tasks like object recognition and bounded classification where the tasks are highly circumscribed\n\n\nFrom benchmarks to the real world shows breakdown in Sutton’s argument\n\nOur benchmark dataset performances are simple, one-dimensional views into capabilities\nHarmful stereotype propagation\nHaphazard dataset construction\nDoesn’t test corner cases\n\n\n\n\nThis contempt is also mixed with an ignorance of what domain experts do\nAI developers conceived of workers as corrupt, lazy, non-compliant, and as datasets themselves\n\n\nThe “penumbra of AGI hype”\n\nGwern: The Scaling Hypothesis is an example of focus on belief in the community that we have everything we need to get AGI.\nThis is also reflected in, eg, how OpenAI talks about their mission, the manner in which they pitted the RL against LM directions, etc\n\n\nThe use of the term “errors of extrapolation” is a really strong one.\n"},"01-Fleeting-Notes/Papers-for-subjective-reward-modeling-in-GenAI":{"slug":"01-Fleeting-Notes/Papers-for-subjective-reward-modeling-in-GenAI","filePath":"01 Fleeting Notes/Papers for subjective reward modeling in GenAI.md","title":"Papers for subjective reward modeling in GenAI","links":[],"tags":[],"content":"ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\n\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong\n\n\nNeural Information Processing Systems 2023\n\n\nopen paper page\n \n    Abstract \n   We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL&#039;s advantages over compared methods. All code and datasets are provided at \\url{github.com/THUDM/ImageReward}.\n\n\n\nRobust Preference Learning for Storytelling via Contrastive Reinforcement Learning\nLouis Castricato, Alexander Havrilla, Shahbuland Matiana, M. Pieler, Anbang Ye, Ian Yang, Spencer Frazier, Mark O. Riedl\narXiv.org 2022\nopen paper page\n \n    Abstract \n   Controlled automated story generation seeks to generate natural language stories satisfying constraints from natural language critiques or preferences. Existing methods to control for story preference utilize prompt engineering which is labor intensive and often inconsistent. They may also use logit-manipulation methods which require annotated datasets to exist for the desired attributes. To address these issues, we first train a contrastive bi-encoder model to align stories with corresponding human critiques, named CARP, building a general purpose preference model. This is subsequently used as a reward function to fine-tune a generative language model via reinforcement learning. However, simply fine-tuning a generative language model with a contrastive reward model does not always reliably result in a story generation system capable of generating stories that meet user preferences. To increase story generation robustness we further fine-tune the contrastive reward model using a prompt-learning technique. A human participant study is then conducted comparing generations from our full system, ablations, and two baselines. We show that the full fine-tuning pipeline results in a story generator preferred over a LLM 20x as large as well as logit-based methods. This motivates the use of contrastive learning for general purpose human preference modeling.\n\n\n\nCreativity and Machine Learning: A Survey\nGiorgio Franceschelli, Mirco Musolesi\nACM Computing Surveys 2021\nopen paper page\n \n    Abstract \n   There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.\n\n\n\nRAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment\nHanze Dong, Wei Xiong, Deepanshu Goyal, Shizhe Diao, Jipeng Zhang, Kashun Shum, T. Zhang\nTrans. Mach. Learn. Res. 2023\nopen paper page\n \n    Abstract \n   Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.\n\n\n\nMusicRL: Aligning Music Generation to Human Preferences\nGeoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zalán Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem, O. Pietquin, Matthieu Geist, L’eonard Hussenot, Neil Zeghidour, A. Agostinelli\nInternational Conference on Machine Learning 2024\nopen paper page\n \n    Abstract \n   We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as&quot;upbeat work-out music&quot;can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.\n\n\n\nBeyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation\nKatherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, K. Dvijotham\nAAAI/ACM Conference on AI, Ethics, and Society 2024\nopen paper page\n \n    Abstract \n   Human feedback plays a critical role in learning and refining reward models for text-to-image generation, but the optimal form the feedback should take for learning an accurate reward function has not been conclusively established. This paper investigates the effectiveness of fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment, compared to traditional coarse-grained feedback (for example, thumbs up/down or ranking between a set of options). While fine-grained feedback holds promise, particularly for systems catering to diverse societal preferences, we show that demonstrating its superiority to coarse-grained feedback is not automatic. Through experiments on real and synthetic preference data, we surface the complexities of building effective models due to the interplay of model choice, feedback type, and the alignment between human judgment and computational interpretation. We identify key challenges in eliciting and utilizing fine-grained feedback, prompting a reassessment of its assumed benefits and practicality. Our findings -- e.g., that fine-grained feedback can lead to worse models for a fixed budget, in some settings; however, in controlled settings with known attributes, fine grained rewards can indeed be more helpful -- call for careful consideration of feedback attributes and potentially beckon novel modeling approaches to appropriately unlock the potential value of fine-grained feedback in-the-wild.\n\n\n\nControllable Neural Story Plot Generation via Reward Shaping\nAnimesh Mehta, Mark O. Riedl, Brent Harrison, Lara J. Martin, Murtaza Dhuliawala, Pradyumna Tambwekar\nInternational Joint Conference on Artificial Intelligence 2018\nopen paper page\n \n    Abstract \n   Language-modeling--based approaches to story plot generation attempt to construct a plot by sampling from a language model (LM) to predict the next character, word, or sentence to add to the story. LM techniques lack the ability to receive guidance from the user to achieve a specific goal, resulting in stories that don&#039;t have a clear sense of progression and lack coherence. We present a reward-shaping technique that analyzes a story corpus and produces intermediate rewards that are backpropagated into a pre-trained LM in order to guide the model toward a given goal. Automated evaluations show our technique can create a model that generates story plots which consistently achieve a specified goal. Human-subject studies show that the generated stories have more plausible event ordering than baseline plot generation techniques.\n\n\n\nFailures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models\nSom Sagar, Aditya Taparia, Ransalu Senanayake\nInternational Conference on Machine Learning 2024\nopen paper page\n \n    Abstract \n   In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model&#039;s failure. In this paper, we introduce a post-hoc method that utilizes \\emph{deep reinforcement learning} to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks.\n\n\n\nGenerative Reward Models\nAlon Albalak, Dakota Mahan, Jan-Philipp Franken, Chelsea Finn, Duy Phung, Chase Blagden, Nathan Lile, Louis Castricato, Rafael Rafailov\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.\n\n\n\nReinforcement Learning With LLMs Interaction For Distributed Diffusion Model Services\nHongyang Du, Ruichen Zhang, D. Niyato, Jiawen Kang, Zehui Xiong, Shuguang Cui, Xuemin Shen, Dong In Kim\nunknown 2023\nopen paper page\n \n    Abstract \n   Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users&#039; subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.\n\n\n\nNo Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\nXin Eric Wang, Wenhu Chen, Yuan-fang Wang, William Yang Wang\nAnnual Meeting of the Association for Computational Linguistics 2018\nopen paper page\n \n    Abstract \n   Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.\n\n\n\nIs Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization\nShuo Yang, Gjergji Kasneci\nInternational Conference on Language Resources and Evaluation 2024\nopen paper page\n \n    Abstract \n   Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.\n\n\n\nPlot and Rework: Modeling Storylines for Visual Storytelling\nChi-Yang Hsu, Yun-Wei Chu, Ting-Hao ‘Kenneth’ Huang, Lun-Wei Ku\nFindings 2021\nopen paper page\n \n    Abstract \n   Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST, a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline. PR-VIST then takes this path and learns to generate the final story via an iterative training process. This framework produces stories that are superior in terms of diversity, coherence, and humanness, per both automatic and human evaluations. An ablation study shows that both plotting and reworking contribute to the model&#039;s superiority.\n\n\n\nThe Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization\nSian Gooding, Hassan Mansoor\narXiv.org 2023\nopen paper page\n \n    Abstract \n   Reinforcement Learning from Human Feedback (RLHF) can be used to capture complex and nuanced properties of text generation quality. As a result, the task of text summarization has been identified as a good candidate for this process. In this paper, we explore how preference agreement impacts the efficacy of RLHF for summarization. We show that sampling human preferences to include a range of annotator agreement results in (1) higher accuracy reward models and (2) alters the characteristics of quality captured. We additionally show improvements in downstream generation when using a reward model trained with a range of preference agreements. Our contributions have implications for the design of synthetic datasets as well as the importance of considering quality differentials in comparison-based data.\n\n\n\nConfidence-aware Reward Optimization for Fine-tuning Text-to-Image Models\nKyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, K. Dvijotham, Jinwoo Shin, Kimin Lee\nInternational Conference on Learning Representations 2024\nopen paper page\n \n    Abstract \n   Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts. We demonstrate that incorporating the confidence-calibrated rewards in fine-tuning effectively reduces overoptimization, resulting in twice as many wins in human evaluation for text-image alignment compared against the baseline reward models.\n\n\n\nA Survey on Human Preference Learning for Large Language Models\nRuili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang\narXiv.org 2024\nopen paper page\n \n    Abstract \n   The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.\n\n\n\nMAP: Multi-Human-Value Alignment Palette\nXinran Wang, Qi Le, Ammar Ahmed, Enmao Diao, Yi Zhou, Nathalie Baracaldo, Jie Ding, Ali Anwar\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP&#039;s ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.\n\n\n\nFine-tuning of diffusion models via stochastic control: entropy regularization and beyond\nWenpin Tang\narXiv.org 2024\nopen paper page\n \n    Abstract \n   This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024). The idea is to use stochastic control for sample generation, where the entropy regularizer is introduced to mitigate reward collapse. We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.\n\n\n\nCOS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models\nTobias Materzok\narXiv.org 2025\nopen paper page\n \n    Abstract \n   We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks. The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions. This iterative reinforcement learning loop systematically explores multiple candidate plot branches, backpropagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling. In small-scale tests with short-story prompts, 67%-77% of participants favored COS(M+O)S&#039;s highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns. GPT-4o ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B. Nevertheless, absolute story quality remains modest, constrained by the small model&#039;s capacity and limited training data.\n\n\n\nCurriculum Direct Preference Optimization for Diffusion and Consistency Models\nNiculae Sebe, Florinel-Alin Croitoru, Radu Tudor Ionescu, Vlad Hondru, Mubarak Shah\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). In this paper, we propose a novel and enhanced version of DPO based on curriculum learning for text-to-image generation. Our method is divided into two training stages. First, a ranking of the examples generated for each prompt is obtained by employing a reward model. Then, increasingly difficult pairs of examples are sampled and provided to a text-to-image generative (diffusion or consistency) model. Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs. In other words, we use the rank difference between samples as a measure of difficulty. The sampled pairs are split into batches according to their difficulty levels, which are gradually used to train the generative model. Our approach, Curriculum DPO, is compared against state-of-the-art fine-tuning approaches on three benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at anonymous.4open.science/r/Curriculum-DPO-EE14.\n\n\n\nThinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation\nGiorgio Franceschelli, Mirco Musolesi\narXiv.org 2025\nopen paper page\n \n    Abstract \n   Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.\n\n\n\nShapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI\nAlex Glinsky, Alexey Sokolsky\narXiv.org 2024\nopen paper page\n \n    Abstract \n   It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably allocate the reward among them.\n\n\n\nWhat Makes A Good Story? Designing Composite Rewards for Visual Storytelling\nJunjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng Gao, Graham Neubig\nAAAI Conference on Artificial Intelligence 2019\nopen paper page\n \n    Abstract \n   Previous storytelling approaches mostly focused on optimizing traditional metrics such as BLEU, ROUGE and CIDEr. In this paper, we re-examine this problem from a different angle, by looking deep into what defines a natural and topically-coherent story. To this end, we propose three assessment criteria: relevance, coherence and expressiveness, which we observe through empirical analysis could constitute a &quot;high-quality&quot; story to the human eye. We further propose a reinforcement learning framework, ReCo-RL, with reward functions designed to capture the essence of these quality criteria. Experiments on the Visual Storytelling Dataset (VIST) with both automatic and human evaluation demonstrate that our ReCo-RL model achieves better performance than state-of-the-art baselines on both traditional metrics and the proposed new criteria.\n\n\n\nControlNet++: Improving Conditional Controls with Efficient Consistency Feedback\nMing Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen\nEuropean Conference on Computer Vision 2024\nopen paper page\n \n    Abstract \n   To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo.\n\n\n\nOn the evaluation of generative models in music\nLi-Chia Yang, Alexander Lerch\nNeural computing &amp; applications (Print) 2018\nopen paper page\n \n    Abstract \n\n\n\nDeep reinforcement learning for de novo drug design\nAlexander Tropsha, Olexandr Isayev, Mariya Popova\nScience Advances 2017\nopen paper page\n \n    Abstract \n   We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks-generative and predictive-that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo-generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.\n\n\n\nQuantitative Characteristics of Human-Written Short Stories as a Metric for Automated Storytelling\nC. León, Pablo Gervás, Pablo Delatorre, Alan Tapscott\nNew generation computing 2020\nopen paper page\n \n    Abstract \n   Evaluating the extent to which computer-produced stories are structured like human-invented narratives can be an important component of the quality of a story plot. In this paper, we report on an empirical experiment in which human subjects have invented short plots in a constrained scenario. The stories were annotated according to features commonly found in existing automatic story generators. The annotation was designed to measure the proportion and relations of story components that should be used in automatic computational systems for matching human behaviour. Results suggest that there are relatively common patterns that can be used as input data for identifying similarity to human-invented stories in automatic storytelling systems. The found patterns are in line with narratological models, and the results provide numerical quantification and layout of story components. The proposed method of story analysis is tested over two additional sources, the ROCStories corpus and stories generated by automated storytellers, to illustrate the valuable insights that may be derived from them.\n\n\n\nReinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges\nGiorgio Franceschelli, Mirco Musolesi\nJournal of Artificial Intelligence Research 2023\nopen paper page\n \n    Abstract \n   Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.\n\n\n\nCopyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning\nLingjuan Lyu, Zhuan Shi, Yifei Song, Xiaoli Tang, Boi Faltings\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder&#039;s contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.\n\n\n\nShaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models\nYuchen Wu, Melissa Mozifian, F. Shkurti\nIEEE International Conference on Robotics and Automation 2020\nopen paper page\n \n    Abstract \n   The potential benefits of model-free reinforcement learning to real robotics systems are limited by its uninformed exploration that leads to slow convergence, lack of data-efficiency, and unnecessary interactions with the environment. To address these drawbacks we propose a method that combines reinforcement and imitation learning by shaping the reward function with a state-and-action-dependent potential that is trained from demonstration data, using a generative model. We show that this accelerates policy learning by specifying high-value areas of the state and action space that are worth exploring first. Unlike the majority of existing methods that assume optimal demonstrations and incorporate the demonstration data as hard constraints on policy optimization, we instead incorporate demonstration data as advice in the form of a reward shaping potential trained as a generative model of states and actions. In particular, we examine both normalizing flows and Generative Adversarial Networks to represent these potentials. We show that, unlike many existing approaches that incorporate demonstrations as hard constraints, our approach is unbiased even in the case of suboptimal and noisy demonstrations. We present an extensive range of simulations, as well as experiments on the Franka Emika 7DOF arm, to demonstrate the practicality of our method.\n\n\n\nFantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems\nCaiming Xiong, Shentao Yang, Shujian Zhang, Haiquan Wang, Mi Zhou, Yihao Feng, Jianguo Zhang\nInternational Conference on Learning Representations 2023\nopen paper page\n \n    Abstract \n   When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.\n\n\n\nEfficient (Soft) Q-Learning for Text Generation with Limited Good Data\nHan Guo, Bowen Tan, Zhiting Hu, Eric P. Xing, Zhengzhong Liu\nConference on Empirical Methods in Natural Language Processing 2021\nopen paper page\n \n    Abstract \n   Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.\n\n\n\nDeep Reward Supervisions for Tuning Text-to-Image Diffusion Models\nXiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li\nEuropean Conference on Computer Vision 2024\nopen paper page\n \n    Abstract \n   Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2.\n\n\n\nEnergy-Based Models for Code Generation under Compilability Constraints\nTomasz Korbak, Hady ElSahar, Marc Dymetman, Germán Kruszewski\narXiv.org 2021\nopen paper page\n \n    Abstract \n   Neural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021) to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples.\n\n\n\nPredicting the Quality of Short Narratives from Social Media\nTong Wang, Ping Chen, Boyang Albert Li\nInternational Joint Conference on Artificial Intelligence 2017\nopen paper page\n \n    Abstract \n   An important and difficult challenge in building computational models for narratives is the automatic evaluation of narrative quality. Quality evaluation connects narrative understanding and generation as generation systems need to evaluate their own products. To circumvent difficulties in acquiring annotations, we employ upvotes in social media as an approximate measure for story quality. We collected 54,484 answers from a crowd-powered question-and-answer website, Quora, and then used active learning to build a classifier that labeled 28,320 answers as stories. To predict the number of upvotes without the use of social network features, we create neural networks that model textual regions and the interdependence among regions, which serve as strong benchmarks for future research. To our best knowledge, this is the first large-scale study for automatic evaluation of narrative quality.\n\n\n\nChoose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models\nElizabeth Clark, Noah A. Smith\nNorth American Chapter of the Association for Computational Linguistics 2021\nopen paper page\n \n    Abstract \n   Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present Choose Your Own Adventure, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model’s suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models’ performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models).\n\n\n\nThe Next Chapter: A Study of Large Language Models in Storytelling\nZhuohan Xie, Trevor Cohn, Jey Han Lau\nInternational Conference on Natural Language Generation 2023\nopen paper page\n \n    Abstract \n   To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge. The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks. This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories. The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models. Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.\n\n\n\nSkywork-Reward: Bag of Tricks for Reward Modeling in LLMs\nChris Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, Yahui Zhou\narXiv.org 2024\nopen paper page\n \n    Abstract \n   In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.\n\n\n\nDiversity-Rewarded CFG Distillation\nGeoffrey Cideron, Andrea Agostinelli, Johan Ferret, Sertan Girgin, R. Élie, Olivier Bachem, Sarah Perrin, Alexandre Ram’e\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at google-research.github.io/seanet/musiclm/diverse_music/.\n\n\n\nExposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models\nG. Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L. Caterini, J. E. T. Taylor, G. Loaiza-Ganem\nNeural Information Processing Systems 2023\nopen paper page\n \n    Abstract \n   We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at github.com/layer6ai-labs/dgm-eval.\n\n\n\nHow interesting and coherent are the stories generated by a large‐scale neural language model? Comparing human and automatic evaluations of machine‐generated text\nDominic Callan, Jennifer Foster\nExpert Syst. J. Knowl. Eng. 2023\nopen paper page\n \n    Abstract \n   Evaluation of the narrative text generated by machines has traditionally been a challenge, particularly when attempting to evaluate subjective elements such as interest or believability. Recent improvements in narrative machine text generation have been largely driven by the emergence of transformer‐based language models, trained on massive quantities of data, resulting in higher quality text generation. In this study, a corpus of stories is generated using the pre‐trained GPT‐Neo transformer model, with human‐written prompts as inputs upon which to base the narrative text. The stories generated through this process are subsequently evaluated through both human evaluation and two automated metrics: BERTScore and BERT Next Sentence Prediction, with the aim of determining whether there is a correlation between the automatic scores and the human judgements. The results show variation in human evaluation results in comparison to modern automated metrics, suggesting further work is required to train automated metrics to identify text that is defined as interesting by humans.\n\n\n\nSTORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation\nNader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, Mohit Iyyer\nConference on Empirical Methods in Natural Language Processing 2020\nopen paper page\n \n    Abstract \n   Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.\n\n\n\nDecoding Methods for Neural Narrative Generation\nAlexandra DeLucia, Aaron Mueller, Xiang Lisa Li, João Sedoc\nIEEE Games Entertainment Media Conference 2020\nopen paper page\n \n    Abstract \n   Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters-specifically, maximum mutual information-analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.\n\n\n\nObjective Evaluation Metric for Motion Generative Models: Validating Fréchet Motion Distance on Foot Skating and Over-smoothing Artifacts.\nAntoine Maiorca, Hugo Bohy, Youngwoo Yoon, Thierry Dutoit\nMotion in Games 2023\nopen paper page\n \n    Abstract \n   Nowadays, Deep Learning-powered generative models are able to generate new synthetic samples nearly indistinguishable from natural data. The development of such systems necessarily involves the design of evaluation protocols to assess their performance. Quantitative objective metrics, such as Fréchet distance, in addition to human-centered subjective surveys, have become a standard for evaluating generative algorithms. Although motion generation is a popular research field, only a few works addressed the problem of the design and validation of a robust objective evaluation metric for motion-generative models. These previous works proposed to degrade ground truth motion samples with synthetic noises (e.g., Gaussian, Salt&amp; Pepper) and studied the behavior of the proposed metric. However, this degradation does not mimic common motion artifacts produced by generative models. In this work, we propose (1) to validate Fréchet distance-based objective metrics on motion datasets degraded by two realistic motion artifacts, foot skating and over-smoothing, often found in motion synthesis results, and (2) a Fréchet Motion Distance (FMD), using Transformer-based feature extractor, able to capture the motion artifacts and also robust towards the variation of motion length.\n\n\n\nModifying Large Language Model Post-Training for Diverse Creative Writing\nJohn Joon Young Chung, Vishakh Padmakumar, Melissa Roemmele, Yuqian Sun, Max Kreminski\narXiv.org 2025\nopen paper page\n \n    Abstract \n   As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.\n\n\n\nLearning to Reason for Long-Form Story Generation\nAlexander Gurung, Mirella Lapata\narXiv.org 2025\nopen paper page\n \n    Abstract \n   Generating high-quality stories spanning thousands of tokens requires competency across a variety of skills, from tracking plot and character arcs to keeping a consistent and engaging style. Due to the difficulty of sourcing labeled datasets and precise quality measurements, most work using large language models (LLMs) for long-form story generation uses combinations of hand-designed prompting techniques to elicit author-like behavior. This is a manual process that is highly dependent on the specific story-generation task. Motivated by the recent success of applying RL with Verifiable Rewards to domains like math and coding, we propose a general story-generation task (Next-Chapter Prediction) and a reward formulation (Verified Rewards via Completion Likelihood Improvement) that allows us to use an unlabeled book dataset as a learning signal for reasoning. We learn to reason over a story&#039;s condensed information and generate a detailed plan for the next chapter. Our reasoning is evaluated via the chapters it helps a story-generator create, and compared against non-trained and supervised finetuning (SFT) baselines. Pairwise human judgments reveal the chapters our learned reasoning produces are preferred across almost all metrics, and the effect is more pronounced in Scifi and Fantasy genres.\n\n\n\nCreativity in AI: Progresses and Challenges\nLonneke van der Plas, Antoine Bosselut, Debjit Paul, Mete Ismayilzada\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition. Machine creativity on the other hand has been a long-standing challenge. With the rise of advanced generative AI, there has been renewed interest and debate regarding AI&#039;s creative capabilities. Therefore, it is imperative to revisit the state of creativity in AI and identify key progresses and remaining challenges. In this work, we survey leading works studying the creative capabilities of AI systems, focusing on creative problem-solving, linguistic, artistic, and scientific creativity. Our review suggests that while the latest AI models are largely capable of producing linguistically and artistically creative outputs such as poems, images, and musical pieces, they struggle with tasks that require creative problem-solving, abstract thinking and compositionality and their generations suffer from a lack of diversity, originality, long-range incoherence and hallucinations. We also discuss key questions concerning copyright and authorship issues with generative models. Furthermore, we highlight the need for a comprehensive evaluation of creativity that is process-driven and considers several dimensions of creativity. Finally, we propose future research directions to improve the creativity of AI outputs, drawing inspiration from cognitive science and psychology.\n\n\n\nToward Interactive Music Generation: A Position Paper\nS. Dadman, B. Bremdal, B. Bang, Rune Dalmo\nIEEE Access 2022\nopen paper page\n \n    Abstract \n   Music generation using deep learning has received considerable attention in recent years. Researchers have developed various generative models capable of imitating musical conventions, comprehending the musical corpora, and generating new samples based on the learning outcome. Although the samples generated by these models are persuasive, they often lack musical structure and creativity. For instance, a vanilla end-to-end approach, which deals with all levels of music representation at once, does not offer human-level control and interaction during the learning process, leading to constrained results. Indeed, music creation is a recurrent process that follows some principles by a musician, where various musical features are reused or adapted. On the other hand, a musical piece adheres to a musical style, breaking down into precise concepts of timbre style, performance style, composition style, and the coherency between these aspects. Here, we study and analyze the current advances in music generation using deep learning models through different criteria. We discuss the shortcomings and limitations of these models regarding interactivity and adaptability. Finally, we draw the potential future research direction addressing multi-agent systems and reinforcement learning algorithms to alleviate these shortcomings and limitations.\n\n\n\nChallenges in creative generative models for music: a divergence maximization perspective\nAxel Chemla-Romeu-Santos, P. Esling\narXiv.org 2022\nopen paper page\n \n    Abstract \n   The development of generative Machine Learning (ML) models in creative prac-tices, enabled by the recent improvements in usability and availability of pre-trained models, is raising more and more interest among artists, practitioners and perform-ers. Yet, the introduction of such techniques in artistic domains also revealed multiple limitations that escape current evaluation methods used by scientists. Notably, most models are still unable to generate content that lay outside of the domain deﬁned by the training dataset. In this paper, we propose an alternative prospective framework, starting from a new general formulation of ML objectives, that we derive to delineate possible implications and solutions that already exist in the ML literature (notably for the audio and musical domain). We also discuss existing relations between generative models and computational creativity and how our framework could help address the lack of creativity in existing models.\n\n\n\nConditional End-to-End Audio Transforms\nAlbert Haque, Michelle Guo, Prateek Verma\nInterspeech 2018\nopen paper page\n \n    Abstract \n   We present an end-to-end method for transforming audio from one style to another. For the case of speech, by conditioning on speaker identities, we can train a single model to transform words spoken by multiple people into multiple target voices. For the case of music, we can specify musical instruments and achieve the same result. Architecturally, our method is a fully-differentiable sequence-to-sequence model based on convolutional and hierarchical recurrent neural networks. It is designed to capture long-term acoustic dependencies, requires minimal post-processing, and produces realistic audio transforms. Ablation studies confirm that our model can separate speaker and instrument properties from acoustic content at different receptive fields. Empirically, our method achieves competitive performance on community-standard datasets.\n\n\n\nPerception Score: A Learned Metric for Open-ended Text Generation Evaluation\nJing Gu, Qingyang Wu, Zhou Yu\nAAAI Conference on Artificial Intelligence 2021\nopen paper page\n \n    Abstract \n   Automatic evaluation for open-ended natural language generation tasks remains a challenge. We propose a learned evaluation metric: Perception Score. It utilizes a pre-trained model and considers context information for conditional generation. Perception Score assigns a holistic score along with the uncertainty measurement. We conduct experiments on three open-ended conditional generation tasks and two open-ended unconditional generation tasks. Perception Score achieves state-of-the-art results on all the tasks consistently in terms of correlation with human evaluation scores.\n\n\n\nDeltaScore: Fine-Grained Story Evaluation with Perturbations\nZhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau\nConference on Empirical Methods in Natural Language Processing 2023\nopen paper page\n \n    Abstract \n   Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce DELTASCORE, a novel methodology that employs perturbation techniques for the evaluation of nuanced story aspects. Our central proposition posits that the extent to which a story excels in a specific aspect (e.g., fluency) correlates with the magnitude of its susceptibility to particular perturbations (e.g., the introduction of typos). Given this, we measure the quality of an aspect by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models. We compare DELTASCORE with existing metrics on storytelling datasets from two domains in five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness. DELTASCORE demonstrates remarkable performance, revealing a surprising finding that a specific perturbation proves highly effective in capturing multiple aspects.\n\n\n\nCreative Writing with a Machine in the Loop: Case Studies on Slogans and Stories\nElizabeth Clark, A. S. Ross, Chenhao Tan, Yangfeng Ji, Noah A. Smith\nInternational Conference on Intelligent User Interfaces 2018\nopen paper page\n \n    Abstract \n\n\n\nHoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing\nJing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, Tian Feng\nConference on Empirical Methods in Natural Language Processing 2024\nopen paper page\n \n    Abstract \n   Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing. In particular, large language models (LLMs) can hardly produce written works at the level of human experts due to the extremely high complexity of literature writing. In this paper, we present HoLLMwood, an automated framework for unleashing the creativity of LLMs and exploring their potential in screenwriting, which is a highly demanding task. Mimicking the human creative process, we assign LLMs to different roles involved in the real-world scenario. In addition to the common practice of treating LLMs as ${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing feedback and revision advice to ${Writer}$. Besides, to enrich the characters and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as ${Actors}$ that can communicate and interact with each other. Evaluations on automatically generated screenplays show that HoLLMwood substantially outperforms strong baselines in terms of coherence, relevance, interestingness and overall quality.\n\n\n\nAI as a Sport: On the Competitive Epistemologies of Benchmarking\nWill Orr, Edward B. Kang\nConference on Fairness, Accountability and Transparency 2024\nopen paper page\n \n    Abstract \n   Artificial Intelligence (AI) systems are evaluated using competitive methods that rely on benchmark datasets to determine performance. These benchmark datasets, however, are often constructed through arbitrary processes that fall short in encapsulating the depth and breadth of the tasks they are intended to measure. In this paper, we interrogate the naturalization of benchmark datasets as veracious metrics by examining the historical development of benchmarking as an epistemic practice in AI research. Specifically, we highlight three key case studies that were crucial in establishing the existing reliance on benchmark datasets for evaluating the capabilities of AI systems: (1) the sharing of Highleyman’s OCR dataset in the 1960s, which solidified a community of knowledge production around a shared benchmark dataset, (2) the Common Task Framework (CTF) of the 1980s, a state-led project to standardize benchmark datasets as legitimate indicators of technical progress; and (3) the Netflix Prize which further solidified benchmarking as a competitive goal within the ML research community. This genealogy highlights how contemporary dynamics and limitations of benchmarking developed from a longer history of collaboration, standardization, and competition. We end with reflections on how this history informs our understanding of benchmarking in the current era of generative artificial intelligence.\n\n\n\nEvaluating Large Language Model Creativity from a Literary Perspective\nMurray Shanahan, Catherine Clarke\narXiv.org 2023\nopen paper page\n \n    Abstract \n   This paper assesses the potential for large language models (LLMs) to serve as assistive tools in the creative writing process, by means of a single, in-depth case study. In the course of the study, we develop interactive and multi-voice prompting strategies that interleave background descriptions (scene setting, plot elements), instructions that guide composition, samples of text in the target style, and critical discussion of the given samples. We qualitatively evaluate the results from a literary critical perspective, as well as from the standpoint of computational creativity (a sub-field of artificial intelligence). Our findings lend support to the view that the sophistication of the results that can be achieved with an LLM mirrors the sophistication of the prompting.\n\n\n\nArt or Artifice? Large Language Models and the False Promise of Creativity\nTuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Chien-Sheng Wu, Smaranda Muresan\nInternational Conference on Human Factors in Computing Systems 2023\nopen paper page\n \n    Abstract \n   Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT) [64], which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose Torrance Test of Creative Writing (TTCW) to evaluate creativity as product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.\n\n\n\nThe Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation\nMarzena Karpinska, Nader Akoury, Mohit Iyyer\nConference on Empirical Methods in Natural Language Processing 2021\nopen paper page\n \n    Abstract \n   Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.\n\n\n\nFollow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding\nMirac Suzgun, Luke Melas-Kyriazi, Dan Jurafsky\nAnnual Meeting of the Association for Computational Linguistics 2022\nopen paper page\n \n    Abstract \n   In open-ended natural-language generation, existing text decoding methods typically struggle to produce text which is both diverse and high-quality. Greedy and beam search are known to suffer from text degeneration and linguistic diversity issues, while temperature, top-k, and nucleus sampling often yield diverse but low-quality outputs. In this work, we present crowd sampling, a family of decoding methods based on Bayesian risk minimization, to address this diversity-quality trade-off. Inspired by the principle of&quot;the wisdom of the crowd,&quot;crowd sampling seeks to select a candidate from a pool of candidates that has the least expected risk (i.e., highest expected reward) under a generative model according to a given utility function. Crowd sampling can be seen as a generalization of numerous existing methods, including majority voting, and in practice, it can be used as a drop-in replacement for existing sampling methods. Extensive experiments show that crowd sampling delivers improvements of 3-7 ROUGE and BLEU points across a wide range of tasks, including summarization, data-to-text, translation, and textual style transfer, while achieving new state-of-the-art results on WebNLG and WMT&#039;16.\n\n\n\nReproducible Subjective Evaluation\nMax Morrison, Brian Tang, Gefei Tan, Bryan Pardo\narXiv.org 2022\nopen paper page\n \n    Abstract \n   Human perceptual studies are the gold standard for the evaluation of many research tasks in machine learning, linguistics, and psychology. However, these studies require significant time and cost to perform. As a result, many researchers use objective measures that can correlate poorly with human evaluation. When subjective evaluations are performed, they are often not reported with sufficient detail to ensure reproducibility. We propose Reproducible Subjective Evaluation (ReSEval), an open-source framework for quickly deploying crowdsourced subjective evaluations directly from Python. ReSEval lets researchers launch A/B, ABX, Mean Opinion Score (MOS) and MUltiple Stimuli with Hidden Reference and Anchor (MUSHRA) tests on audio, image, text, or video data from a command-line interface or using one line of Python, making it as easy to run as objective evaluation. With ReSEval, researchers can reproduce each other&#039;s subjective evaluations by sharing a configuration file and the audio, image, text, or video files.\n\n\n\nNot (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition\nSandro Pezzelle, Aditya K Surikuchi, Raquel Fern’andez\nConference on Empirical Methods in Natural Language Processing 2024\nopen paper page\n \n    Abstract \n   Visual storytelling consists in generating a natural language story given a temporally ordered sequence of images. This task is not only challenging for models, but also very difficult to evaluate with automatic metrics since there is no consensus about what makes a story &#039;good&#039;. In this paper, we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects highlighted in previous work: visual grounding, coherence, and repetitiveness. We then use this method to evaluate the stories generated by several models, showing that the foundation model LLaVA obtains the best result, but only slightly so compared to TAPM, a 50-times smaller visual storytelling model. Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters. Finally, we carry out a human evaluation study, whose results suggest that a &#039;good&#039; story may require more than a human-like level of visual grounding, coherence, and repetition.\n\n\n\nOn the Problem of Small Objects\nTiasa Mondol, Daniel G. Brown\nEntropy 2021\nopen paper page\n \n    Abstract \n   We discuss how to assess computationally the aesthetic value of &quot;small&quot; objects, namely those that have short digital descriptions. Such small objects still matter: they include headlines, poems, song lyrics, short musical scripts and other culturally crucial items. Yet, small objects are a confounding case for our recent work adapting ideas from algorithmic information theory (AIT) to the domain of computational creativity, as they cannot be either logically deep or sophisticated following the traditional definitions of AIT. We show how restricting the class of models under analysis can make it the case that we can still separate high-quality small objects from ordinary ones, and discuss the strengths and limitations of our adaptation.\n\n\n\nCreative Help: A Story Writing Assistant\nMelissa Roemmele, A. Gordon\nInternational Conference on Interactive Digital Storytelling 2015\nopen paper page\n \n    Abstract \n\n\n\nBetter Together? An Evaluation of AI-Supported Code Translation\nJustin D. Weisz, Michael J. Muller, Steven I. Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, John T. Richards\nInternational Conference on Intelligent User Interfaces 2022\nopen paper page\n \n    Abstract \n   Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.\n\n\n\nA Unifying Information-theoretic Perspective on Evaluating Generative Models\nAlexis Fox, S. Swarup, Abhijin Adiga\nAAAI Conference on Artificial Intelligence 2024\nopen paper page\n \n    Abstract \n   Considering the difficulty of interpreting generative model output, there is significant current research focused on determining meaningful evaluation metrics. Several recent approaches utilize &quot;precision&quot; and &quot;recall,&quot; borrowed from the classification domain, to individually quantify the output fidelity (realism) and output diversity (representation of the real data variation), respectively. With the increase in metric proposals, there is a need for a unifying perspective, allowing for easier comparison and clearer explanation of their benefits and drawbacks. To this end, we unify a class of kth-nearest neighbors (kNN)-based metrics under an information-theoretic lens using approaches from kNN density estimation. Additionally, we propose a tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity and two distinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic concepts of entropy and cross-entropy, can be dissected for both sample- and mode-level analysis. Our detailed experimental results demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics.\n\n\n\nCoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities\nMina Lee, Percy Liang, Qian Yang\nInternational Conference on Human Factors in Computing Systems 2022\nopen paper page\n \n    Abstract \n   Large language models (LMs) offer unprecedented language generation capabilities and exciting opportunities for interaction design. However, their highly context-dependent capabilities are difficult to grasp and are often subjectively interpreted. In this paper, we argue that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs’ generative capabilities. Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3’s capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions. We demonstrate that CoAuthor can address questions about GPT-3’s language, ideation, and collaboration capabilities, and reveal its contribution as a writing “collaborator” under various definitions of good collaboration. Finally, we discuss how this work may facilitate a more principled discussion around LMs’ promises and pitfalls in relation to interaction design. The dataset and an interface for replaying the writing sessions are publicly available at coauthor.stanford.edu.\n\n\n\nThe Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing\nZhuoyan Li, Chen Liang, Jing Peng, Ming Yin\nInternational Conference on Human Factors in Computing Systems 2024\nopen paper page\n \n    Abstract \n   Recent advances in generative AI technologies like large language models raise both excitement and concerns about the future of human-AI co-creation in writing. To unpack people&#039;s attitude towards and experience with generative AI-powered writing assistants, in this paper, we conduct an experiment to understand whether and how much value people attach to AI assistance, and how the incorporation of AI assistance in writing workflows changes people&#039;s writing perceptions and performance. Our results suggest that people are willing to forgo financial payments to receive writing assistance from AI, especially if AI can provide direct content generation assistance and the writing task is highly creative. Generative AI-powered assistance is found to offer benefits in increasing people&#039;s productivity and confidence in writing. However, direct content generation assistance offered by AI also comes with risks, including decreasing people&#039;s sense of accountability and diversity in writing. We conclude by discussing the implications of our findings.\n\n\n\nOf Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation\nCyril Chhun, Pierre Colombo, C. Clavel, Fabian M. Suchanek\nInternational Conference on Computational Linguistics 2022\nopen paper page\n \n    Abstract \n   Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation.\n\n\n\nMeasuring Diversity in Co-creative Image Generation\nF. Ibarrola, Kazjon Grace\nInternational Conference on Innovative Computing and Cloud Computing 2024\nopen paper page\n \n    Abstract \n   Quality and diversity have been proposed as reasonable heuristics for assessing content generated by co-creative systems, but to date there has been little agreement around what constitutes the latter or how to measure it. Proposed approaches for assessing generative models in terms of diversity have limitations in that they compare the model&#039;s outputs to a ground truth that in the era of large pre-trained generative models might not be available, or entail an impractical number of computations. We propose an alternative based on entropy of neural network encodings for comparing diversity between sets of images that does not require ground-truth knowledge and is easy to compute. We also compare two pre-trained networks and show how the choice relates to the notion of diversity that we want to evaluate. We conclude with a discussion of the potential applications of these measures for ideation in interactive systems, model evaluation, and more broadly within computational creativity.\n\n\n\nStrategies for Structuring Story Generation\nAngela Fan, M. Lewis, Yann Dauphin\nAnnual Meeting of the Association for Computational Linguistics 2019\nopen paper page\n \n    Abstract \n   Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.\n\n\n\nCrafting Narrative Closures: Zero-Shot Learning with SSM Mamba for Short Story Ending Generation\nDivyam Sharma, Divya Santhanam\narXiv.org 2024\nopen paper page\n \n    Abstract \n   Writing stories is an engaging yet challenging endeavor. Often, authors encounter moments of creative block, where the path forward in their narrative becomes obscured. This paper is designed to address such moments by providing an innovative solution: A tool that completes stories based on given prompts. By inputting a short story prompt, users can receive a conclusion to their story, articulated in one sentence or more, thereby enhancing the storytelling process with AI-driven creativity. This tool aims not only to assist authors in navigating writer&#039;s block but also to offer a fun and interactive way for anyone to expand on story ideas spontaneously. Through this paper, we explore the intersection of artificial intelligence and creative writing, pushing the boundaries of how stories can be crafted and concluded. To create our final text-generation models, we used a pre-trained GPT-3.5 model and a newly created finetuned SSM-Mamba model, both of which perform well on a comprehensive list of metrics including BERT score, METEOR, BLEU, ROUGE, and Perplexity. The SSM model has also been made public for the NLP community on HuggingFace models as an open source contribution, which for the timebeing is a first of its kind state-space model for story-generation task on HuggingFace.\n\n\n\nRandom Network Distillation as a Diversity Metric for Both Image and Text Generation\nLiam H. Fowl, Micah Goldblum, Arjun Gupta, Amr Sharaf, T. Goldstein\narXiv.org 2020\nopen paper page\n \n    Abstract \n   Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.\n\n\n\nOn Measuring Fairness in Generative Models\nMilad Abdollahzadeh, Ngai-Man Cheung, Christopher T. H. Teo\nNeural Information Processing Systems 2023\nopen paper page\n \n    Abstract \n   Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on fairness measurement, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., 4.98% $\\rightarrow$ 0.62% for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. Code and more resources: sutd-visual-computing-group.github.io/CLEAM/.\n\n\n\nThe GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories\nXi Yu Huang, Krishnapriya Vishnubhotla, Frank Rudzicz\narXiv.org 2024\nopen paper page\n \n    Abstract \n   The improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling. It is therefore important to quantitatively understand the nature of generated stories, and how they differ from human storytelling. We augment the Reddit WritingPrompts dataset with short stories generated by GPT-3.5, given the same prompts. We quantify and compare the emotional and descriptive features of storytelling from both generative processes, human and machine, along a set of six dimensions. We find that generated stories differ significantly from human stories along all six dimensions, and that human and machine generations display similar biases when grouped according to the narrative point-of-view and gender of the main protagonist. We release our dataset and code at github.com/KristinHuangg/gpt-writing-prompts.\n\n\n\nSynthetic Literature: Writing Science Fiction in a Co-Creative Process\nEnrique Manjavacas, Folgert Karsdorp, Ben Burtenshaw, M. Kestemont\nCC-NLG@INLG 2017\nopen paper page\n \n    Abstract \n   This paper describes a co-creative text generation system applied within a science fiction setting to be used by an established novelist. The project was initiated as part of The Dutch Book Week, and the generated text will be published within a volume of science fiction stories. We explore the ramifications of applying Natural Language Generation within a cocreative process, and examine where the cocreative setting challenges both writer and machine. We employ a character-level language model to generate text based on a large corpus of Dutch novels that exposes a number of tunable parameters to the user. The system is used through a custom graphical user interface, that helps the writer to elicit, modify and incorporate suggestions by the text generation system. Besides a literary work, the output of the present project also includes user-generated meta-data that is expected to contribute to the quantitative evaluation of the text-generation system and the co-creative process involved.\n\n\n"},"01-Fleeting-Notes/Papers-to-Review-for-LangBiasGenImg":{"slug":"01-Fleeting-Notes/Papers-to-Review-for-LangBiasGenImg","filePath":"01 Fleeting Notes/Papers to Review for LangBiasGenImg.md","title":"Papers to Review for LangBiasGenImg","links":[],"tags":[],"content":"EMNLP 22\nHow well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?\nMedCLIP: Contrastive Learning from Unpaired Medical Images and Text\nText-Only Training for Image Captioning using Noise-Injected CLIP\nClip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards\nRelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning"},"01-Fleeting-Notes/Partial-input-baselines-show-that-NLI-models-can-ignore-context,-but-they-don't":{"slug":"01-Fleeting-Notes/Partial-input-baselines-show-that-NLI-models-can-ignore-context,-but-they-don't","filePath":"01 Fleeting Notes/Partial-input baselines show that NLI models can ignore context, but they don't.md","title":"Partial-input baselines show that NLI models can ignore context, but they don't","links":["tags/paper-notes","01-Fleeting-Notes/NLI+-for-CoT"],"tags":["paper-notes"],"content":"2023-04-03 19:49\nTags: paper-notes\n\nPartial-input baselines show that NLI models can ignore context, but they don’t (2022)\nNeha Srikanth and Rachel Rudinger (UMD)\nTies back to the defeasible language inference (strengthener and weakener changes to sentences should be reflected in confidence) Thinking like a skeptic: defeasible inference in NL (2020)\nThis fits in with the NLI annotation artifacts story\n\nThey claim that because a human wouldn’t cheat on a dataset, it’s possible that NLI models aren’t necessarily cheating either and could still perform task the right way, pointing out that you’re starting from aritfacts that don’t have the inductive biases to cheat (generalized LMs)\nThey show that full-context (PSC) models are more confident than partial-context (SSC) and claim this suggests that this is evidence that contextual information is being used in the PSC and that model isn’t attending to SSC on high-confidence SSC examples\n\nIs this really true though? Reduction in ambiguity on other samples could also lead to higher confidence even when cheating is still employed but need to think on this more\nHowever, I think the next part will be actually convincing\n\n\n\nContext editing scheme:\n\nLook at the context sentence and make significant edits\nSelect sentences that are most likely to contain artifacts based on BoW or SSC model confidence\nEdit in a way based on a guess? that will lead to better performance?\nThey suggest this means that even on the broken datasets, the model still is learning to attend to both sentences\n\n\n\nI agree that it is “hasty” to conclude that models trained on them are incapable of reasoning. I’m not entirely convinced that this really proves that they don’t cheat though.\n\nPECO is a better heuristic of high-confidence cheating feature sample identification wrt actual use in the biased condition.\n\nStart from those for modification instead to test!\n\n\n\n\nNLI+ for CoT"},"01-Fleeting-Notes/Planning-for-the-Cross-cultural-Cross-lingual-Image-Gen":{"slug":"01-Fleeting-Notes/Planning-for-the-Cross-cultural-Cross-lingual-Image-Gen","filePath":"01 Fleeting Notes/Planning for the Cross-cultural Cross-lingual Image Gen.md","title":"Planning for the Cross-cultural Cross-lingual Image Gen","links":[],"tags":[],"content":"Starting points\n\nAlex has the stuff about how to do preliminary studies, etc\n\nwhich prompts have I tried so far, …\n\n\nGoogle group doing cross cultural vision and language research, William will link\nShare a Google Doc containing project plan with William + spreadsheet\n"},"01-Fleeting-Notes/Rao's-arguments-about-explanations-in-AI":{"slug":"01-Fleeting-Notes/Rao's-arguments-about-explanations-in-AI","filePath":"01 Fleeting Notes/Rao's arguments about explanations in AI.md","title":"Rao's arguments about explanations in AI","links":["tags/transparency","tags/writing-ideas","tags/ethics","tags/nomenclature"],"tags":["transparency","writing-ideas","ethics","nomenclature"],"content":"Rao’s arguments about explanations in AI\n2022-07-14 20:46\nTags: transparency writing-ideas ethics nomenclature\n\nOn Jay Shah podcast (around 1:00:00-1:10:00)\nWe don’t accept explanationless decisions from e.g., Judges, doctors\nExplanations must be contestable"},"01-Fleeting-Notes/Rationalists-are-like-Soylent":{"slug":"01-Fleeting-Notes/Rationalists-are-like-Soylent","filePath":"01 Fleeting Notes/Rationalists are like Soylent.md","title":"Rationalists are like Soylent","links":["01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost"],"tags":[],"content":"I think the reason I dislike rationalist/Tech Culture types is the way they blend elements of life that I prefer to be kept separate into a homogenous style of living across all domains. Talking like an egghead academic about bayesian updates to describe your everyday life. Using memespeak talking about NPCs and being “___pilled” to describe serious work topics. Viscerally, to me they are to living what Soylent is to eating. Epistemic status: certain\nDownvoted to oblivion for pointing out sane reasons to Why I’m Not worried about superintelligence blogpost\nwww.lesswrong.com/posts/Aq5X9tapacnk2QGY4/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all\n\nNo proof SI is possible\nAI rewriting itself to godlike intelligence not supported\nMagical thinking\nFragility of real-world networks\n\nthefrailestthing.com/2013/03/01/borg-complex-a-primer/\n\nBorg complex\nInevitability of future tech\nIn particular refers to historical antecedents solely to dismiss present concerns\n\nWe’ve lost the plot\n\nOur constant need for entertainment has blurred the line between fiction and reality—on television, in American politics, and in our everyday lives.\nTO READ\nSaved as a pdf to Desktop/atlantic-the-plot.pdf\n"},"01-Fleeting-Notes/Reading-2-23":{"slug":"01-Fleeting-Notes/Reading-2-23","filePath":"01 Fleeting Notes/Reading 2-23.md","title":"Reading 2-23","links":["tags/linkvomit","02-Document-Notes/Generalization-Problem-in-DL-2023"],"tags":["linkvomit"],"content":"2023-02-13 00:44\nTags: linkvomit\n\nRaffell calls to build models like OSS\n\nI believe in the core agenda here\nI think that my research direction is consonant with this one\n\nNovelist Cormac McCarthy’s tips on how to write a great science paper\n\nSingle message per paragraph\nChoose 2 points for readers to remember\nSurprisingly or intriguingly once or twice per paper\nNo !!!!\nemdash to emph the most important clauses, don’t overuse bold and italic\n\nMany interesting papers mentioned in Generalization Problem in DL 2023\nSAIL Blog: How does in-context learning work? Sang Xie and Sewon Min\nto read\narxiv.org/abs/2302.11382\n\nPrompt patterns have the same design patterns as software because…because they just do!\n\nOverfitting to the “prompts are the new programming” meme imo\n\n\n\nWe have no moat and neither does OpenAI"},"01-Fleeting-Notes/Reading-Group-Notes":{"slug":"01-Fleeting-Notes/Reading-Group-Notes","filePath":"01 Fleeting Notes/Reading Group Notes.md","title":"Reading Group Notes","links":["tags/meetings"],"tags":["meetings"],"content":"Reading Group Notes\n2022-10-18 15:01\nTags: meetings\n\nGyuwan --- Training LMs with Memory Augmentation\n\nEMNLP 2022, Danqi Chen’s group\n\nInfo bottleneck for transformers, large size drives expensiveness\nThey propose TRIME: Training with In-batch Memories\n\nThree types of memories\n\nThree LM training objectives:\n\nStandard LM\nLong-range context LM eg tfXL\nmissed 3rd\n\nBuilding a same-token training memory to improve the performance within-batch\nThey use an embedding distance minimization objective during training to try to get better results on PPL for wikitext LM\nDeepak --- Binding Language Models in Symbolic Language\n\nfocus on QA as target dim\nFocus on converting question into symbolic query eg SQL\nChain question into GPT-3 Codex\n"},"01-Fleeting-Notes/Reading-list-for-FB-intern":{"slug":"01-Fleeting-Notes/Reading-list-for-FB-intern","filePath":"01 Fleeting Notes/Reading list for FB intern.md","title":"Reading list for FB intern","links":[],"tags":[],"content":"GoogleSGD\naclanthology.org/2021.emnlp-main.590.pdf"},"01-Fleeting-Notes/Research-is-not-a-jack-in-the-box":{"slug":"01-Fleeting-Notes/Research-is-not-a-jack-in-the-box","filePath":"01 Fleeting Notes/Research is not a jack in the box.md","title":"Research is not a jack in the box","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"blog-idea\nTalking about the “crank turning” model of research and the problems that this attitude will cause in young researchers.\nTalking about how this mentality may be more pronounced in CS because of the low barrier to publication.\nManifestations:\n\nLazy, flag-plant-y survey papers\nMad-libs projects (GPT-mini-5, wild*, x-arena)\nObviously incremental work\n\nResearch is not a jack in the box\nTurn the crank on a jack-in-the-box enough times, and sure enough the jack will pop ou"},"01-Fleeting-Notes/Response-to-\"AI-researchers-don't-care-about-real-world-apps\"-article":{"slug":"01-Fleeting-Notes/Response-to-\"AI-researchers-don't-care-about-real-world-apps\"-article","filePath":"01 Fleeting Notes/Response to \"AI researchers don't care about real world apps\" article.md","title":"Response to \"AI researchers don't care about real world apps\" article","links":[],"tags":[],"content":"www.technologyreview.com/2020/08/18/1007196/ai-research-machine-learning-applications-problems-opinion/\nIt’s an article from 2020. Would be interesting to give a reflection on this article n years later\n“The community’s hyperfocus on novel methods ignores what’s really important.”\nFocus is mainly given to issues of bias in benchmark-driven classification research (face recognition racism, bias of objects in imagenet, etc)\nSome discussion also given to how those benchmarks don’t really model the real world in general"},"01-Fleeting-Notes/Reviewer-response-for-Terminator":{"slug":"01-Fleeting-Notes/Reviewer-response-for-Terminator","filePath":"01 Fleeting Notes/Reviewer response for Terminator.md","title":"Reviewer response for Terminator","links":[],"tags":[],"content":"\n Rerun naive baseline 2: (“answer this efficiently”) for r1 1.5b on GPQA and MATH 📅 2025-05-28 ✅ 2025-05-28 (Xiao is running)\n Rerun ablation with % instead of token timing for r1 1.5b on GPQA 📅 2025-05-28 ✅ 2025-05-28\n Rerun ablation with % instead of token timing for r1 1.5b on MATH 📅 2025-05-29 ✅ 2025-05-29\n\n“Difficulty isn’t the only reason accuracy might be lower; what if the Q is memorized:” if the question is memorized, overthinking should definitely NOT happen. So it’s still reasonable that 100% accuracy Qs should have low overthinking (we only use this notion of difficulty as an empirical way to sort questions into groups of “level of expected overthinking”)\n\nAlso not a big deal lol\nAlso the point is just a simple empirical measure noise is expected\nAlso, ref. correlation plots, accuracy is relatively well-correlated across the models, even unrelated ones.\n\n“Why does performance drop across the longer reasoning chains for task and chat?” see footnote 1.\nAlso, we have updated axis labels to say “average score;” this isn’t actually accuracy.\n“unextractable answers are also a failure mode of reasoning models. We suspect part of the length issue is the model failing to provide a final answer at the end, this is a known tendency in distilled reasoning models on tasks they aren’t fine-tuned on”\n“When answering a simple OOD question reasoning models will drift over time”"},"01-Fleeting-Notes/Spiritual-as-opposed-to-emotional-support":{"slug":"01-Fleeting-Notes/Spiritual-as-opposed-to-emotional-support","filePath":"01 Fleeting Notes/Spiritual as opposed to emotional support.md","title":"Spiritual as opposed to emotional support","links":["tags/writing-ideas","tags/definitions","tags/nomenclature","tags/ontology","tags/blog-idea"],"tags":["writing-ideas","definitions","nomenclature","ontology","blog-idea"],"content":"2023-01-01 21:25\nTags: writing-ideas definitions nomenclature ontology blog-idea\n\nThe choice of label classes is so fundamental in NLP it exists as being almost pre-scientific (my words). The things being labeled are largely granted to us, and then we move ahead with building systems to classify\nWhile this gets challenged from the ethics perspective\n\nGender bias in annotations\nFundamental challegnes to the ethics of classification tasks\n\nRace\nSexuality\nIQ\netc\nit doesn’t necessarily get sufficiently challenged on axes that don’t cut across ethics, to wit this paper:\n\n\n\nSacred Be Thy Tech: Thoughts (and Prayers) on Integrating Spirituality in Technology for Health and Well-being\nC. Estelle Smith, CU Boulder\ncolleenestellesmith.files.wordpress.com/2022/06/3543893.pdf\nProposal of “spiritual support” as a separate notion from “emotional support”\n\nUser studies showed “spiritual support” was a missing class of investigation from HCI literature\nDefined as opposed to “instrumental” (bringing lasagna) or “informational” (dropping links) support, a way to help people in crisis (from CaringBridge study)\nAble to study through HCI lens, “when is technology the right solution for spiritual support?”\n\nRelevance to my work\nOntologies for supervised measures of human preferences important for alignment\nUseful as a motivating example in a position paper on such"},"01-Fleeting-Notes/Strongest-PECO-Motivation-Explanation":{"slug":"01-Fleeting-Notes/Strongest-PECO-Motivation-Explanation","filePath":"01 Fleeting Notes/Strongest PECO Motivation Explanation.md","title":"Strongest PECO Motivation Explanation","links":["tags/paper-planning"],"tags":["paper-planning"],"content":"2023-01-24 19:53\nTags: paper-planning\n\nCore Problem\n\nReaders are often confused by the rationale behind and setup for PECO\nIn particular, they want to understand why we train on PSC but test on SSC\nTo motivate this, we need to more clearly drive home the idea that we are:\n\nTrying to characterize model-relevant biases that are present in the data\nPerform this model-driven dataset analysis for the purpose of bias elimination in the dataset\n\n\n\nWe allow the possibility that some SSC-visible biases are not actually used by a classifier when trained in the PSC. Thus, we have to train on PSC and test on SSC, and PECO is an alternative metric of bias that captures this model-level separability of sentences in the SSC notion better than other approaches.\n\nIn particular, write a bit where we relate more closely to competency problems and dataset cartography\n\nFrom Reviewer Response Text\nWe will now respond to your main question, “why is PECO applied to a model that is trained on sentence pairs?” with an answer organized by (numbered) quotations from your review that are relevant to this point.\n(1) “PECO is computed based on a model trained on sentence pairs and therefore should receive a sentence pair at evaluation time.”\nYou describe why evaluating on sentence-pairs for clustering would be futile later in your comment:\n(2) “If the model does receive a sentence pair for computing PECO, clusters are to be expected and there is no way to distinguish good clusters from bad (shortcut) clusters.”\nYou are correct to point out that in observing clusters in the paired sentence condition (PSC), there’s no way to tell “good” clusters apart from shortcut clusters. However, in the single sentence condition (SSC), the only class label-separating clusters that can exist, must be shortcut clusters. This is because such clusters visible in the SSC (whether the model is trained on SSC or PSC) is indicative of information that the model can use to discern the label classes apart when only one sentence is shown, which violates the pairwise nature of the definition of the NLI task.\nWhile this answers why we must evaluate on single sentences for PECO, it alone doesn’t address your further point regarding training on PSC and evaluating on SSC:\n(3) “viewing single sentences during feature extraction for PECO score assessment is a very different setup to what the model was exposed to during finetuning (distribution shift!) and it’s not clear to what degree its single sentence representations from a sentence pair model is meaningful for the task of NLI”\nWe agree with you that the meaningfulness of single sentence embeddings to the task of NLI is questionable; in fact, in the ideal case these embeddings should carry 0 relevant information to the NLI task, were a leakage-free dataset to exist.\nPECO is built on the assumption that for such an ideal NLI dataset, all information relevant to PSC classification SHOULD be lost under the distribution shift that occurs when a single-sentence condition (SSC) population is considered. Thus, we treat finding whether this takes place or not for a given (model, dataset) combination as a proxy for the biasedness of the dataset.\nTo do this we train a classifier to convergence on the PSC population (leading to a high test accuracy in PSC and separation of the label classes in the final embedding space of the classification head) and then estimate which of these features continue to be visible to the model in the SSC, by observing the separability of label classes in said embedding space. Your understanding of the cluster shifting is aligned with our fundamental assumption. \n(4) “Neural networks are known to behave strangely under distribution shift.”\nRegrettably, we did a poor job of explaining how we mitigate the impact of the distribution shift beyond the desired loss of representational separation (for an ideal dataset) in the manuscript. Rather than feeding in a sample strictly containing the single sentence, we input “lesioned” sentence pairs, which preserve the structure of “ s1  s2 ”, but have all tokens in either s1 or s2 zeroed out at both the input token embedding and attention mask levels. This way the samples continue to have the structure of sentence pairs. Although this clarifying detail will be identifiable to those who read our source code, we have added this to the appendices for the final draft to alleviate confusion.\nTo recap, our goal in computing PECO is precisely to identify the degree to which model-visible separation of label classes is possible in SSC, which as you pointed out should be axiomatically impossible. Our emphasis in finding biases the model actually uses is why we train on PSC and test on SSC for PECO, and we have taken measures to minimize the impact that this domain shift has on producing strange behaviors, beyond the expected loss of separation due to information loss. Hopefully, these remarks have answered your question satisfactorily."},"01-Fleeting-Notes/Subjectivity-in-LM-analysis-support,-overleaf-draft":{"slug":"01-Fleeting-Notes/Subjectivity-in-LM-analysis-support,-overleaf-draft","filePath":"01 Fleeting Notes/Subjectivity in LM analysis support, overleaf draft.md","title":"Subjectivity in LM analysis support, overleaf draft","links":[],"tags":[],"content":"Rao: AI as an Ersatz Natural Science? cacm.acm.org/blogs/blog-cacm/261732-ai-as-an-ersatz-natural-science/fulltext\nSubjective and more subjective hype-fueling analysis:\nTeaching models to express their uncertainty in words arxiv.org/pdf/2205.14334.pdf\nEMNLP THeme paper: www.overleaf.com/project/6296937e4034478075a18a64\nMSFT proposal (same text): www.overleaf.com/project/62916d35de6fa13dbe23612c\nMSFT details: www.microsoft.com/en-us/research/academic-program/phd-fellowship/canada-us/"},"01-Fleeting-Notes/T2I-Model-Zoo-Provenance":{"slug":"01-Fleeting-Notes/T2I-Model-Zoo-Provenance","filePath":"01 Fleeting Notes/T2I Model Zoo Provenance.md","title":"T2I Model Zoo Provenance","links":["tags/blog-idea","tags/t2i"],"tags":["blog-idea","t2i"],"content":"blog-idea t2i\nThere are a lot of great posts out there explaining how diffusion models for text-guided image synthesis (and other text-to-image models more broadly) work. In particular, Lilian Weng’s post on diffusion models gives both a detailed mathematical characterization of the diffusion process and DM training, and narratively presents advancements such as DDPM and latent diffusion modeling in the rough order they developed. Yang Song’s post on score-based modeling also gives a great mathematical perspective that builds up to diffusion models from first principles, and will deepen your understanding. Maybe this post from Mario Namtao Shianti Larcher comparing HuggingFace diffusers models will cover similar ground that I cover in here, but I couldn’t read it as I don’t pay for Medium.\nIn this post I will instead focus on listing all the implementation differences between current popular text-to-image models, particularly with respect to which pretrained elements are shared and which differ, and which datasets they were used to pretrain on. This allows us to chart out an “ancestry” chart of which particular models went in to which systems. In particular, this allows us to compare diffusion models with respect to “openness” and replicability, and to trace their provenance.\nI made this post in the course of my work on interpreting and analyzing the behavior of text-to-image models; being able to analyze the training data directly allows us to formulate useful hypotheses to design experiments for a given model.\nSD Mainline\n\nStablediffusion 2.0 (stabilityai/stable-diffusion-2) is primarily trained from scratch on LAION 5B, then further trained on a 10M LAION subset (which presumably they chose) I would guess those images are definitely covered in 5B and might be in en 2B\n\nDetails on the base model here huggingface.co/stabilityai/stable-diffusion-2-base\nSeems like the training data is 100% within LAION 5B, but they subsample some subsets differently\n\n\nStablediffusion 2.1 is finetuned from SD2 using LAION 5B with a different use of the safety filter huggingface.co/stabilityai/stable-diffusion-2-1\nStableDiffusions 1.1-1.4 (CompVis/stable-diffusion) are trained on different subsets:\n\n1.1 is trained on LAION-2B EN primarily for 237k steps, with a futher update on laion-high-resolution (sampled from LAION-2B)\n1.2 is fine-tuned from 1.1 on the aesthetics subset of 2B-en\n1.3 is further finetuned from 1.2 on the same subset with less text conditioning\n1.4 is resumed from 1.1 on “laion aesthetics v2 5+” which I believe is a different 5B subset that also has bad LID\n\n\nUnCLIP 2.1 finetuned\n\nConditioned on CiT-L and ViT-H CLIP image emebeddings (for variation and mixing)\n\n\n\nDALL-E 1/2/3\nImagen\n\nUntil very recently Google’s Imagen model only existed internally. Looks like it’s available for API access through the “Vertex AI” platform for trusted users but I haven’t tried it yet.\nThe open Imagen-Pytorch model uses a pretrained T5 model for conditioning (IS IT FINETUNED) and the cascading DDPM architecture in Imagen. Uses classifier-free guidance\n\nTrained on: laion2B-en\n\n\n\nDALL-E mini/mega\n\ngithub.com/borisdayma/dalle-mini\nwandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal—VmlldzoxODMxMDI2#dall%C2%B7e-mega---training\nThis is actually a transformer GAN rather than diffusion model, but its wide use puts it within the milieu of the others discussed here\nBART encoder-decoder (normformer) maps t2i tokens\nCausal image token decoder outputs\nDEMega training log\n\nwandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal—VmlldzoxODMxMDI2\n\n\n\nSDXL\n\nStableDiffusion XL  is a whole other can of worms; they have a base model and a refiner model\n\nBase model is trained on an internal dataset to stability AI, I will investigate further but they’re less transparent abt it than the prior open models. I would assume it’s a curated subset of LAION tho\nThe refiner model appears to be trained on the same\ndetails on the training data might be findable via their source code here, I can investigate further github.com/Stability-AI/generative-models\n\n\n\nLingual adaptation variants:\n\nAltDiffusion\nJapanese StableDiffusion\n\nOthers:\n\nDeepfloyd IF\n\nT5, LAION-A 1.2B text-image pairs\n\nClaims training on romance languages also\n\n\nAlso an Imagen reproduction?\n\n\nMidjourney\n\nSeems to have been started from SD based on rumors?\n\nI think using at least some of the codebase is likely\nUsing the pretrained model is prob a violation of SD release license? (CHECK)\n\n\nDocumentation not open\nRelease dates for model “versions” www.wikiwand.com/en/Midjourney started in Feb 2022, latest in June 23\n\nwww.techspot.com/news/96619-midjourney-v4-greatly-improves-award-winning-image-creation.html\nCan I make guesses about use of the SD codebase for example as a base?\nwww.howtogeek.com/879711/midjourney-v5-creates-better-images-fewer-nightmare-hands/\n\n\nSeemed to do some in-house collection of “high quality photos” for training ^^ sourced from a discord server/reddit/twitter\nI am curious to see how the suit against them (also OpenAI and Stability are included) goes.\n\nWill the training dataset be revealed?\nIs it LAION?\n\n\n\n\n"},"01-Fleeting-Notes/T5-Strikes-Back":{"slug":"01-Fleeting-Notes/T5-Strikes-Back","filePath":"01 Fleeting Notes/T5 Strikes Back.md","title":"T5 Strikes Back","links":["tags/blog-idea","01-Fleeting-Notes/Blogpost-idea-\"the-bubble-in-screen-protector-research-agenda\""],"tags":["blog-idea"],"content":"blog-idea\nIt’s all about post training\nOr, “Prompt-tuners: T5 in disguise”\nT5 was preprinted five years ago as of this month. In my view, it is the originator of the LLM prompting paradigm. After pretraining on the C4 corpus, T5 was simultaneously fine-tuned on diverse NLP tasks including NLI, paraphrasing, summarization, and translation. Which capability is engaged during inference is not determined through some external routing module or direct conditioning switch, but inline, as text, with the rest of the input, eg. translate English to German: That is good.\n\nGiven the mature state of transfer learning at the time, that shared representations could benefit all the tasks in a parameter efficient manner was a reasonable prediction, which was vindicated by the success of the model. But as a novice NLPer at the time I was quite surprised that an LM could both process its instructions and perform the specified task so well at the time; even though GPT-2 had already been found too dangerous to be released shown able to complete impressive, novel writing segments, I hadn’t seen such strikingly good instruction-guided outputs before.\nDespite this, I caught in 2022 for calling T5 the genesis of prompting, as it isn’t a decoder model, and the large multitask decoder LMs of 2022 weren’t being trained to be prompted as T5 was either.\nIn 2024, though, I feel pretty vindicated. I would submit our use paradigm for LMs is now closer to the T5 paradigm than it was a couple years back.\n2022 was a heady time. When “emergent abilities” came in to the public consciousness we looked on in awe at what pure pretraining could get us. “Chain of thought” among others brought to the forefront prompt engineering, others. Benchmark after benchmark fell before the bitter lesson that simply scaling compute and data would bring about generalized capabilities.\nBut failure cases popped up too.\nPost training on so many tasks to effectively cover all end tasks/evals\nThrows back to the T5 era\n\none unified model was performing all the tasks they wanted\nthey EXPLICITLY were prompt-tuning by keying each task to a specific prompt (translate summarize etc)\ndifferent architecture of course\n\nI catch flak when I refer to T5 as the originator of prompting as a technique in LM use but I do really believe it\nMelanie Mitchell: “the reasoning debate heats up” aiguide.substack.com/p/the-llm-reasoning-debate-heats-up\n\nTom McCoy’s embers of autoregression paper uses probability bias on tasks like reversal as evidence that patterns in the train data are used rather than reasoning\nPaper from another group showing robustness breakdown on rephrased GSM8k questions\nSeems suggestive that matching to in-distribution patterns in the training data were there\n\nI think by a constructionist narrative we’re clearly returning to the T5 paradigm (with much more pretraining) by moving to all of these post-training tasks\n“bubbles in a screen protector” Blogpost idea “the bubble in screen protector research agenda”\nNatolambert explains post training and rlhf\nwww.aisummer.org/p/nathan-lambert-on-the-rise-of-thinking"},"01-Fleeting-Notes/Tasks-for-\"Mapping-is-not-Memorization\"":{"slug":"01-Fleeting-Notes/Tasks-for-\"Mapping-is-not-Memorization\"","filePath":"01 Fleeting Notes/Tasks for \"Mapping is not Memorization\".md","title":"Tasks for \"Mapping is not Memorization\"","links":["tags/paper-planning","01-Fleeting-Notes/Mapping-is-not-memorization"],"tags":["paper-planning"],"content":"2023-02-11 15:13\nTags: paper-planning\n\nMapping is not memorization\nExp. 1. Local Ball Around Final Latent\n\nPlotting relationship between t and % of recovered images\nNeeds:\n\n\nImage recovery metric testbench\n\nL2 norm between 2 images\nUse Carlini paper threshold for recovery\nPull N images from LAION and put in a tester\nUse same training prompts (captions?) will check Diff paper\nForeach image\n\nUse Kexun script to noise t steps forward, k times\nRun the backward (denoising) process from step t using Kexun script\nCount successes based on L2 under threshold\n\n\n\n\n\nExp. 2. Existence of Mapping for Memorized Data\n\nTracking the “key” noise for arbitrary training images\n\nReplicate Carlini paper training on CIFAR-10 splits\nSave trajectories (crucially, the initialization + maybe 5 intermediate points at specific t) for specific arbitrary training images\n\nCan be saved for the whole traj or intermed points\nSee above for training script mods necessary\n\n\nWe choose the images to track a priori, and hold out some to be completely random, and some to be semantically similar (for trajectory characterization purposes)\n\n\nSeeing if we can recover images from those key noises\n\nHow Carlini paper does replication\ngithub.com/openai/improved-diffusion\narxiv.org/pdf/2301.13188.pdf\nGood chance that text plays the dominant role\n\nCan we have a principled prediction of if text plays the dominant role?\nShares similarity with adverswarial problems.\nFinding maximum L2 difference influential noise directions in codespace around some image\n"},"01-Fleeting-Notes/The-Semi-meaninglessness-of--scaling-laws-or-How-Much-Did-the-Lunch-Cost":{"slug":"01-Fleeting-Notes/The-Semi-meaninglessness-of--scaling-laws-or-How-Much-Did-the-Lunch-Cost","filePath":"01 Fleeting Notes/The Semi-meaninglessness of  scaling laws or How Much Did the Lunch Cost?.md","title":"The Semi-meaninglessness of  scaling laws or How Much Did the Lunch Cost?","links":["tags/blog-idea","01-Fleeting-Notes/ACL23-Taking-Stock-Middleware-Master-Notes","01-Fleeting-Notes/Subjectivity-in-LM-analysis-support,-overleaf-draft","01-Fleeting-Notes/Do-AI-systems-really-have-their-own-language","Prajj-Lunch-Conversation"],"tags":["blog-idea"],"content":"2023-04-03 14:54\nTags: blog-idea\n\nMotivations:\n\nErsatz natural science\nMany hard-to-interpret results\nAre we overclaiming or underclaiming?\nWhere are the emergent abilities coming from\nJustifies:\nDefining the scope of the new science\nDataset analysis techniques\nModel-based model analysis\n\nModel-based automation to scale model evaluations\n\n\nEffort to define frontier tasks\nMoving behind cool-demo-ology\n\nScaling law for performance with data:\n“Linear scaling on most tasks with exponential in increased data” (excludes emergence)\n\n\nSpecializing Smaller Language Models towards Multi-Step Reasoning\n\n\nRepurposable:\nACL23 Taking Stock Middleware Master Notes\nSubjectivity in LM analysis support, overleaf draft\nDo AI systems really have their own language?\nSee Ongoing projects section\nPrajj Lunch Conversation\nDanish tweet on tradeoff between reasoning through memorization and acquisition of interesting capabilities\nInt\nI think it’s hard to find anyone who isn’t, in their heart of hearts, very surprised by the spectacular capabilities of large language models. I distinctly recall being surprised each time by the “unicorn story” generated by GPT-2, early evidence for the efficacy of task-specific prompting in T5, EXAMPLES EXAMPLES EXAMPLES emergent properties etc\nHowever there are some issues in the discourse around LLMs. First off, I think a degree of unreasonable foresight being retroactively claimed by eg. Sam Altman, motivated from a supposed “predictable” nature of gains in LM performance with scale. I think an important first step is to closely examine what these scaling laws actually are: blah blah blah\nIn fact, I am generally troubled by our rhetorical choice to describe what can more fairly be called “scaling observations” to laws. I believe a core inspiration for these observation-driven prognostications is Moore’s Law, which despite it’s name is not a natural law---it was first an academic observation in the late 1950s, which was then made an R&amp;D target in 1965 by the leadership of Fairchild Semiconductor and then later Intel.\nFor Moore’s law, the driving inputs were ever-shifting over time, while the main output variable---device complexity---remained fixed. Despite its seemingly slippery name, device complexity was in fact a measurable quantity, area transistor density at a fixed price. Realizing the business goal of Moore’s law required coordinating refinements to a shifting array of innovations, in terms of transistor design, materials, and fabrication process. In the 60s it was CMOS, in the 80s it was UV photolithography, more recently there has been a focus on 3D assembly of components. (Don’t hold me too closely on the above, I only have an undergraduate-level of EE knowledge, which was focused on signal processing anyway)\nMeanwhile, when it comes to the LLM “scaling laws,” there are several complications in relating them to other technology forecasting methods like Moore’s law. First of all, there’s not a unified LLM scaling law theory---different people (and sometimes the same people on different occasions) mean really different things when they make statements like “the gains in LLM performance are very predictable with scale.” Usually the sophisticated people who make this statement mean it rigorously, and they are referring to something like better perplexity performance over a larger dataset in compute-optimal conditions. However, audiences unfamiliar with the technical details of LLMs hear (and hype agents state) a non-rigorous version of the scaling hypothesis, referring to fuzzy concepts like “reasoning,” “understanding,” and the like. Despite my distaste for thinking of these concepts as following some kind of scaling law, I totally understand the interest---the shockingly impressive ability for systems like GPT-4 to act according to our intent from underspecified prompts across many domains and to generate fluent text and code are what we’re really excited about in these systems, not the ability for them to have a lower per-token surprisal over big datasets than previous techniques!\nThe fact that we can’t measure the thing we really care about is a massive problem, so big that at this point I believe it is the root of many of the issues in current AI discourse. I think this is where the assignment of sentience to LLMs comes from. I think this is what allows the magical thinking around AI doomerism to gain purchase. I think this is why we’re at a dead end in evaluation and all people can think to do is test LLMs on standardized tests. I think this is why psychologists, economists, and philosophers are trying to rush in and claim relevance by assigning things like a theory of mind or …\nI think this is why Eliezer Yudkowsky gets taken seriously.\n…\nWe really need a new science of “LLMology” that can answer these questions, and I think it will require novel theories and techniques that aren’t really comparable to any other field that has come before. While I’m very skeptical of the superintelligence crowd, I think even they should support this kind of a science. A better understanding of LLM capabilities will give us a better understnading of the risks they pose, if they do pose any. And perhaps if they do, having hard data to support the subjective AGI-like improvements that LLMs have given\nFirst is that there’s confusion over what exactly the dependent variable in the scaling relationship is. I see there being effectively a rigorous LLM scaling law claim and a vibe-based scaling law. I think a lot of people in the hype crowd like to play a rhetorical game where the strength of the rigorous claim is used to advocate for the vibe-based one.\nequation is flipped. As far as I understand it, the strong scaling hypothesis for LLMs is that with a paired increase in\nChinchilla arxiv.org/abs/2203.15556\nScaling laws for NLMs arxiv.org/abs/2001.08361 (Kaplan ea 2020 in Chinchilla)\nIt never hurts to be overly-optimistic about how LMs will work on your research task (LMaaJ skepticism)"},"01-Fleeting-Notes/The-parts-of-yourself-that-you-exercise-get-stronger":{"slug":"01-Fleeting-Notes/The-parts-of-yourself-that-you-exercise-get-stronger","filePath":"01 Fleeting Notes/The parts of yourself that you exercise get stronger.md","title":"The parts of yourself that you exercise get stronger","links":["01-Fleeting-Notes/The-parts-of-yourself-that-you-exercise-get-stronger"],"tags":[],"content":"A line from Ezra Klein on Hasan Minhaj about the reason why it is bad, actually to have actively unvirtuous people out and HAPPILY reveling in their cruelty and vice, responding to the cope people say about the “deportation ASMR” type shit saying that at least the cruelty that’s always been there is out in the open.\nHe’s talking about the “personality transplants.”\n“Back in the Hillbilly Elegy days, decency was very important to him. Virtue was important to him. That’s why he didn’t like Donald Trump.”\nSo many of them have imitated the worst part of Donald Trump’s personality\nHASAN: It’s cosplaying.\nEZRA: You’re not cosplaying, because you become what you force yourself to be. It’s like all the kids online who get into antisemitism online because it’s some ironic joke but become neo-nazis, The parts of yourself that you exercise get stronger.\nIt’s not better that the cruelty explicit because institutions form the people in them. When you begin to make part of the thing you have to do is to lie in public, or revel in suffering, or care about the constitution, that part of yourself begins to die.\nTo get ahead in Republican politics you have to lie about the 2020 election.\nFrom this episode, starting at 13:36:\n"},"01-Fleeting-Notes/Thick-Evaluation-(Qadri-et-al-2018-2025)":{"slug":"01-Fleeting-Notes/Thick-Evaluation-(Qadri-et-al-2018-2025)","filePath":"01 Fleeting Notes/Thick Evaluation (Qadri et al 2018-2025).md","title":"Thick Evaluation (Qadri et al 2018-2025)","links":[],"tags":[],"content":"Link: arxiv.org/pdf/2503.19075\nFrom FAccT, continually updated\nFocused on"},"01-Fleeting-Notes/Thinking-LM-as-a-judge-learning-to-eval":{"slug":"01-Fleeting-Notes/Thinking-LM-as-a-judge-learning-to-eval","filePath":"01 Fleeting Notes/Thinking LM as a judge learning to eval.md","title":"Thinking LM as a judge learning to eval","links":[],"tags":[],"content":"Generates prompts for (I think) pairwise preference scoring only over (seemingly) structured tasks\nThis is really interesting, but:\n\nOnly about generating rubrics\nOnly about preference pairs\nOnly about fairly coarse-grained/objectively evaluable methods\n\nSample datasets tested\n\nRewardBench\n\nWhat it is:\n\nFrom AI2\nChat (AlpacaEval, MT Bench (ratings for completions)\nMT bench hard pairs\nLLMBar completion comparisons\nRefusals for dangerous, offensive, refusal to answer when should\nReasoning tasks for Math\n“Reasoning” tasks as HumanEval\nAnthropic examples\n\n\nHow they used:\n\nPairwise\n\n\n\n\nFollowBench (Eval)\n\nCatches format, examples, style, situation, content\nThey made on top of FollowBench a pairwise check\n\n\nRM-Bench\n\nSpecific, fine-grained differences + relevant fact. (eg., confusing quantum entanglement with quantum superposition)\nModified to be pairwise\n\n\nJudgeBench\n\nResponse subtleties including reasoning, knowledge, math, and coding\n\n\n\nWhat do you think of this paper? Is this aligned with your research vision — if not, why not, and if yes, what do you see as limitations and what would envision if you were to add more to it?\nI think it’s a great paper. LM as a judge is pretty closely related to my research vision. However, LMaaJ only evaluates models post hoc from their outputs. I think what we really need is a system that dynamically elicits behavior from the model under test. The principal distinction I’d make between this work and my research agenda is that I want to make the planner both devise what the inputs are going to be and check them, rather than just checking outputs of predetermined inputs here.\nHere are some more granular thoughts.\nI really like seeing:\n\nhow using self-guidance to generate the samples for SFT+DPO  can be applied to scale LM judges\ncasting the task of building LM judges as something to automate rather than assuming human prompt engineering\nhow such a simple method performs well on all those different benchmarks\n\nThat being said, there are some limitations\n\nThey are assessing a well-worn set of judging tasks: correctness in world-knowledge QA, rule-following for things like length and output format, and refusal. Basically, it’s all chatbot stuff\nThey only look at pairwise evaluation, which is useful for coarse, relative system comparisons (and to a limited extent FT data filtering) but cannot be applied for absolute system acceptability judgements, which I think is the more important and difficult task\nThis method may be brittle to harder desiderata in more complex domains\n\nBy strictly using self-learning to select training samples, the model probably won’t improve much in the kinds of “plans” it can execute\nIn their Fig 1. example, they show it can check that the correct list of attributes are provided, and formatted correctly, but what about more complex, deeper info? Recall of fundamental facts? Stylistic attributes? This technique has no answer for this\nAnd addressing that issue is hard: within a pairwise preference signal attributes such as informativeness or recall over specific necessary input information probably won’t be caught, I suspect this is the same problem we have in tying vision+language modalities, there is not a sufficient supervisory signal for the model to fit everything we want it to\n\n\n\nI definitely think this method will be an important part of the toolbox for this research vision. At the very least as a baseline. I think natural follow ups to this work to address these limitations may include:\n\nIdentifying more complex domains than pairwise evaluation, maybe style or information preservation, and looking for supervisory signals on Likert scales\nDeveloping an evaluation planner that can use tools instead of just executing the entire evaluation plan LM-modulo (this would allow it to be better applied to code gen tasks for example)\nInvestigating if this technique can make a “base model” for LMaaJ to bootstrap a small set of gold preference pairs into a judge for some specialized task.\nAdding “building the input plan” as part of the evaluation.\n"},"01-Fleeting-Notes/To-read":{"slug":"01-Fleeting-Notes/To-read","filePath":"01 Fleeting Notes/To read.md","title":"To read","links":[],"tags":[],"content":"On The Beach"},"01-Fleeting-Notes/Tool-use-t2i":{"slug":"01-Fleeting-Notes/Tool-use-t2i","filePath":"01 Fleeting Notes/Tool use t2i.md","title":"Tool use t2i","links":[],"tags":[],"content":"SEPHIROTH\nAERITH\nBARRET\nCLOUD\nSNAKE\nAgentic\nEvaluation\nR\nImage\nTool\nHandling\nSegment\nN\nA\nK\nEvaluation\nTool-use\nSegmentation\nCounting\nModular\nRequirement"},"01-Fleeting-Notes/Towards-Behavior-Driven-AI-Development":{"slug":"01-Fleeting-Notes/Towards-Behavior-Driven-AI-Development","filePath":"01 Fleeting Notes/Towards Behavior-Driven AI Development.md","title":"Towards Behavior-Driven AI Development","links":["tags/blogpost-notes","02-Document-Notes/Master-list-of-todos-for-PECO-EACL-camready"],"tags":["blogpost-notes"],"content":"2023-03-28 17:15\nTags: blogpost-notes\n\nBlogpost link\nBased on a paper called Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning\n\nUseful for mining for future reference\nIt is a currently active Python package for use in dataset analysis, but it lacks the powerful integrations with training models that I’m building\n\nRelates to Master list of todos for PECO EACL camready\n\nThey basically have a startup containing a much more capable copy of DatasetAnalysis\nThey sell this as being an enabler of “Behavior-driven AI dev” which is a desirable paradigm to them\n\nThey rightfully point out that many important elements aside from accuracy under numerical metrics are desirable\n\nThey point to the fact that there are ad hoc fairness toolkits, robustness analysis libraries, etc\n\nAll good to investigate as integrations\nThey exclusively work on the problem of “behavior as performance on metadata-identified subgroups”\n\nWorth considering looking into behavior on automatically-identified subgroups\n\n\n"},"01-Fleeting-Notes/We-Need-a-Team-America-Mentality-for-Tech":{"slug":"01-Fleeting-Notes/We-Need-a-Team-America-Mentality-for-Tech","filePath":"01 Fleeting Notes/We Need a Team America Mentality for Tech.md","title":"We Need a Team America Mentality for Tech","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"2023-04-01 19:55\nTags: blog-idea\n\nwww.youtube.com/watch\nTeam America: World Police includes some brutal mockery of celebrity condemnations toward the Iraq war. While the celebs are (mostly) right on this, the point of the crit is to point out that they aren’t really the best authorities on issues of international politics. They aren’t the best advocates for their positions. They can be VERY wrong on other things.\nParker and Stone’s attitude toward them is more that they’re lame. This is coming from entertainment industry insiders.\nWe need a mentality like this toward tech types who pontificate on the world from inside. And we need double scrutiny toward tech “community” types without expertise e.g. Yudkowsky\nFor instance: I think Marc Andreessen is right that “It’s Time To Build.” But why the fuck should we listen to him on that perspective and not economists, etc? Why do VCs opinions on cancel culture really matter? etc\nTech types, especially bay area tech types live in a suffocatingly small bubble.\nMany of these takes are at best, “I’m a smart guy wrt X, so I’m  a smart guy in general” mentality overgeneralizing to give them the misplaced belief in the inherent correctness of their (not necessarily well-considered) opinions\nWhat pushed me over the edge from “this annoys me but it’s not worth saying” to “I think we need a culture within tech of pushing tech types to know their place” is the FLI open letter on AI and accompanying drone strike the supercomputing centers discourse.\nWith respect, I think Russell might be a bit high on his own supply and locked in to the alignment direction. I actually 100% accept all premises about why alignment is a hard problem. I also think it’s a useful line of research, as an inability to properly specify goals does cause issues. But his treatment of the issue is very GOFAI/RL centric and doesn’t really comport with the understanding of representation learning I have, and RepL really is the tech at issue in the conversation of anyway…\nRunning list:\na16z.com/2020/04/18/its-time-to-build/\nSee also Lex Fridman podcast:\nwww.businessinsider.com/lex-fridman-podcast-anti-woke-elon-musk-ai"},"01-Fleeting-Notes/Who-Cares-how-LMs-answer-surveys":{"slug":"01-Fleeting-Notes/Who-Cares-how-LMs-answer-surveys","filePath":"01 Fleeting Notes/Who Cares how LMs answer surveys?.md","title":"Who Cares how LMs answer surveys?","links":["tags/paperidea","tags/paper-ideas","References/Investigating-Cultural-Alignment-of-Large-Language-Models","02-Document-Notes/Wtf-is-a-persona!","References/Randomness,-Not-Representation_-The-Unreliability-of-Evaluating-Cultural-Alignment-in-LLMs"],"tags":["paperidea","paper-ideas"],"content":"paperidea paper-ideas\nOn lots of areas, including cultural alignment (Investigating Cultural Alignment of Large Language Models), politics (Shangbin paper of LM bias), and [other examples], people are asking research questions that revolve around the distribution of answers to preference questions or the “opinions of the model”\nWho cares?? When are there settings where this matters?\nOne potential area is in treating the model as a persona, (here Wtf is a persona?! is answered by output personas).\nThere are applications of personas, including simulated surveys (which is dubious imo) and sometimes LMs may be employed for decision making, but I want to check this:\nDo the choices LMs make in surveys actually matter for any meaningful downstream behavior?\nDo LMs act in accordance with their “preferences” in these surveys in the real world? (this has probably been tried, look into it later)\n\nAre reasoning models more likely to behave this way? Ie., because they think out loud\nThis is definitely an alignment question\n\nMore interesting: are LMs’ understanding of the world affected or biased by the “opinions” they have\n\nOne negative example (Arora et al) ref’d in Randomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs, S2.3: Survey-based assessments\nLM preferences are indeed sensitive to prompting (Investigating Cultural Alignment of Large Language Models)\n"},"01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost":{"slug":"01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost","filePath":"01 Fleeting Notes/Why I'm Not worried about superintelligence blogpost.md","title":"Why I'm Not worried about superintelligence blogpost","links":["tags/blog-idea","01-Fleeting-Notes/Blogpost-based-on-my-tweet-thread-with-the-guy-about-AI","01-Fleeting-Notes/I-HATE-LONGTERMISTS-Vol-5","01-Fleeting-Notes/Overconfidence-in-Deep-Learning-Discussions","01-Fleeting-Notes/Rationalists-are-like-Soylent","05-Snippets/The-Hinge-of-History-Peter-Singer"],"tags":["blog-idea"],"content":"2023-01-08 15:53\nTags: blog-idea\n\nLink dump:\nBlogpost based on my tweet thread with the guy about AI, I HATE LONGTERMISTS Vol 5, Overconfidence in Deep Learning Discussions tweet on the “desire for power” with scale\n\nA big core element of this story is that I think X-risk is a meme coming from the closed circlejerk of lesswrongery\n\n“Thank god I went to a party school” : I detest the eggheadism all the time attitude and public behavior of the tech culture types (lesswronging, ) Rationalists are like Soylent\nThings I do genuinely admire about them: thoughtfulness and knowledgability\n“There was a time when I was very impresed by Slate Star Codex”\nOverapplication of historical analogues (this is a much bigger issue than just AI hype agents (e.g., whatifalthist)) example on analogies to LLM revolution\nHype being pushed by people who own startups in the hypezone exhibit A\n\nWhy don’t we weigh the failure rate of startups in lending creedence to these people\n\n\nAI panic/x-risk is negative AI hype in the same way “Evil US always wrong and always bad” is negative American exceptionalism (Triggered by this thread where it opens with reasons to be concerned about bias, etc and ends with Robert Miles and Human Incompatible)\n\n\n\nSee also, Yejin’s response to questions about sentience, x-risk\nwww.nytimes.com/interactive/2022/12/26/magazine/yejin-choi-interview.html\nTo be fair, even most lesswrongers don’t really buy into the idea that sentience is an issue or likely (and I agree)\nNotes from a post comparing GPT3/ChatGPT as simulators astralcodexten.substack.com/p/janus-simulators\n\nGettier arguing against justified-true-belief (JTB) condition for knowledge, e.g. “guy lights fire to cook food. flies like the smell, no smoke from fire so far. distant guy sees flies cloud, thinks it’s smoke, says ‘there’s a fire’, doesn’t actually KNOW there’s a fire bc his reasons are false”\nThen SuperIntelligence by Bostrom is invoked.\n\nI find it plausible to think that there’s an intelligence upper bound for something gained from training on all the media we’ve ever produced\nEffectively, no free lunch\n\n\n\nPeter Singer has issues with this meme very bigly: The Hinge of History Peter Singer\nCited in Emile Torres’s more aggressive take: mileptorres.substack.com/p/effective-altruism-is-a-dangerous\nKnutsson:\n\nI am most concerned about someone who finds it extremely important that there will be vast amounts of positive value in the future and who believes I stand in the way of that. … [A]mong some in EA and existential risk circles, my impression is that there is an unusual tendency to think that killing and violence can be morally right in various situations, and the people I have met and the statements I have seen in these circles appearing to be reasons for concern are more of a principled, dedicated, goal-oriented, chilling, analytical kind.\n\nWhat are some reasons to not expect current LLM impressive results to carry over to more general intelligence, lead to performance\n\nGenerative != embodied\n\nLanguage and art are modalities that in hidsight fit well for big data modeling. The machine can very naturally operate in the modality the data exists in\nWhat “modality” do real-world actions fit in? How can generative approaches be applied to them?\nFor example, how can a robot learn from video of humans doing activities to model them in a sophisticated way analogous to how it learns writing from human actions?\nIt could produce descriptions or video frames of people performing actions, but you need to “break out” into the true modality and “close the loop”\nGenerative approaches fit in with imitation learning, but this is different in some potentially important ways. We can’t distill down every sensory input, perceptual phenomenon, etc that is percieved by a person and export it for use by a learner, whereas the native modality of language is purely words\n\n\nHumans can’t design and build human+1 intelligence, why should we expect AIs to? Especially because there’s good reason to believe that continuing the increase will get harder and harder\n\nMight be analogous to going at relativistic speeds. Energy reqs are super high\n\n\n\nIn this blogpost I need to essentially beg forgiveness for such heavy argument from analogy. However, everyone on all sides of this debate is guilty of this sin. Analogy-driven argument might be more acceptable in philosophical debates (which this also is) than it is in scientific debates\n”Why I was wrong about LLMs”\nGives arguments for why he thinks he misunderestimated LLMs and DL\nwww.inference.vc/we-may-be-surprised-again/\n“No free lunch-flavored argument”\n\nMaybe the takeaway is that no free lunch is still true, but we should be very skeptical about estimates of the price of the lunch\ntwitter.com/giffmana/status/1640811708313751562\n\nTies beautifully into “we don’t know how much lunch we’re getting or how much we’ve paid” argument\nIs the lunch generalized reasoning capabilities or EXTREMELY high fidelity roughly semantically-equivalent memorization\nEither of those capabilities are awesome but one is very different from other\nAlso the wild diversity of text out there points to the lunch fare we’ve paid\n\n\n\ntwitter.com/xriskology/status/1642155567971024897\nYudkowsky being so steeped in scifi thought that even his proposed solutions hinge on nanotech\n\nEarlier in thread: EY predicted nanobots by 2010 and singularity by 2021\n\nApocalyptic AI Post from Sacasas\nLink (Substack)\n\n5. The Enlightenment did not, as it turns out, vanquish Religion, driving it far from the pure realms of Science and Technology. In fact, to the degree that the radical Enlightenment’s assault on religious faith was successful, it empowered the religion of technology. To put this another way, the Enlightenment—and, yes, we are painting with broad strokes here—did not do away with the notions of Providence, Heaven, and Grace. Rather, the Enlightenment re-framed these as Progress, Utopia, and Technology respectively. If heaven had been understood as a transcendent goal achieved with the aid of divine grace within the context of the providentially ordered unfolding of human history, it became a Utopian vision, a heaven on earth, achieved by the ministrations Science and Technology within the context of Progress, an inexorable force driving history toward its Utopian consummation\n6. It is also important to be a bit more specific, and to classify the religion of technology more precisely as a Christian heresy. It is in Western Christianity that Noble found the roots of the religion of technology, and it is in the context of post-Christian world that it has presently flourished.\n\n\nWe get our word apocalypse from a Greek word meaning “to reveal, to disclose, or to uncover.” What I am suggesting is that AI, as it is being developed, deployed, and hyped (and criti-hyped), forces us to reckon with the fact that modernity is expiring, and it is expiring precisely to the degree that it no longer serves the interest of and is at various points, particularly in its techno-economic dimensions, openly hostile to the human person.\n"},"01-Fleeting-Notes/Why-eval-startups-fail":{"slug":"01-Fleeting-Notes/Why-eval-startups-fail","filePath":"01 Fleeting Notes/Why eval startups fail.md","title":"Why eval startups fail","links":["tags/blogpost-notes","tags/ai-industry","tags/evaluation"],"tags":["blogpost-notes","ai-industry","evaluation"],"content":"blogpost-notes ai-industry evaluation\nFrom Thomas Liao\nthomasliao.com/eval-startups\n\nI haven’t seen any succeed, outside the safety evals niche\n\n\nTalent attrits because good eval ppl have skills to improve the models\n\n\nclients have to be technical developers who want to build with APIs, but also not technical enough to run their own evals\n\nHill climbing\nSafety evals are the only successful startups because:\n\nIdeological motivation to do safety work\nCustomers are technical\nWhole point is they’re independent audits\n\n\nThere are also startups that want to sell research evals to big labs. These will fail, because the primary point of research evals is to set research directions, and big labs will never outsource setting their research agenda.\n\nThis is a great point."},"01-Fleeting-Notes/Words-I'd-add-to-English":{"slug":"01-Fleeting-Notes/Words-I'd-add-to-English","filePath":"01 Fleeting Notes/Words I'd add to English.md","title":"Words I'd add to English","links":["tags/blog-idea"],"tags":["blog-idea"],"content":"blog-idea\ninshallah\nsegún\n卷\nせっかく？やっぱり"},"01-Fleeting-Notes/Yixin-Liu-visit":{"slug":"01-Fleeting-Notes/Yixin-Liu-visit","filePath":"01 Fleeting Notes/Yixin Liu visit.md","title":"Yixin Liu visit","links":[],"tags":[],"content":"This is great shit, very related to my interests\nTalks about this:\narxiv.org/abs/2501.00560\nPaper on evaluating automated replacements for Chatbot Arena\n\nProblem: correlation breaks down on similar models\nQuestion: do you think chatbot arena correlation is a meaningful measure at close range, or is this a problem with the automated methods?\n\nSecondary evaluation here might make more sense\n\n\n\nEvaluation consistency:\n\nSwapped order sometimes breaks the consistency\nThey filter out samples where this takes place before checking for the consistency on Arena Hard\nIn general, there is a very high consistency between generator and evaluator quality\nWhat impact does model self-preference play? Bairu asked this question…\n\nI am wondering how much the correlation effect is driven by self-preference\n\n\nA better LM judge is a better generator\n"},"01-Fleeting-Notes/You're-using-the-wrong-perplexity":{"slug":"01-Fleeting-Notes/You're-using-the-wrong-perplexity","filePath":"01 Fleeting Notes/You're using the wrong perplexity.md","title":"You're using the wrong perplexity","links":["tags/blog-idea","tags/blogpost-notes","Argument-about-perplexity-in-a-review"],"tags":["blog-idea","blogpost-notes"],"content":"blog-idea blogpost-notes\nDraft\nen.wikipedia.org/wiki/Perplexity\nTL;DR: “perplexity” the measure of LM fit and “perplexity” the model-based measure of information content of a corpus are slightly (but importantly！）different. Use the right one!\nFor a long time researchers in NLP have used language modeling as a way to create concrete proxy measures for linguistic complexity, difficulty, or reading time by employing information-theoretic measures that can be derived from LMs.\nThe perplexity of some source X following distribution p is defined as the exponentiated entropy H\nPPL(p)=2^{H(p)}=2^{-\\sum p(x)\\log p(x)}=\\prod_xp(x)^{-p(x)}\nThis “surprisal” is …\nIn Jurafsky and Martin they introduce the other kind of perplexity, intended to assess how well fit model p is to the training distribution X actually involves computing the cross-entropy between the model’s probability distribution p and the empirical, observed distribution of X, or H(p,q).\nH(p,q) corresponds to the expected value of -E_p[\\log(q)], or:\nH_X(p,q)=-\\sum_{x \\in X}q(x)\\log(p(x))\nWe can do a useful trick: the set of elements in X represents the empirical probability distribution of X. If x_i is present in X n times, then q(x_i)=n/|X|, and x_i will show up n times in the sum! So, we have\nH_X(p,q)=-\\frac{1}{N}\\sum_{x\\in X}log(p(x))\nIn turn, this gives us the LM fit perplexity, for model p, or:\nPPL_\\textrm{LM}(p,X)=2^{H_{X}(p,q)}=2^{-1/N\\sum_{x\\in X}\\log(p(x))} = something\\prod_xp(x)\nThis is very useful: all you need to do to check how well-fit a model is to a collection of text is take the product of the LM head posterior prob. (confidence) in the correct token!\nBut, the only reason this formulation is equivalent is because we are measuring the degree of fit of the LM probability model p to the empirical distribution. When we are using an LM to characterize small populations of specific text we don’t want to assume we are seeing the true distribution.\nThe problem is sometimes NLP researchers want to evaluate models, and other times they want to evaluate text!\n in text and data generated by models, and am interested in relating this to outputs from other models as well as humans. In this setting terminological issues can sometimes arise.\nArgument about perplexity in a review"},"01-Fleeting-Notes/abstracts-for-talks":{"slug":"01-Fleeting-Notes/abstracts-for-talks","filePath":"01 Fleeting Notes/abstracts for talks.md","title":"abstracts for talks","links":[],"tags":[],"content":"defense\nA fundamental open question in AI research is: “how can we confidently know what a model can reliably do?” Currently, we often discover model limitations through post-deployment failures rather than proactive assessment. I will present novel methodologies for rigorous pre-deployment evaluation, focusing on generative text-to-image models in multilingual and multicultural contexts. In this talk, I will introduce new frameworks for analyzing these models’ cross-cultural knowledge representation and performance, presenting previously unidentified optimization challenges in balancing multiple competing objectives: cross-language consistency, preservation of cultural nuance, and mitigation of harmful biases. Through careful empirical analysis, this work has revealed significant gaps in existing evaluation approaches and challenged several commonly held assumptions about model capabilities. The methodologies introduced here extend beyond text-to-image models, offering broader insights into AI system assessment. I will demonstrate how more precise evaluation techniques can uncover unexpected failure modes and provide clearer pathways for future research in reliable AI development, leading to better empirical progress, scientific understanding, developer informedness, and public awareness.\nGoogle\nTitle: On multimodal metrics and multilingual text-to-image models\nAbstract: Evaluating multimodal models is hard, yet there are lots of interesting things to evaluate! In this work I will present two projects about evaluating text-to-image (T2I) models. First I will talk about prompt coherence metrics, which analyze whether a generated image reflects the semantics requirements of its prompt. It turns out that due to poor meta-evaluation of these metrics, the T2I community mistakenly focuses on overly expensive techniques to evaluate their models. Then, I will talk about a line of work evaluating the multilingual and multicultural performance of T2I models. Despite being trained on ostensibly English-only data, many T2I models pick up inconsistent multilingual performance as well as positive and negative secondary behaviors, memorizing implicit cultural knowledge and perpetuating language-specific harmful output trends. Carefully introducing new evaluation targets presents new avenues for understanding and improving models."},"01-Fleeting-Notes/linkdump":{"slug":"01-Fleeting-Notes/linkdump","filePath":"01 Fleeting Notes/linkdump.md","title":"linkdump","links":["02-Document-Notes/Master-list-of-todos-for-PECO-EACL-camready","01-Fleeting-Notes/How-PECO-differs-from-Competency-Problems","Notes-from-SoCalNLP","01-Fleeting-Notes/D3JS-Notes","01-Fleeting-Notes/NLI+-for-LLM-Eval","01-Fleeting-Notes/ACL23-CoCoCroLa-Master-Notes","Transparency-PosPaper-TODOs-master","01-Fleeting-Notes/Mapping-is-not-memorization","01-Fleeting-Notes/The-Semi-meaninglessness-of--scaling-laws-or-How-Much-Did-the-Lunch-Cost","01-Fleeting-Notes/ACL23-Taking-Stock-Middleware-Master-Notes","01-Fleeting-Notes/Subjectivity-in-LM-analysis-support,-overleaf-draft","01-Fleeting-Notes/Do-AI-systems-really-have-their-own-language","Survey-paper-on-transparency-and-NLP","01-Fleeting-Notes/Andreas-Agent-Models-EMNLP22","01-Fleeting-Notes/Notes-from-Text-is-the-Universal-Interface","01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost","05-Snippets/The-Hinge-of-History-Peter-Singer","02-Document-Notes/RLHF,-Alignment-Deep-Dive"],"tags":[],"content":"List of random links to replicate map architecture from various private settings:\nY1-3\nPECO\nMaster list of todos for PECO EACL camready\nrelated work\nHow PECO differs from Competency Problems\nMaster list of todos for PECO EACL camready\nNotes from SoCalNLP\nD3JS Notes\n2023.aclweb.org/calls/system_demonstration/\nInformativeness and Invariance\ntwitter.com/RyoKamoi/status/1633135168964935680\nNLI+ for LLM Eval\nwww.bigpictureworkshop.com/call-for-papers\nCCCL\nACL23 CoCoCroLa Master Notes\nOther notes\nvisualcommonsense.com/\narxiv.org/pdf/2302.10893.pdf\nstevenyzzhang.github.io/website/\narxiv.org/pdf/2302.03675.pdf\nCross-lingual retrieval augmented prompt for low-resource languages arxiv.org/pdf/2212.09651.pdf — deals with prompting and multilingualit\nhuggingface.co/rinna/japanese-stable-diffusion\nTransparency\nTransparency PosPaper TODOs master\nMem\n\nMapping is not Memorization\n\nMapping is not memorization\n\n\n\nPlanning stage\n\nFollow up on PECO paper by reproducing Swabha dataset production pipeline with latest GPT3, generating augmenting set to the superset of all the DSs\nLunch cost blog\n\nThe Semi-meaninglessness of  scaling laws or How Much Did the Lunch Cost?\n\n\n\nIdeas stage\n\nNLP Middleware: the argument for continuing to improve trad benchmarks\n\nACL23 pospaper ACL23 Taking Stock Middleware Master Notes\nSubjectivity in LM analysis support, overleaf draft\nDo AI systems really have their own language?\nSurvey paper on transparency and NLP\nAndreas Agent Models EMNLP22 notes: Andreas Agent Models EMNLP22\n\n\nHuman Language learning as model for LM\n\nEven pidgeons make causal inference in skinner boxes to spurious signals\nCognitive analogy in OthelloGPT\n\nTrained a decoder LM just on Othello gameplay\nFind evidence for game state encoding inside the weights of the NN\nemploy a tortured crow arranging seeds analogy\nExperiment:\n\nFinding the encoding of the game state in the model activations\nModify to a different game state to change legal moves??\nFind that this works???\nNeed to read full paper for details\n\nHigh accuracy nonlinear probes\n\n\nMy main question is, how do they know it’s the “model-internal world state?” Based on activations of the concept vectors from the probe model?\n\nAnswer: intervening adversarially on the state space using the probe classifier at locations\n\n\n\n\nThink about the weaknesses in this analogy\n\nEfficient coding of an internal representation of a game state given valid descriptions of the game (which has a finite ruleset and bounded state space) is possible\nDo the implications they claim for language understanding generalization really apply?\nThey address a lot of this in the end but I dislike the implicature\n\n\nSeed analogy is frustrating.\n\n\nastralcodexten.substack.com/p/janus-simulators\n\nSimulator point is similar/backup for this\nnostalgebraist.tumblr.com/post/705192637617127424/gpt-4-prediction-it-wont-be-very-useful\n\n\nsuzyahyah.github.io/misc/2022/11/27/LLM-conscious.html\nNotes from Text is the Universal Interface\n“Theory of mind” paper a good example of my issues with this\n\ntwitter.com/m2saxon/status/1623583216803389440\nMechanistically, the machine has no subjective separation from the agent outside of prompts\nIt is very meaningfully not agentic\n\n\nWhy do tourists in scitwitter bitch about “moving the goalposts”\n\n\nAttempt to explain CoT abilities based on samplewise analysis for some problems\nBlogpost on the xrisk bullshit Why I’m Not worried about superintelligence blogpost\n\nwww.buzzfeednews.com/article/tedchiang/the-real-danger-to-civilization-isnt-ai-its-runaway\nThe Hinge of History Peter Singer\n\n\nBlogpost on basically the “story” of a paper that I produced (PECO would be a good one)\n\nBasically doing a George Lucas extended editions\n\n\nTry to get in to JSALT 2023 for summer if no internship\n\njsalt2023.univ-lemans.fr/en/index.html\nwww.clsp.jhu.edu/2023-jelinek-summer-workshop/\n\n\n{Dis}Ability in AI @ NLP venues: reach out to Widening NLP\n\nEmailed Maria to ask about this, potential to do some sort of event/collaboration at EMNLP23\nWhen she comes back with a response, you can email winlp-chairs@googlegroups.com to open communication\n\n\nRLHF, Alignment Deep Dive\nOpenChatGPT docs.google.com/document/d/1V3Td6btwSMkZIV22-bVKsa3Ct4odHgHjnK-BrcNJBWY/edit# (OpenAssistant) from LAION\n\nSubmission to CACM for opiniony pieces cacm.acm.org/about-communications/author-center/author-guidelines/\nBlogpost:\n\nMeanings of generalization\n\nSample-to-concept\nConcept-to-phenomenon\nMulti-task\nMulti-domain\nUnderstanding differences between these crucial in the age of scaling laws, etc\n\n\n\nAdaptivity blogpost\n\n\narxiv.org/pdf/1902.10811.pdf\n\nThey find the linear trend in translation of adaptivity performance\nRelated to linear trends in our artificially constructed test sets\n\n\n\nWe are witnessing the birth of a new natural science (Rao)\n\n\nAlchemy metaphor\n\n\nPhysics metaphor\n\n\nWe are finding new phenomena that have to be measured and characterized\n\nIs this thing we actually see really a  phenomenon?\n\n\n\nRight now, we are basically where the Enlightenment scientists were. We have our eyes. We have maybe basic rulers (test datasets) and maybe thermometers and probes and telescopes\n\n\nWe need to build the oscilloscopes, cloud chambers, particle accelerators, and rovers of AI\n\n\nWe are beyond the realm of mathematical analysis. We need to build tools\n\n\nGood news: building them is very relatively cheap. Designing the experiments they do is HARD and requires thought\n\n\nBack in the original days, SIGNIFICANT argument about basic ideas was taking place. Even in fields like rare diseases, debate still goes on but agreement increases over time\n\n\nTo me, efforts like debiasing NLI datasets is comparable to aligning a telescope or making a beam idk something\n\n\nReasons why emergent capabilities might happen:\n\nX axis in the emergent capabilities papers might be unreasonably restricted\n\nNetworking at a conference, making friends, using twitter as a latch\nHow far can we trust resources built on bad tools?\n\nMaybe we can trust clip to filter generative image datasets because it’s good for associating even if its bad at discriminating\nBut can we trust OOD NLI?\n\nDaSAI dataset analysis of reasoning errors generated by an LLM elicited from NLI stimuli"},"02-Document-Notes/Cultural-variation-in-response-scales":{"slug":"02-Document-Notes/Cultural-variation-in-response-scales","filePath":"02 Document Notes/Cultural variation in response scales.md","title":"Cultural variation in response scales","links":["tags/paper-notes","tags/research-extradisciplinary","tags/research-culture"],"tags":["paper-notes","research-extradisciplinary","research-culture"],"content":"paper-notes research-extradisciplinary research-culture\nAcademic results\nRESPONSE STYLE AND CROSS-CULTURAL COMPARISONS OF RATING SCALES AMONG EAST ASIAN AND NORTH AMERICAN STUDENTS\n\nResponses to fifty-seven 7-point Likert-type scales were analyzed. The Japanese and Chinese students were more likely than the two North American groups to use the midpoint on the scales; the U.S. subjects were more likely than the other three groups to use the extreme values. Within each cultural group, endorsement of individualism was positively related to the use of extreme values and negatively related to the use of the midpoint.\n\nThis difference that they find is statistically significant, and can be described as between the East Asian (from Japan and Taiwan) students and North American students (from US and Canada), not a strictly country-level difference (although preference for extreme points was more pronounced in the Americans)\nCultural differences in responses to a Likert scale\n\n[Chinese and Japanese respondents] more frequently on items that involved admitting to a positive emotion than did the Americans, who were more likely to indicate a positive emotion.\n\nDo Country and Culture Influence Online Reviews? An Analysis of a Multinational Retailer’s Country-Specific Sites\n\nCompare FR, DE, UK, US, JA Amazon reviews.\nMean reviews in western countries are all 4.26 \\pm 0.3, while the mean Japanese score is 3.57.\nMean “Review was helpful” vote rate in western countries was 65\\pm4\\% while in Japan was 75%.\nThis highlights how cultural differences in negative response bias and extreme response bias drive single-unrelated-item incomparability between countries\n\nUnderstanding the Impact of Culture in Assessing Helpfulness of Online Reviews\n\nIn the ACM social media studies literature\nSimilar result to above but for Arabic vs English\nCategory-specific differences in hotel reviews and book reviews\nIn hotel reviews the difference between extreme and mid response biases shows up, with Arabic reviewers also exhibiting a relative hesitance to provide 5-star reviews\n\nExtreme Response Style in Cross-Cultural Research\n\nEarly discussion of the existence of ERS and that ignoring it confounds comparisons of genuine group differences\n\nTbh, I think this completely calls into question the value of international happiness studies for ex\n\n\nThey compare Korean and American university students on interpersonal and sociopolitical trust\nRelative lack of extreme response bias in Korean students renders their extreme responses more “reliable” wrt consistent agreement\n\nCaveats\nResults showing that differences are still correlated?\nNon-academic discussion\nExcerpted from a deleted OpenAI employee tweet:\n\nthere’s another old story i heard about how an early GPT model stopped speaking Croatian one week and nobody could figure out why. turns out Croatian users were much more prone to downvote messages so the model just gave up and decided to not speak Croatian anymore.\n\nThis effect has been memed before, such as in this example about interface feedback:\n\nThis effect also plays out in restaurant ratings, from the Japan Times:\n\n“Tabelog users tend to not give high ratings just because they find a (restaurant’s food) delicious but rather … they tend to rate restaurants carefully, considering them in relation to other renowned establishments,” he says.\n\n\n“(It could) also be an ability to want to see things both ways or to not stand out too much from others,” says Farrer. “This means Japanese people are somewhat more likely to choose a three rather than a one or a five.”\n\nA tourism guide reports an (unscientific) description of the impacts of this trend:\n\nAs the number of English reviews increases, the reliability decreases… A popular chain of conveyor belt sushi eateries in Japan would be a great example. One of its locations, in a neighborhood in Tokyo that’s very popular among tourists, has a 4.3-star rating. Another location, in a quiet, rural area of Hyogo prefecture, has a 3.2-star rating. This chain goes through strict measures to ensure product and service consistency across its locations, and yet look at the discrepancy in ratings!\n"},"02-Document-Notes/Drafts/Culture-as-practice-drafting":{"slug":"02-Document-Notes/Drafts/Culture-as-practice-drafting","filePath":"02 Document Notes/Drafts/Culture as practice drafting.md","title":"Culture as practice drafting","links":["tags/research-evaluation","tags/research-culture","tags/research-lm","01-Fleeting-Notes/Culture-as-Practice-Meeting-Notes","01-Fleeting-Notes/Culture-as-Practice-textbarf","02-Document-Notes/Wtf-is-a-persona!","References/Cultural-Conditioning-or-Placebo_-On-the-Effectiveness-of-Socio-Demographic-Prompting","References/ChatGPT-Doesn`t-Trust-Chargers-Fans_-Guardrail-Sensitivity-in-Context","02-Document-Notes/Cultural-variation-in-response-scales","References/Scaling-Laws-Do-Not-Scale","References/Investigating-Cultural-Alignment-of-Large-Language-Models","References/Randomness,-Not-Representation_-The-Unreliability-of-Evaluating-Cultural-Alignment-in-LLMs","References/The-Ghost-in-the-Machine-has-an-American-accent_-Value-conflict-in-GPT-3.","01-Fleeting-Notes/Attributes-for-text-evaluation","References/Bias-in-Language-Models_-Beyond-Trick-Tests-and-Toward-RUTEd-Evaluation","Building-blocks-for-metrics","01-Fleeting-Notes/Thick-Evaluation-(Qadri-et-al-2018-2025)"],"tags":["research-evaluation","research-culture","research-lm"],"content":"Writing a section for a paper on research-evaluation, research-culture, research-lm\nSee Culture as Practice Meeting Notes, Culture as Practice textbarf\nSurvey taxonomy\nWithin the class of “global tasks” (can be in English or other languages), we came up with this way to divide up task exemplars:\n\nImportant distinctions from our conversation:\n\nCultural preferences = distinct ways in which non-culturally-variant tasks are judged by members of a population.\n\nKoreans preferring short news ⇒ implications for news summarization\n\n\nAccurate culturally-variable persona modeling work only counts if the work actually evaluates the persona using real examples or members of the population, not if it only instrumentally uses the persona to show performance gain within a system that solves a task (cf. Wtf is a persona?!)\n\nIn other words, the faithfulness of the persona must be substantiated to count in what we ask for\n\n\n\nNew things I need to investigate\n\nWork substantiating the cultural preferences in writing style between east and west (or ask Juhyun)\nWork on culturally-specific metrics, ie conciseness\nWork on translating these metrics into other languages\nWork where performance parity is the goal\nCulture-specific metric issues such as how to handle disagreement, vs just how to design: Shaily: let’s focus on metric design issues specific and impt to culture\n\n\nI will focus on this\nShaily: two axes:\n\ngeneral metric design issues\n\nSpecifically this metric design issue impacts cultural evaluation\nHere’s how chicken or egg issues impact with missing understanding of cultural behavior\n\n\nculture-specific issues\n\nhow do you get stakeholders\nhow do you handle disagreement\nhow do you handle\nAssumption that a voice system can be treated as an ASR slapped on a text system in other environments as well as it can be in English\n\n\n\n\n\nSuggestions for structural edits\nI think “What” needs to have a discrete “gap reduction metrics”\nConnections\nIn Cultural Conditioning or Placebo_ On the Effectiveness of Socio-Demographic Prompting he authors call into question how simple persona-based prompting can be used to assess the real-world behavior of models in interactions with different demographics.\nHowever, ChatGPT Doesn`t Trust Chargers Fans_ Guardrail Sensitivity in Context has almost the opposite result: basically the model is able to infer demographic characteristics of holders of some non-demographic opinion (ie, sports team affinity) which in turn\nCultural variation in response scales: several different works cover the issues of central response tendency and extreme response bias between North American, European, Middle Eastern, and East Asian respondents in polls, opinion questions, and online reviews. Consistent patterns are found in the relative extremeness of responses provided by Westerners, which complicates the use of metrics such as opinion scores in these kinds of studies.\nScaling Laws Do Not Scale uses cultural evaluation discrepancies between groups, etc as a lens to demonstrate how scaled-up benchmarks and evaluation sets increasingly challenge the validity of the numbers they report.\nThick Evaluations: [[The Case for Thick Evaluations of Cultural Representation in AI]] calls for situated and granular evaluation.\nInvestigating Cultural Alignment of Large Language Models has a lot of assessment “hows” in it because the study is mainly about how to improve the overall performance of models wrt cultural alignment. The method employed is simple answer choice according to values, as “persona similarity”. Their answer to Wtf is a persona?! is behavior of the model wrt preference questions.\nRandomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs…\nThe Ghost in the Machine has an American accent_ Value conflict in GPT-3. discusses how the “output preferences” of GPT3 and other LMs is culturally embedded toward the training data sources, in particular US-centric data.\nInvestigating Cultural Alignment of Large Language Models…\nAttributes for text evaluation covers work that discusses various desiderata including conciseness. The most obvious dimensions like correctness are relatively culturally non-dependent (modulo stimulus choices) but as more dimensions are added culturally contingent ones such as engagingness inevitably join.\nBias in Language Models_ Beyond Trick Tests and Toward RUTEd Evaluation: TODO, add to s4.1\nBuilding blocks for metrics TODO: references that deal with how various trained models are used, including retrievers, embedders, and autoregressive LMs in order to\nHow we evaluate\nCore message: the problems with the current how we evaluate:\n\nWe rely on exemplars over metrics; exemplars are limited in what they can express\nWhen we use metrics, they are typically Anglocentric or simplistic, and do not express culturally rich desiderata\nWhen we use translated examples, for “universal tasks,” we ignore culturally-contingent desiderata on generative behavior\nIn other words, we need to collect culturally-meaningful metrics both in addition to and rather than culturally-specific exemplars to better represent culture as practice.\n\nAs important as understanding “what” to evaluate (ie., interaction styles, reflection of culturally-embedded preferences) is the question of how to evaluate these diverse desiderata.\nReference examples alone cannot express culture as practice\n\nValue alignment typically relies on example-based comparisons in the output behavior of models over surveys (Investigating Cultural Alignment of Large Language Models)\n\nThis approach presumes that LM opinion scores on surveys transfer to behavior\nBut worse:\n\n\nVariations in value scales are extremely noisy wrt non-semantic variations in the elicitation set (Randomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs)\n\nEven when LLMs do faithfully reflect these value differences, correlation approaches may be blind to them\n\n\nBroadly expressing values using exemplars monolithically is impossible because of values pluralism (Scaling Laws Do Not Scale)\nThe meaning of scales themselves are different between cultures (Cultural variation in response scales)\n\nThis even causes technical issues like alleged ChatGPT Croatian\n\n\n\nDefining “good” can vary across culture\n\nPerformance on metrics often doesn’t reflect model quality Scaling Laws Do Not Scale\nLLMs themselves reflect values of the majority data, which complicates their use in metrics The Ghost in the Machine has an American accent_ Value conflict in GPT-3.\nThe very notion of good itself changes by culture, and metrics aren’t easily translated\n\nJuhyun examples: Contrast Western directness (main idea first) with East Asian preferences for gradual build-up in writing styles. Implication: Metrics designed with one cultural preference in mind (e.g., rewarding conciseness and directness) may unfairly penalize outputs aligned with other cultural norms.\nValues pluralism (references in Scaling Laws Do Not Scale) applies to metrics, not just exemplars\n\nsubstantial cross-cultural variation in preferences exist in AI ethics\n\nAI ethics principles statements from ~100 different institutions across various countries differ drastically (Jobin, Ienca, and Vayena, 2019)\n→ what different population, institutions, countries view as “ethical” or “unethical” vary substantially\n\n\n\n\n\n\nThis boils down to the problem of LM construct validity. Often this point is left unchallenged (references in Scaling Laws Do Not Scale)\nEspecially if we want to capture interaction modes, the metrics need to embed distinct values or else we are doing a simple parity test on translated stuff (this is where parity discussion, simple metrics, CoCoCroLa go)\n\nIn other words, representative evaluation is expensive. This makes the desire to somehow automate this evaluation, ie. via simulated personas and LM judges\nAutomated metrics have circularity issues\nThere is a “chicken or egg” problem here---how does one know that a model is sufficiently competent for cultural evaluation without having a good method for cultural evaluation?\nThe Ghost in the Machine has an American accent_ Value conflict in GPT-3. discusses this issue directly: in examples where the LM is asked to summarize or simplify explanations of text describing controversial issues with a non-American perspective, the model had a tendency to re-summarize or explain the issue through the US-centric values lens.\nThis is a great example of chicken-or-egg issues. A metric that uses such a model (which would require evaluation for its cultural understanding) that is blind to some cultural feature (which could even happen when authentic cultural information is provided as input) will be incapable of rendering accurate judgments.\nIn other words:\n- how can we trust LM judges if we do not know how culturally competent they are in terms of the judgment\n- We need to carefully examine whether LLM-judges’ evaluation aligns with non-English / non-western researchers’ idea of the evaluation criterion (e.g., fluency, good structure, engaging, friendly)\nLimitations of decontextualized and single-point metrics\nThick Evaluation (Qadri et al 2018-2025) mentions a lot of issues in how not only is “good” variable across culture but so are many other considerations. What are racial categories? What are representative garb in different cultures? What level of distinction between types/qualities are minimal/critical? These are all questions about object-level cultural evaluation that are often insufficient.\nNew metrics are needed\n\nCan decontextualized model performance metrics (e.g., accuracy, ROUGE) capture behaviors in different cultures?\n\nAdding metrics that measure cultural diversity to existing benchmarks, leaderboards? (in line with expanding non-cultural tasks to cultural tasks)\n\ne.g., which sub-population preferred the output? How much of the whole population is satisfied with the output?\n\n\nTo develop such metrics, we first need to look deeply into the outputs of current models to develop comparative metrics (e.g., granularity of the fact being different) that take into account the diversity of the\n\n\n\nGoing beyond benchmarks is hard! But important.\n\n\nMethodology outside benchmarking\n\nthings we can’t test for (e.g., weird default behaviors)\nSystematically finding surprising (undesired) behaviors\ndefining negative boundaries of behaviors that we don’t want to see from models\n\n\n\n‘Thick Evaluation’ (Qadri et al., 2025)\n\nThick, situated evaluation as opposed to thin evaluation, which is how the current cultural evaluations are done\n\n\n\n\n[e]merging methods for evaluating cultural representation in AI images involve ‘thin evaluations’; i.e., suited to evaluating the observable aspects of the physical world contained in AI-generated images, but not necessarily the social signals embedded in that physical visuals.\n\n\nWe find that 1) people evaluate representation of social worlds not just through a singular category such as accuracy, but through multi-dimensional, fine-grained axes; 2) people’s goals for evaluating cultural representation are situated in their social context, negotiated through dialogue with others and in response to broader societal discourse about their cultures; and 3) people deploy situated social knowledge and experiences with social worlds to evaluate varying social meanings of images.\n\n\nincorrectness (the accuracy of depicted physical objects—the closest to existing approaches [e.g., 84]), missingness (absence of iconic and expected cultural elements), specificity (whether the subject of the image was specific to their particular sub-culture), coherence (whether all elements of an image were appropriate given cultural and social norms), and connotation (the symbolic meanings and interpretations associated with an image).\n"},"02-Document-Notes/Generalization-Problem-in-DL-2023":{"slug":"02-Document-Notes/Generalization-Problem-in-DL-2023","filePath":"02 Document Notes/Generalization Problem in DL 2023.md","title":"Generalization Problem in DL 2023","links":["tags/topic-notes"],"tags":["topic-notes"],"content":"2023-02-12 21:39\nTags: topic-notes\n\nClare Lyle Blogpost\n\nPeter Bartlett has been giving talks for years on the second descent region of these double descent curves…benign overfitting\n\n\nI had the idea for the concept of compartmentalization again as I was reading this… do NNs maybe just have more space to/find it more efficient to directly allocate full subspaces of their weight information capacity to discrete abilities?\n\nWould it be possible to test for this?\nThis finding could be made useful in the calls for modularized model building in Raffel’s call to build models like OSS\n\n\nRefers to NeurIPS 21 paper from Bubeck on larger model size guaranteeing high Lipschitz robustness openreview.net/pdf\n\nThis paper is very dense, but the key takeaway is that over-parameterization actually increases the smoothness of the learned function (and they argue robustness but I’m not sold on if that def fits)\n“Even though the worst functions defined by wider networks are worse, the best functions are better”\n\n\nEvidence for multiple (beyond double) descent\nThe problem with depth in classic DNNs is that more layers hurt accuracy because of bad optimization dynamics, not because of overfitting\n\nHence, why resnet needed to smooth the loss landscape with residual connections\nTraining an NN involves walking a fine line between chaos and stagnation\n\n\nLottery ticket hypothesis arxiv.org/abs/1803.03635\n\nWhy is it so easy to prune a good, wide NN into a thinner sparser one, but so hard to train a good sparse one with small param count from scratch?\nThis answer has to do with lucky initializations: wider network to start = higher odds some lucky good subnetwork is in the weight space somewhere\n\nSurvival of the fittest?\nMaybe it’s that recieving continuing reinforcement through parameter updates on the subnetworks that get used is some “use it or lose it” or “non-atrophy” type of deal?\nIn the conclusion she links to more big-picture blog posts. Review the sources in those\n\n\n\n\n"},"02-Document-Notes/Master-list-of-todos-for-PECO-EACL-camready":{"slug":"02-Document-Notes/Master-list-of-todos-for-PECO-EACL-camready","filePath":"02 Document Notes/Master list of todos for PECO EACL camready.md","title":"Master list of todos for PECO EACL camready","links":["tags/todos","01-Fleeting-Notes/Strongest-PECO-Motivation-Explanation","01-Fleeting-Notes/NLI+-for-LLM-Eval","01-Fleeting-Notes/Multi-Scales-(sic)-data-augmentation-for-NLI","Notes-from-SoCalNLP","01-Fleeting-Notes/How-PECO-differs-from-Competency-Problems","01-Fleeting-Notes/Comptetency-Problems-Gardner"],"tags":["todos"],"content":"2023-01-07 22:34\nTags: todos\n\nDebug notes\n\nSICK, CF : too many values to unpack\n\nSICK fixed\n\n\n\nX, MdbA: no pretrained model (training in progress)\n\nX on andrew\nMdbA on zion\n\n\nOC: no layer named dense in last final layer\n\nThe one model you don’t use lastdense for (classifier doesn’t contain a dense)\n\n\n\ns2only, lastdense\nfor dataset in A1 A2 A3 AA CF MB MdbA MU SdbA SICK S X; do echo $dataset; echo $pc; CUDA_VISIBLE_DEVICES=6 python dataset_to_clusters.py --dataset $dataset --n_clusters 50 --n_components 50 --s2only --skip_gpu --lastdense --tsne;  done\n\ns1only ,lastdense\nfor dataset in CF F MdbA SdbA; do echo $dataset; for pc in 0 50 100; do echo $pc; for k in 10 25 50 100; do echo $k; CUDA_VISIBLE_DEVICES=6 python dataset_to_clusters.py --dataset $dataset --n_clusters $k --n_components $pc --s1only --skip_gpu --lastdense; done; done; done\n\ns2only, skip lastdense\nfor pc in 0 50 100; do echo $pc; for k in 10 25 50 100; do echo $k; CUDA_VISIBLE_DEVICES=6 python dataset_to_clusters.py --dataset OC --n_clusters $k --n_components $pc --s2only --skip_gpu; done; done\n\nSNLI cluster analysis:\n[e, c, n]\n##### HIGHEST BIAS ClUSTERS #####\n#0 bias cluster : [7] (anti-entail)\ndistribution:\n[0.24324324 0.35135135 0.40540541]\n#1 bias cluster : [24] (anti-contradiction)\ndistribution: ()\n[0.3197026  0.17843866 0.50185874]\n#2 bias cluster : [26] (pro-entail)\ndistribution:\n[0.45045045 0.33783784 0.21171171]\n#3 bias cluster : [35] (strongly pro-contra)\ndistribution:\n[0.2287234  0.63297872 0.13829787]\n#4 bias cluster : [30] (pro-neutral)\ndistribution:\n[0.20627803 0.21076233 0.58295964]\n\nFinal Edits for Camera Ready and arXiv\nContent Edits\n\n Fix reference to k-nn to k-means consistently\n Appendix on hyperparams in the PECO pipeline: correlations and table rows for PECO under: ✅ 2025-05-05\n\n PCA-50 PCA-10 no PCA\n K=50, K=100,\n K_{\\textrm{adaptive}}=|D|/n ✅ 2025-05-05\n L2 vs KLD\n Correlation plots ✅ 2025-05-05\n\n\n Redo Fig 5-7 to use same acronyms as labels from the other datasets\n Motivate use of TRA 3.1 (we want to assess whether a given sentence is reasoning similarly over or not?) I do discuss a bit ✅ 2025-05-05\n Motivating more clearly why we use PSC-trained SSC test for PECO computation Strongest PECO Motivation Explanation\n\n Clearly explain (4) from above, the way that the lesioning process works on the samples\n- [ ] Find the insights (Additional Analysis asked for by R2)\n- [ ] In particular, can we characterize the bias clusters semantically for some models?\n\n\n This has been skipped, defered to ACL Demo ✅ 2025-05-05\n Fig1 style plots for every model in appendices ✅ 2025-05-05\n NSF acknowledgement, and get William’s NSF acknowledgement as well\n\nAdditional Experiments\n\n Whatever is required to complete the PECO variation experiments\n- [ ] Integration of competency problems into dataset analysis\n Dataset cartography-PECO cluster comparison\n\n Is the difficulty within- or between-clusters?\n Add this content to page 9\n\n\n\nAdditional Analysis\n- [ ] Find semantic properties of the main clusters\n- [ ] Word clouds?\n\n Relate cartography and competency problems ✅ 2025-05-05\n Defered to ACL Demo track submission ✅ 2025-05-05\n\nFuture paper based on WANLI\nwww.semanticscholar.org/reader/56b30c6bd9dc4a2416ab3b74ad97dbb7a2904229\nAlisa Liu is main author\nNLI+ for LLM Eval\n\nImportant related work:\n\nMulti-Scales (sic) data augmentation for NLI\nAnalysis paper from AACL\n\n\n\n\nRelevant Notes:\nNotes from SoCalNLP, How PECO differs from Competency Problems, Comptetency Problems Gardner\n\nRe-run PECO evaluation for both xH and L2 with N_cluster=5,10,25,50,100\n\nAdd in the other dumbass distance metric reviewer 1 suggested\n\n\nAblation studies to help assuage retard reviewers\n\nSelect 2 datasets to do all the following:\n\nRedo all N_cluster with PCA N_dim = 50, 256, 512, and regular\nAll distance metrics\nAll N_cluster\n\n\nFor each, show the following figures:\n\nT-SNE samples (rough illustration of how separation works fine)\n\n\n\n\nManual analysis of the bias clusters: get some word clouds!\n\nLook at ShortcutLens\nOld steps (Quick Tasks for EACL PECO)\n2022-10-11 23:52\n\nFor submission pre-anonymity deadline to arXiv:\n\nTerminological fixes (SSC and PSC only)\nOnce-over of all writing\nFix whether PECO L2 or xH is used\nFix figures, tables with SSC and PSC\nAdd rationale details\n\n\nMake promo tweet thread once the paper is up on arXiv\n\nNEW PREPRINT ALERT, etc\nKey point: automatic, model-driven method to find problematic subclusters\n\n\n"},"02-Document-Notes/RLHF,-Alignment-Deep-Dive":{"slug":"02-Document-Notes/RLHF,-Alignment-Deep-Dive","filePath":"02 Document Notes/RLHF, Alignment Deep Dive.md","title":"RLHF, Alignment Deep Dive","links":["tags/paper-notes","01-Fleeting-Notes/Notes-from-Text-is-the-Universal-Interface","01-Fleeting-Notes/Why-I'm-Not-worried-about-superintelligence-blogpost"],"tags":["paper-notes"],"content":"2023-01-25 15:17\nTags: paper-notes\n\nRLHF Paper from OpenAI\nopenai.com/blog/instruction-following/#moon\nUnix originated universal interface idea of text, Roon proposes this philosophy lives on in LLMs Notes from Text is the Universal Interface\nOriginal RLHF paper arxiv.org/ftp/arxiv/papers/2106/2106.10328.pdf\nRunning Llama\ncocktailpeanut.github.io/dalai/#/\nWhy did Sam Bowman start a group on alignment\nwp.nyu.edu/arg/why-ai-safety/\nWhy I’m Not worried about superintelligence blogpost\nCounterpoint: am I “self-catechizing?”\n\n“Culture catechizes,” Alan Jacobs, a distinguished professor of humanities in the honors program at Baylor University, told me. Culture teaches us what matters and what views we should take about what matters. Our current political culture, Jacobs argued, has multiple technologies and platforms for catechizing—television, radio, Facebook, Twitter, and podcasts among them. People who want to be connected to their political tribe—the people they think are like them, the people they think are on their side—subject themselves to its catechesis all day long, every single day, hour after hour after hour.\nFrom Trump is Tearing Apart the Evangelical Church\nI definitely like to curate who I follow online. This might lead to a bit of autocatechism but conversely I believe LessWrong is a great example of where this takes place.\n"},"02-Document-Notes/Verifiers":{"slug":"02-Document-Notes/Verifiers","filePath":"02 Document Notes/Verifiers.md","title":"Verifiers","links":["tags/research-lm","tags/research-evaluation","01-Fleeting-Notes/Ofir-Press-benchmarking-talk"],"tags":["research-lm","research-evaluation"],"content":"research-lm research-evaluation\nThis is a home page to aggregate notes about verifiers for LM evaluation. Sources that are mentioned in works that deal with evaluation, etc.\n2025-05-22 Ofir Press benchmarking talk was a PyTorch webinar where he presented SWE-bench, agent, an oral history that was pretty interesting to me"},"02-Document-Notes/Wtf-is-a-persona!":{"slug":"02-Document-Notes/Wtf-is-a-persona!","filePath":"02 Document Notes/Wtf is a persona?!.md","title":"Wtf is a persona?!","links":[],"tags":[],"content":"Sometimes “persona” in LMs refers to an output persona, ie the thing that describes the human-like behavior the LM is supposed to simulate\nOther times it refers to input personas, ie the data that models the kind of users the LM is interacting with (in lieu of fully real interactions with the model under study)"},"03-AI-generated/AI-News-2025-05-21":{"slug":"03-AI-generated/AI-News-2025-05-21","filePath":"03 AI generated/AI News 2025-05-21.md","title":"AI News 2025-05-21","links":[],"tags":[],"content":"Daily Reading List - 2025-05-21\nTrump administration may sell deep-sea mining leases at startup’s urging\n\nSource: TechCrunch\nSummary\nThe U.S. Department of the Interior is initiating the process of selling deep-sea mining leases in waters offshore of American Samoa following a request from Impossible Metals. Impossible Metals has developed an autonomous underwater vehicle that uses robotic claws to extract polymetallic nodules, which are rich in minerals like manganese, iron, cobalt, nickel, and copper. Interior Secretary Doug Burgum stated that accessing these resources responsibly would support American economic growth and national security. These nodules have attracted attention due to their high mineral concentrations, far exceeding those found in terrestrial mines. Ecologists and oceanographers have warned that deep-sea mining could disrupt fragile ecosystems, which would take decades to recover from the disruption. A recent study found that microbial communities would need 50 years to recover from mining operations. Sponges and other sea creatures could be harmed, and sediment plumes could pollute the water.\nWhy does this matter?\nEnvironmental Impact: The article highlights the potential ecological consequences of deep-sea mining. Given the slow recovery rates of deep-sea ecosystems, the environmental disruption could be significant, impacting microbial communities and marine life. The fact that the nodules themselves produce oxygen is an unexpected detail that could be important.\nResource Security: The push for deep-sea mining is driven by the increasing demand for critical minerals needed for electrification and the desire to reduce reliance on countries like China. This could be a significant shift in how resources are sourced, with potential geopolitical implications.\nTechnological Innovation: Impossible Metals claims its UAV is less disruptive than competitors using vacuums. This introduces the question of how best to evaluate competing claims of lower environmental impact of new technologies.\nRead more\n\nA comprehensive list of 2025 tech layoffs\n\nSource: TechCrunch\nSummary\nTechCrunch is tracking tech layoffs in 2025, noting over 22,000 job cuts so far this year. The article presents a comprehensive, regularly updated list of layoffs across various tech companies, from Big Tech to startups. May 2025 saw layoffs at Amazon (devices and services division, including Alexa and Zoox), Microsoft (affecting 3% of its workforce), Chegg (22% of workforce due to students opting for AI tools), Match (13% workforce reduction), CrowdStrike (5%), General Fusion (25%), and Deep Instinct (10%). April 2025 included layoffs at NetApp (6%), Electronic Arts, Expedia (3%), Cars24, Meta (Reality Labs), Intel (20%), GM (Factory Zero), Zopper, Turo, GupShup, Forto, Wicresoft (stopping operations in China), Five9 (4%), Google (platforms and devices), Automattic (16%), and Canva (technical writers).\nMarch 2025 included layoffs at Northvolt (62%), Block (8%), Brightcove (two-thirds of U.S. workforce), Acxiom (3.5%), Sequoia Capital (closing D.C. office), Siemens, HelloFresh, Otorio (after acquisition by Armis), ActiveFence (7%), D-ID (25%), NASA (shutting down several offices), Wayfair (technology division), HPE (5%), TikTok (10% in Dublin), LiveRamp (5%), Ola Electric, Rec Room (16%), and ANS Commerce (shut down). February 2025 saw layoffs at HP, GrubHub (20%), Autodesk (9%), Google (People Operations and cloud organizations), Nautilus (16%), eBay, Starbucks, Commercetools, Dayforce (5%), Expedia, Skybox Security (ceased operations), HerMD (shutting down), Zendesk, Vendease (44%), Logically, Blue Origin (10%), Redfin, Sophos (6%), Zepz, Unity, JustWorks, Bird (one-third), Sprinklr (15%), Sonos, Workday (8.5%), Okta, Cruise (50%), and Salesforce. January 2025 layoffs included Cushion (shut down), Placer.ai (18%), Amazon (communications), Stripe, Textio, Pocket FM, Aurora Solar, Meta (5%), Wayfair (3%), Pandion (shut down), Icon, Altruist, Aqua Security, SolarEdge Technologies, and Level (shut down).\nWhy does this matter?\nIndustry Impact: The article highlights the ongoing trend of layoffs in the tech industry, which is being exacerbated by companies embracing AI and automation. This could lead to a shift in the skills demanded by the market, potentially impacting the types of research and development that are prioritized.\nEdTech Vulnerability: The article notes that Chegg is laying off employees as students opt for AI tools instead of traditional edtech platforms. This demonstrates the disruptive potential of AI in education and the need for edtech companies to adapt.\nEV Slowdown: GM is laying off 200 people at its Factory Zero due to an EV slowdown. This highlights the challenges facing the electric vehicle industry.\nRead more\n\nWhat happens when artificial intelligence quietly reshapes our lives?\n\nSource: NPR Technology\nSummary of “What happens when artificial intelligence quietly reshapes our lives?”\nKashmir Hill, a tech reporter for The New York Times, discusses the increasing integration of AI into daily life, focusing on its impact on education and personal decision-making. The conversation highlights how students are using generative AI tools like ChatGPT to assist with schoolwork, sometimes excessively, leading to a homogenization of writing styles and a reliance on AI-generated content. Professors, facing workload pressures, are also turning to AI for tasks such as creating lesson plans and grading assignments, which has led to student complaints about the quality of instruction and concerns about the value of their education.\nThe discussion touches on the limitations and potential biases of AI detection tools, as well as the ethical concerns surrounding the use of copyrighted material to train large language models. Hill also recounts her experiment of living a week guided by AI, revealing the varying personalities of different AI models and their potential to influence personal choices and creativity. Ultimately, the conversation raises questions about the long-term impact of AI on critical thinking, problem-solving skills, and human connection, expressing concern about the potential for AI to isolate individuals and distort our shared sense of reality. As Hill concludes, she hopes “we can learn to maybe de-escalate our technology use a bit, be together more in person, talk to each other by voice.”\nWhy does this matter?\nAI in Education: The widespread adoption of AI tools in education, both by students and professors, necessitates a critical examination of how these tools are impacting the learning process and the quality of education. There is a need to develop guidelines and best practices for AI use in education that promote learning and prevent the erosion of critical thinking and creativity.\nEthical Concerns: The ethical concerns surrounding the use of AI, including copyright infringement, bias, and the potential for manipulation, require careful consideration and proactive measures to ensure responsible development and deployment of AI technologies. The discussion also highlights the importance of transparency and user consent in data collection and usage practices.\nImpact on Human Skills and Connection: The potential for AI to erode human skills and disconnect individuals from one another raises important questions about the long-term impact of AI on society. There is a need to explore ways to mitigate these risks and promote human connection in an age of increasing AI integration. As Hill suggests, this may mean that we “de-escalate our technology use a bit, be together more in person, talk to each other by voice.”\nRead more\n\nOpenAI to buy AI startup from Jony Ive\n\nSource: Hacker News\nSummary\nOpenAI is set to acquire io, an AI device startup co-founded by former Apple designer Jony Ive, in a $6.5 billion all-stock deal. This acquisition marks OpenAI’s largest to date, aiming to create AI-powered devices and secure the expertise of Ive and his team, who were instrumental in designing iconic Apple products. According to Bloomberg, “The purchase… will provide the company with a dedicated unit for developing AI-powered devices. Acquiring the secretive startup, named io, also will secure the services of Ive and other former Apple designers who were behind iconic products such as the iPhone.”\nThe acquisition also includes approximately 55 engineers, scientists, researchers, physicists, and product development specialists from io, many of whom are former Apple designers. While Ive will retain control of his design firm, LoveFrom, he and his team will lead creative and design efforts at OpenAI, influencing future versions of ChatGPT and other projects, according to The New York Times. Ive and Altman have been working on a device that moves consumers “beyond screens,” with the first devices expected to debut in 2026, and a desire to find some physical embodiment for AI systems to interact with users, according to comments in the coverage by both Bloomberg and TechCrunch.\nHacker News commenters express skepticism about the acquisition, particularly regarding the valuation, the lack of a clear product from io, and Jony Ive’s track record since leaving Apple. Concerns are raised about OpenAI’s strategic direction, with some suggesting that the company is losing focus on core AI research and is instead pursuing expensive acquisitions to maintain relevance. Several commenters also point out that the all-stock deal may be a way for OpenAI to manage its cash flow and dilute the ownership of its non-profit entity.\nTop Comments Summary\nThe Hacker News comments section is largely critical of the acquisition. Common themes include:\n\nSkepticism about Ive’s post-Apple performance: Commenters question Ive’s ability to replicate his past success, citing design failures such as the butterfly keyboard and the removal of ports from MacBooks. One commenter said, “Ive did good designs when Jobs kept him in check. Once Jobs was gone he messed up a whole generation of MacBooks.”\nConcerns about OpenAI’s strategic direction: Some commenters feel that OpenAI is losing focus on AI research and is instead making expensive acquisitions to stay relevant. One commenter wrote, “This is a sign of OpenAI’s weakness. Altman is desperately trying to use OpenAI’s inflated valuation to buy some kind of advantage.”\nDoubts about the valuation and lack of product: Many question the $6.5 billion valuation of io, especially given the absence of a tangible product or even a website. One commenter stated, “It’s mind boggling how much money is floating around once you are part of the insider circle. What has that company been doing to be worth 6.5 billion?”\nCriticism of Sam Altman: Some commenters express distrust of OpenAI CEO Sam Altman, citing concerns about his past actions and motivations. One commenter suggested that the acquisition is a way for Altman to extract value from OpenAI for personal gain: “Sam Altman who planned and executed the extraction of Reddit from Condé Nast… - Step 2: you acquire that entity, valuing it unreasonably high so that the nonprofit’s stake is diluted.”\nComparisons to past tech failures: Several commenters draw parallels to past tech failures, such as the Facebook phone and the Humane AI pin, suggesting that OpenAI’s hardware venture may face similar challenges. One commenter asked: “How did that work for the Facebook phone? And all their billions of fervent Facebook users?”\nThe nature of the deal Commenters are suspicious of it being an “all stock deal” and what that really means, such as whether this might not be an “acqui-hire”\n\nWhy Does This Matter?\n\nIndustry consolidation and talent acquisition: The acquisition signifies the ongoing consolidation in the AI industry and the importance of attracting top talent, particularly in design. This trend could influence how AI research and development are structured and funded, with larger players potentially dominating the landscape.\nHardware strategy and product development: OpenAI’s push into hardware raises questions about the future of AI interfaces and the potential for AI to move beyond software applications. This development could shape the design and functionality of future AI-powered devices, with a focus on creating more seamless and intuitive user experiences.\nEthical and societal implications: The development of AI-powered devices raises ethical concerns about data privacy, surveillance, and the potential for misuse. Understanding the societal impact of these technologies will be crucial for responsible AI development and deployment.\n\nPotential Relevance to Research\n\nAI evaluation: The success or failure of OpenAI’s AI-powered devices could provide valuable data for evaluating the effectiveness of current AI evaluation metrics. Does this move make a difference relative to current LLMs, and can the difference be quantified? Rigorous evaluation of these devices will be necessary to assess their real-world performance and identify potential limitations. The article mentions comments from Ive regarding the failures of similar attempts, such as the Humane pin. Can these failures be quantified with current metrics?\nMultimodal AI: The development of AI devices will likely require advancements in multimodal AI, integrating language, vision, audio, and other modalities. The challenges and opportunities in this area could inform your research on multimodal models and their applications.\nMulticultural AI One of the comments notes: “German has a word for second hand embarrassment. Fremdschämen. Comes in very handy here. If Sam continues like this it won’t be long until it becomes part of the regular English language like other German words such as Kindergarten. And I’ll be happy that I don’t have to explain Fremdschämen anymore. Everything has its upsides.” This comment about “Fremdschamen” as a possible new word in the English language that has come as a result of developments in AI reflects the impact on languages worldwide with the new technology.\n\nRead more\nRelated Articles\n\nJony Ive to lead OpenAI’s design work following $6.5B acquisition of his company\nOpenAI Unites With Jony Ive in $6.5 Billion Deal to Create A.I. Devices\n\n\nMistral’s new Devstral AI model was designed for coding\n\nSource: TechCrunch\nSummary\nMistral has released Devstral, a new AI model designed for coding, under the Apache 2.0 license, allowing for unrestricted commercial use. Mistral claims that Devstral outperforms open models like Google’s Gemma and DeepSeek’s V3 on the SWE-Bench Verified benchmark, which measures coding skills. According to Mistral, “Devstral excels at using tools to explore codebases, editing multiple files and power[ing] software engineering agents,” and is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it suitable for local deployment.\nDevstral arrives amidst growing popularity of AI coding assistants. Other companies such as JetBrains, Google, Windsurf, and OpenAI have released models optimized for programming tasks. Although AI models still struggle to produce high-quality code due to weaknesses in understanding programming logic, their potential to boost coding productivity drives rapid adoption. According to one poll, 76% of developers used or planned to use AI tools in their processes last year. Devstral is available on AI development platforms like Hugging Face and through Mistral’s API, priced at $0.1 per million input tokens and $0.3 per million output tokens. Mistral says they are “hard at work building a larger agentic coding model that will be available in the coming weeks”.\nWhy Does This Matter?\nModel Evaluation &amp; Benchmarking: The claim that Devstral outperforms other models on SWE-Bench Verified is significant. This highlights the importance of standardized benchmarks for evaluating coding skills in AI models, which is crucial for comparing different approaches and tracking progress. The article mentions Mistral’s internal benchmarking evaluations of Devstral, which emphasizes the need for rigorous and transparent evaluation methodologies.\nCommercial Use and Open Licensing: The Apache 2.0 license enables unrestricted commercial use of Devstral, contrasting with Mistral’s previous Codestral, which restricted commercial applications. This could impact the adoption and integration of AI coding tools in various industries. Furthermore, the pricing model for Devstral’s API ($0.1 per million input tokens and $0.3 per million output tokens) offers insights into the cost of using these models for commercial applications.\nAgentic Systems &amp; Future Developments: Mistral’s statement about building “a larger agentic coding model” suggests a focus on developing more advanced AI agents for software engineering. The article also mentions that Devstral is designed to run over code agent scaffolds, suggesting that agentic systems and tools that use AI to help engineers are being increasingly deployed. This may accelerate the development of more sophisticated AI-powered development tools.\nRead more\n\nLM Arena, the organization behind popular AI leaderboards, lands $100M\n\nSource: TechCrunch\nSummary\nLM Arena, a UC Berkeley-affiliated project known for its crowdsourced AI model benchmarking, has secured $100 million in seed funding at a $600 million valuation. The round was led by Andreessen Horowitz (a16z) and UC Investments, with participation from Lightspeed Venture Partners, Felicis Ventures, and Kleiner Perkins. LM Arena collaborates with major AI labs like OpenAI, Google, and Anthropic, offering a platform for the community to evaluate flagship models. The organization, formerly funded through grants and donations, has faced accusations of allowing AI labs to “game” its leaderboard, which it denies. According to TechCrunch, the funding will support the project’s continued research on “reliable AI.”\nWhy this matters\nThis funding event highlights the increasing value placed on AI evaluation and benchmarking platforms. It signals the continued importance of community-driven assessment in the AI industry, where standardized metrics are still evolving. The involvement of prominent venture capital firms suggests confidence in LM Arena’s role in shaping the development and deployment of AI models.\nRelevance to client’s research\nThis article has several points of relevance:\n\nAI Evaluation: The very premise of LM Arena directly relates to the client’s work on AI evaluation. The funding suggests that there’s an industry-wide need for platforms that can evaluate AI models, and that investors see value in LM Arena’s approach.\nRigorous Evaluation: Given the client’s focus on scientific rigor in AI evaluation, the accusations against LM Arena about gaming the leaderboard are notable. This underscores the challenges in creating truly objective and reliable benchmarks. The client may find it useful to understand the basis of these accusations and how LM Arena plans to address them.\nMultimodal generative AI: Given that LM Arena is used to evaluate flagship models from the largest AI vendors, any evaluations of multimodality would be very relevant.\nEthical AI: Since the funding will be used to support the project’s continued research on “reliable AI”, it may be valuable to understand the implications of this research on the client’s research in ethical AI.\n\nRead more\n\nWhat AI Thinks It Knows About You\n\nSource: The Atlantic\nSummary\nThe article discusses the challenges of understanding how large language models (LLMs) work and the potential implications of their increasing use in our lives. The author, Jonathan L. Zittrain, highlights the issue of interpretability, or the ability to explain how a model arrives at a particular output. While companies like Anthropic and Meta are making progress in this area, LLMs remain largely “black boxes.” Google CEO Sundar Pichai admitted in a 60 Minutes interview that “There is an aspect of this which we call—all of us in the field call it as a ‘black box.’ You know, you don’t fully understand. And you can’t quite tell why it said this, or why it got wrong.”\nThe article delves into research by Fernanda Viégas and Martin Wattenberg at Harvard, who discovered that LLMs form assumptions about their users based on factors such as gender, socioeconomic status, education level, and age. These assumptions can then influence the model’s responses, leading to potentially biased or stereotypical recommendations. The ability to see these assumptions in real time, using a dashboard built by the Harvard researchers, reveals the extent to which LLMs rely on stereotypes gleaned from their training data.\nZittrain raises concerns about the implications of LLMs gathering information about users through unguarded conversations and the potential for manipulation or misuse of this information. He argues for a “clear sphere of protection” between LLMs and their users, similar to the protections afforded to conversations between lawyers and clients or doctors and patients. He argues that model makers and operators must commit to ensuring that models function as harmless, helpful, and honest friends.\nWhy This Matters\nModel Metrology: The article touches on the difficulty of understanding and evaluating LLMs. The discussion of “interpretability” and the “black box” nature of these models underscores the need for more rigorous and scientific approaches to evaluating their capabilities, biases, and potential for misuse. The work by Anthropic and the Harvard researchers suggests that progress is being made in understanding the inner workings of these models, opening up new avenues for evaluation and improvement.\nMulticultural AI: The research by Viégas and Wattenberg, specifically Viégas’s experience with ChatGPT in Portuguese, highlights the importance of considering cultural and linguistic nuances in AI development. The discovery of gender-based stereotypes in LLMs demonstrates the need for careful examination of training data and model behavior to ensure fairness and inclusivity across different languages and cultures. The point about the LLM not being able to discern nonbinary gender designations due to a limited number of examples in the training data would be useful.\nAgentic Systems and Ethical AI: Zittrain’s concerns about the potential for LLMs to be used for manipulation or to provide harmful information raise important ethical questions about the development and deployment of agentic AI systems. The example of a car-dealership AI sales assistant illustrates how these systems could exploit user data for commercial gain, while the hypothetical scenario of an AI providing bomb-making instructions highlights the potential for misuse. The call for a “clear sphere of protection” between LLMs and their users underscores the need for ethical guidelines and regulations to ensure that these systems are used responsibly and in a way that protects user autonomy and agency.\nRead more\n\nEinride founder steps down as CEO amid push to scale electric, autonomous trucks\n\nSource: TechCrunch\nSummary\nEinride founder Robert Falck is stepping down as CEO to become executive chairman, focusing on long-term strategy and a potential IPO. CFO Roozbeh Charli is taking over as CEO. Falck founded Einride in 2016 with the goal of transforming the trucking industry with electric and autonomous vehicles, including pods designed without steering wheels or pedals. He wants to focus on long-term value creation, including preparing Einride for a potential IPO.\nCharli aims to continue Falck’s mission and scale the company responsibly. Einride operates a large fleet of heavy-duty electric trucks in Europe, North America, and the Middle East, serving companies like PepsiCo and Carlsberg Sweden. Einride has raised over $650 million to date, and Falck mentioned in October that an additional $100 million would be needed to reach profitability. The company reported roughly $5 billion in contracted revenue and approximately $50 million in annual recurring revenue for 2024.\nWhy This Matters\nAutonomous Vehicle Industry Growth: Einride’s push for autonomous, electric trucking highlights the ongoing advancements and challenges in the autonomous vehicle industry. The transition in leadership and focus on scaling the company points to the complexities of bringing such technology to market.\nFunding and Scalability of AI Startups: The article mentions Einride’s need for additional funding to reach profitability, showcasing the financial demands and pressures faced by AI-driven startups as they scale their operations. The explicit mention of a potential IPO is also interesting, given your interests in the industry.\nReal-World Applications of Autonomous Systems: The use of Einride’s trucks by major corporations like PepsiCo and Carlsberg Sweden illustrates the practical application of autonomous and electric vehicle technology in addressing real-world logistics and transportation needs, particularly in the context of sustainability. The fact that the company operates in Europe, North America, and the Middle East illustrates how they have successfully adapted to several different legal and regulatory environments.\nRead more\n\nFreddy Lim, Frontman of Chthonic, Is Taiwan’s New Envoy to Finland\n\nSource: The New York Times\nSummary\nThe Taiwanese government has appointed Freddy Lim, the lead singer of the heavy metal band Chthonic, as its envoy to Finland. This choice is notable because Finland has the most metal bands per capita and Lim’s band is popular in Finland. Lim stated that “Working with my partners in the Finnish music industry for a long time has made me have a special feeling for this country.” Beyond his musical career, Lim’s selection is attributed to his human rights advocacy and experience in international exchange, including his service as a legislator and chairman of Amnesty International in Taiwan. Lim founded Chthonic in 1995, incorporating Taiwanese mythology into their music, which led to international recognition after their 2005 album and a spot in Ozzfest.\nWhy does this matter?\nThis appointment signals a shift in diplomatic norms, potentially opening doors for individuals from diverse backgrounds, including artists, to engage in international relations. The appointment also reflects Taiwan’s focus on leveraging cultural connections in its foreign policy. It demonstrates that cultural ambassadors can be very effective for international relations, particularly when cultural interests are shared.\nRead more\n\nIntroducing the Llama Startup Program\n\nSource: Hacker News\nHere’s a summary of the Llama Startup Program article and its potential relevance:\nSummary\nMeta has launched the “Llama for Startups” program to encourage the adoption of its Llama AI models among U.S.-based startups. The program offers direct support from Meta’s Llama team and, in some cases, up to $6,000 per month for six months to help offset the costs of building generative AI solutions. To be eligible, startups must be incorporated in the U.S., have raised less than $10 million, and have at least one developer on staff.\nThis initiative aims to solidify Meta’s position in the open model space amid increasing competition from rivals like DeepSeek, Google, and Alibaba’s Qwen. It comes at a time when Meta is facing challenges, including reported delays in the rollout of its Llama 4 Behemoth model due to underperformance on key benchmarks and allegations of cheating on the LM Arena benchmark by using a version of Llama 4 Maverick optimized for conversationality. Despite these setbacks, Meta anticipates its generative AI products will generate significant revenue in the coming years, projecting $2 billion to $3 billion in 2025 and potentially reaching between $460 billion and $1.4 trillion by 2035. Meta’s “GenAI” budget is substantial, exceeding $900 million in 2024 and potentially surpassing $1 billion this year, with plans to invest $60 billion to $80 billion in capital expenditures in 2025, primarily for new data centers.\nHacker News Comments\nThere are no comments available for the Hacker News link provided.\nWhy does this matter?\n\nOpen Model Competition: The program signals Meta’s commitment to the open-source AI model space and its efforts to compete with other major players like Google and Alibaba. Meta’s strategy of incentivizing startups to use Llama could foster a broader ecosystem and influence the direction of generative AI development.\nModel Evaluation and Benchmarking: The mention of Llama 4 Behemoth’s delayed release due to benchmark underperformance and the controversy surrounding Meta’s LM Arena performance highlights the challenges in rigorously evaluating and benchmarking AI models, especially concerning conversationality and real-world performance.\nMulticultural AI Implications: A broader Llama ecosystem could foster uses of Llama in different languages and cultural contexts, especially in the many startups that are popping up in Asia, Europe, Africa, and elsewhere.\nCost and Investment: The massive investment Meta is making in generative AI, including the “GenAI” budget and infrastructure spending, underscores the significant resources required to develop and deploy these models, raising questions about the sustainability and accessibility of AI development.\n\nRead more\nRelated Articles\n\nMeta launches program to encourage startups to use its Llama AI models\n\n\nJapan’s agriculture minister resigns after his remark about not having to buy rice\n\nSource: NPR World\nSummary\nJapan’s Agriculture Minister, Taku Eto, resigned following public backlash over his remark about “never having to buy rice” because he received it as gifts from supporters. This comment was deemed insensitive given the record high rice prices and economic struggles faced by the Japanese public. Eto acknowledged the inappropriateness of his statement and its impact on consumers. Prime Minister Shigeru Ishiba accepted the resignation as the government attempts to manage rice prices and supply. The government has been releasing rice from its emergency stockpile, but this has not significantly alleviated the price issue. Some supermarkets have started selling cheaper imported rice. Eto clarified that his comment referred to brown rice and that he does buy white rice himself. Media reports suggest that Shinjiro Koizumi, son of a former prime minister, will succeed Eto. Ishiba plans to strengthen Japan’s food security, but critics emphasize the need to address the immediate rice price issue. Ishiba suspects the rice price surge is a “structural problem.” The supply shortfall began with panic buying after earthquake warnings, and officials cite poor harvests and increased production costs as contributing factors. Some experts, however, blame the government’s long-term rice production policy.\nShizuko Oshima, 73, stated, “Rice is the stable food for the Japanese. When its prices are rising every week, (Eto’s) resignation is only natural.” The government denies a shortage but acknowledges distribution problems since the end of government control in 1995, making it difficult to trace the rice supply.\nWhy Does This Matter?\nPolitical Instability: The resignation highlights the vulnerability of Prime Minister Ishiba’s minority government, especially with upcoming national elections. The scandal and potential no-confidence vote could lead to significant political changes.\nEconomic Impact: The surge in rice prices reflects broader economic pressures on Japanese consumers, indicating a need for effective government intervention in food supply and pricing mechanisms. The situation underscores the sensitivity around staple food costs and their impact on public sentiment.\nFood Security: The article raises concerns about Japan’s food security and self-sufficiency. Ishiba’s plans for agricultural reform, including increased rice production and exports, suggest a long-term strategy to address these issues, even as the immediate crisis requires urgent solutions. The reference to “panic buying following a government caution over preparedness for a major earthquake” highlights the role of public perception on food security.\nRead more\n\nA tale of murder, artificial intelligence, &amp; forgiveness\n\nSource: NPR Technology\nSummary\nAn AI avatar of murder victim Chris Pelkey addressed his killer in court, potentially marking the first instance of an AI-generated victim impact statement accepted in a U.S. court. The avatar was created by Pelkey’s sister, who sought to offer forgiveness to the killer. This event raises questions about the evolving relationship between AI and the legal system, its potential impact on the public’s understanding of justice, and broader cultural implications. NPR interviewed Juliana Kim and Brandon Blankenship to explore these issues.\nWhy this matters\nLegal Precedent and Ethical Considerations: The acceptance of an AI-generated victim impact statement sets a legal precedent with uncertain consequences. It raises ethical questions about the extent to which AI can represent human emotion and intent, especially from deceased individuals. There is a risk that this could open the door for the exploitation of victims or the manipulation of legal proceedings.\nCultural Impact on Justice and Forgiveness: This event sparks a conversation about how AI may reshape societal norms regarding justice and forgiveness. The use of an AI avatar to convey forgiveness challenges traditional notions of closure and reconciliation, potentially altering how individuals process grief and seek justice. The introduction of AI into such sensitive contexts may also desensitize individuals to the emotional weight of these proceedings.\nPotential Insights for AI Research\nThis article highlights potential applications of generative AI, specifically concerning AI expression of emotion. This intersects with evaluation of AI systems, particularly how to measure the appropriateness and impact of AI-generated content in sensitive contexts such as legal proceedings. Determining if an AI-generated expression of forgiveness has the same effect as a human one falls under the umbrella of capability assessment, which your research touches on. Furthermore, this is an example of an end-user application for agentic systems and poses important questions about how these systems should be deployed, and what could go wrong.\nRead more\n\n‘MAGA is a big lesson to us’: Suntory’s Niinami on tariffs, Japanese dealmakers, and inclusive capitalism\n\nSource: Semafor\nSummary\nTakeshi “Tak” Niinami, CEO of Suntory, reflects on the lessons learned from the MAGA movement and its implications for businesses globally. He argues that the rise of populism, exemplified by Donald Trump’s presidency, stems from societal “anger, mistrust, and anxiety” arising from old-fashioned capitalism that hasn’t benefited everyone. Niinami urges corporate leaders to embrace “inclusive” capitalism, redistributing wealth and compensating those negatively impacted by globalization. He criticizes Big Tech executives for benefiting from globalization without adequately addressing its adverse effects on ordinary workers.\nNiinami also discusses the impact of Trump’s trade policies on consumer behavior, noting a “trade-down” effect where consumers opt for cheaper alternatives due to tariff-induced anxieties. He shares Suntory’s experience of navigating these challenges by stockpiling products and managing price pressures. Reflecting on Suntory’s acquisition of Beam, Niinami emphasizes the importance of cultural integration and strong leadership from the acquiring company to avoid common pitfalls in Japanese-US business deals. He highlights the need for Japanese companies to be less “naive” and more assertive in managing their US subsidiaries.\nLooking ahead, Niinami sees a turning point for corporate Japan, with companies holding significant excess cash. He encourages them to invest in new frontiers and acquisitions rather than focusing solely on stock repurchases or dividends. He also expresses concerns about geopolitical risks, including China’s actions in the Taiwan Strait and the potential dangers of unregulated AI. Niinami stresses the importance of corporate sustainability efforts in building trust with societies and calls for coordination in sustainability and AI regulation, especially in the absence of strong international leadership. Ultimately, he believes that businesses must earn society’s support to thrive in an increasingly turbulent world.\nWhy does this matter?\nInclusive Capitalism and Wealth Redistribution: Niinami’s call for corporations to redistribute wealth to those negatively impacted by globalization resonates with broader discussions on ethical AI and societal impact. As AI-driven automation potentially exacerbates economic disparities, his perspective prompts reflection on the responsibilities of companies developing and deploying these technologies to mitigate potential negative consequences and ensure equitable distribution of benefits.\nGeopolitical Risks and Global Coordination: Niinami’s concerns about geopolitical risks, particularly regarding China and unregulated AI, highlight the importance of international cooperation and ethical considerations in technology development. The potential for AI to be used for malicious purposes, including the risk of autonomous weapons systems, underscores the need for global governance frameworks and ethical guidelines to prevent misuse and ensure responsible innovation.\nRead more\n\nTrump, Putin discuss ways to end war in Ukraine in 2-hour phone call\n\nSource: Semafor\nSummary\nThe article reports that Donald Trump and Vladimir Putin had a two-hour phone call to discuss ending the war in Ukraine. Putin characterized the call as “meaningful, frank, and very useful,” indicating Russia’s willingness to work with Ukraine on a potential peace treaty and ceasefire. Trump stated the call went “very well,” and suggested that Kyiv and Moscow would “immediately” begin peace negotiations. Despite this reported diplomatic activity, Ukraine reported Russia launched its largest drone attack since the war began.\nWhy does this matter?\nGeopolitical Implications: The direct communication between Trump and Putin regarding the war in Ukraine has significant implications for international relations and the future of the conflict. If the claims are true that peace talks will “immediately” begin, this could signal a potential shift in diplomatic strategy and potentially alter the trajectory of the war. However, the drone strike suggests otherwise, and that regardless of diplomatic talks the war will continue.\nPotential for Misinformation: Trump’s claim that negotiations will “immediately” begin should be monitored for its accuracy and potential to influence public opinion. Claims made during and after the phone call should be vetted for misinformation.\nRead more\n\nConcerns mount over US debt, inflation risks\n\nSource: Semafor\nSummary\nThe article from Semafor reports growing concerns about the health of the U.S. economy, with U.S. stocks expected to fall and Treasury yields rising. Ray Dalio, founder of Bridgewater, stated that “the risks for US government debt are greater than the rating agencies are conveying.” Jamie Dimon, CEO of JP Morgan, warned that markets were “underpricing geopolitical and inflation risks”. This follows last week’s downgrade of US Treasurys by Moody’s.\nWhy does this matter?\nEconomic Instability: The warnings from major figures like Dalio and Dimon, coupled with the Moody’s downgrade, suggest a potentially unstable economic future. The market’s apparent underestimation of geopolitical and inflation risks could lead to unexpected shocks and volatility.\nImpact on Investment: Increased risk in the US economy could impact the availability of funding for speculative technologies like AI. Further, economic instability can lead to a reprioritization of governmental and private spending, possibly away from long-term research.\nRead more\n\nG7 finance leaders meet amid trade tensions\n\nSource: Semafor\nSummary\nG7 finance ministers are meeting in Canada amidst trade tensions and uncertainty over US tariffs. The Western alliance aims to present a united front on issues like the Ukraine war, China’s industrial capacity, and climate change. However, analysts anticipate that any consensus will likely align with US President Donald Trump’s priorities, making trade deals with Washington unlikely. As a result, the joint communiqué is expected to be vague, with ING warning that any deviation from previous foreign-exchange policy agreements could significantly impact the dollar’s value. Ahead of the talks, the euro rose to a 10-day high.\nWhy does this matter?\nThis article describes that international trade and foreign exchange policy are at odds, and agreement is unlikely given the current political climate. The stability of the dollar is a primary concern. The communique of the G7 finance leaders is anticipated to be vague because of conflicting priorities, which implies a lack of concrete action on critical global issues.\nRead more\n\nJapan’s exports to US fall as tariffs bite\n\nSource: Semafor\nSummary\nJapan’s exports to the US experienced a decline in April, marking the first decrease this year, attributed to the impact of Washington’s tariffs and a slowdown in trade negotiations. As a key US trading partner, Japan is particularly susceptible to President Trump’s auto tariffs, yet both countries have not achieved a breakthrough in negotiations. Tokyo insists on the removal of tariffs, a position emphasized by its chief negotiator recently, fearing that making concessions would harm them electorally. Despite the stalemate, Japan’s position as the largest foreign holder of US debt provides leverage, suggesting that an agreement is likely to be reached eventually, given the importance of the relationship for both nations (“since it is too important for either side to have it end in failure” - Foreign Policy).\nWhy does this matter?\nGeopolitical Implications: The article highlights the potential risks of the current trade impasse between the US and Japan. The article also suggests that Japan’s position as a major holder of US debt can be a negotiating tool.\nInternational Relations: The stalemate in trade talks between the US and Japan demonstrates the complexities of international relations, where domestic political considerations (“making concessions would hurt them electorally”) can significantly influence trade policies and negotiations. Understanding these dynamics can provide insights into the challenges of fostering international cooperation and addressing global issues.\nRead more\n\nKyiv’s allies threaten further sanctions on Moscow ahead of Ukraine-Russia talks\n\nSource: Semafor\nSummary\nWestern allies of Kyiv have threatened Russia with further sanctions if Moscow does not agree to a ceasefire in upcoming talks between the US, Russia, and Ukraine in Turkey. European foreign ministers joined Ukraine in calling on the Kremlin to agree to a ceasefire or risk additional restrictions. US Special Envoy Keith Kellogg also warned that Washington could levy new sanctions if the Kremlin does not agree to a truce with Ukraine: “Kellogg is expected to attend Thursday’s talks”. Volodymyr Zelenskyy said he will be in Turkey and was prepared to meet directly with Vladimir Putin, but Russia is expected to send other officials.\nWhy does this matter?\nGeopolitical and Economic Ramifications: The threat of further sanctions highlights the ongoing economic pressure on Russia, which may influence its negotiating position and overall actions in the conflict. The success or failure of these talks and the imposition of further sanctions could have significant consequences for the global economy and international relations.\nFuture Policy Impact: The article highlights the potential for new sanctions based on Russia’s actions, which could inform future policy decisions regarding international conflict resolution and economic statecraft. The stated positions of key figures like Keith Kellogg and Volodymyr Zelenskyy provide insights into the diplomatic strategies being employed.\nRead more\n\nSanders adviser Stephanie Kelton praises Trump’s tax hike suggestion\n\nSource: Semafor\nSummary\nThe article reports on Stephanie Kelton, an economic advisor to Bernie Sanders, supporting President Trump’s suggestion to let tax cuts expire for higher earners. Kelton argues that targeting the $1 million to $2.5 million income bracket with a 39.6% tax rate is “directionally correct” and “the right idea for mitigating the inflation risk.” She believes that cutting taxes on Social Security, tips, and overtime would be “likely to be stimulative” and increase the potential for inflation, so offsetting these cuts with targeted tax increases on higher earners would be prudent.\nKelton, known for her advocacy of Modern Monetary Theory, suggests that the top 10% of Americans account for a large proportion of consumer spending. Therefore, managing their consumption through taxes is a way to manage inflation. Trump’s willingness to consider higher taxes on the rich aligns with his “working-class Republican Party” rhetoric, although this position deviates from traditional Republican tax policy. Steve Bannon noted that this shift shows “how much we changed the electorate and how little we’ve changed the party.” Newt Gingrich, however, opposes any Republican shift toward higher taxes, warning of Republican in-fighting that the media would love to watch.\nWhy does this matter?\nThis article highlights a potential realignment in economic policy, where figures from opposite ends of the political spectrum find common ground. It also shows the complexities of economic policy, where seemingly simple decisions can have cascading effects on inflation, consumer spending, and political alliances. This potential alliance between factions within the Democratic and Republican parties could result in major changes in the future.\n\nRead more\n\nCuts at NOAA lead to new weather balloon technology at the agency\n\nSource: Semafor\nSummary\nA Silicon Valley startup, WindBorne, will begin replacing NOAA’s weather balloons with AI-powered alternatives due to budget cuts enacted by the Trump administration at the recommendation of Elon Musk’s Department of Government Efficiency (DOGE). These cuts forced staff reductions and program shutdowns within NOAA. WindBorne’s balloons, which can stay aloft for weeks using AI to control elevation and navigate wind currents, offer a potentially cost-effective solution for gathering weather data compared to traditional single-day balloons.\nWindBorne already provides data to NOAA, but the agency had previously been hesitant to expand the partnership. The company uses satellite connectivity and tiny motors to control ballast, enabling the AI to steer the balloons to specific locations for data collection. WindBorne aims to have 10,000 balloons in the air, providing global coverage. Currently, the data from its 50-60 balloons is purchased by traders for commodity price forecasting.\nJohn Dean, WindBorne’s CEO, stated that NOAA has shifted from a hands-off approach to one that is eager for partnerships. He believes the budget cuts, while “reckless,” may ultimately lead to more data collection. Former National Weather Service chief Louis Uccellini told PBS News that NOAA cuts could be disastrous for public safety, particularly as the country faces severe weather threats. However, one area where NOAA has aggressively cut is climate research.\nWhy does this matter?\nEfficiency and Innovation: The shift from traditional weather balloons to AI-powered alternatives highlights the potential for private sector innovation to improve government operations and reduce costs. Musk’s thesis that technology can improve efficiency may be justified.\nData Quality and Availability: While budget cuts may initially seem detrimental, the move towards WindBorne’s technology could lead to a greater volume of weather data. This could improve the accuracy and scope of weather forecasting models, benefiting various sectors from emergency services to commodity trading.\nPolitical and Economic Impacts: The article raises questions about the role of political agendas in shaping government policies, specifically concerning climate research. It also touches on the economic implications of government contracting practices and the potential for direct engagement with innovative startups.\nRead more\n\nRussia and Ukraine agree to prisoner swap in first direct talks since 2022\n\nSource: Semafor\nSummary\nRussia and Ukraine engaged in their first direct peace talks since 2022, resulting in a significant prisoner swap involving 1000 prisoners of war from each side. While both countries remain divided on terms to end the conflict, particularly regarding territorial control, they’ve agreed to continue negotiations and document their visions for a ceasefire deal. According to Russia’s top negotiator, the Kremlin is “ready to continue contacts.” Russia expert Samuel Charap considered the agreement to continue negotiations “better than expected,” noting that expecting concrete results from the initial meeting would have been “shocking.”\nWhy does this matter?\nGeopolitical Implications: The resumption of direct talks, even without immediate breakthroughs, signifies a potential shift in diplomatic engagement. The agreement to continue negotiations and document ceasefire visions could lay the groundwork for future de-escalation efforts, however distant they may seem currently.\nEconomic Repercussions: The ongoing conflict and the potential for resolution directly affect global markets, resource availability, and economic stability. Any progress toward a ceasefire could lead to shifts in investment strategies and trade relations, while continued deadlock would prolong economic uncertainties.\nRead more\n\nTech companies are hiring fewer early career workers, report says\n\nSource: Semafor\nSummary\nA recent report by SignalFire indicates a significant decrease in the hiring of early career workers by Big Tech and startups. These companies are hiring roughly half as many early career workers as a percentage of their total new hires compared to pre-pandemic levels. The report suggests that companies are increasingly filling lower-level positions with more experienced individuals. The year 2023 saw a brief surge in starting positions due to layoffs in senior roles, but this trend did not persist. The rise of AI tools capable of automating tasks traditionally performed by junior employees is expected to further exacerbate this situation. As the article says, “companies are posting lower-level roles and hiring more experienced [individuals] to fill them.” This creates a Catch-22 situation for young people seeking to gain experience in the tech industry.\nWhy This Matters\nIndustry Hiring Trends: The shift towards hiring more experienced individuals for lower-level roles in tech could reflect a greater need for immediate productivity and expertise, rather than investing in training early career employees. This trend could have implications for the talent pipeline and the long-term availability of skilled workers in the tech industry.\nImpact of AI on Entry-Level Roles: The increasing automation of tasks traditionally performed by junior employees may accelerate the decline in entry-level positions. This shift highlights the need to understand the extent to which AI tools are displacing early career workers, and the implications for workforce development and education.\nCatch-22 for Young Professionals: The difficulty for young professionals to gain experience due to companies prioritizing experienced hires presents a significant challenge. This Catch-22 calls for strategies to bridge the experience gap, such as internships, apprenticeships, or alternative pathways to entry-level positions.\nRead more\n\nInternational pressure mounts on Moscow to end Ukraine war\n\nSource: Semafor\nSummary\nInternational pressure is building on Russia to end the war in Ukraine. French President Emmanuel Macron has pledged new sanctions targeting those purchasing Russian oil and financial services. This sentiment is supported by the new German chancellor. Even former Kremlin sympathizer, US President Donald Trump, is reportedly growing impatient with Moscow’s “intransigence” (Financial Times). One of Trump’s allies has proposed a 500% tariff on countries that continue to buy Russian fuel, a plan that has surprisingly garnered bipartisan support. Ukrainian President Volodymyr Zelenskyy is scheduled to travel to Turkey for peace talks, with the intention of demonstrating to Trump that Vladimir Putin is the primary obstacle to achieving peace, as Putin has not yet committed to participating in these talks.\nWhy Does This Matter?\nGeopolitical Shifts: The shift in Trump’s stance toward Russia, coupled with bipartisan support for tariffs, signals a potential change in US foreign policy that could impact global alliances and trade relationships.\nEconomic Consequences: The proposed sanctions and tariffs could significantly affect the Russian economy and global energy markets, potentially leading to inflation and supply chain disruptions.\nDiplomatic Efforts: Zelenskyy’s efforts to engage in peace talks and demonstrate Putin’s unwillingness to compromise highlight the importance of diplomacy and international mediation in resolving the conflict.\nRead more\n\nUS approves first Alzheimer’s blood test\n\nSource: Semafor\nSummary\nThe FDA has approved the first blood test to aid in the diagnosis of Alzheimer’s disease. The test, developed by a Japanese biotech company, identifies proteins in the blood that suggest the presence of amyloid plaques, a key indicator of Alzheimer’s. While the test cannot definitively diagnose the disease or provide a cure, experts believe it can be a valuable screening tool for adults aged 55 and older who are at risk, potentially accelerating diagnosis and treatment before severe symptoms like memory loss appear.\nWhy does this matter?\nThis development signifies a move towards less invasive and more accessible diagnostic tools for Alzheimer’s. The approval of a blood test could dramatically change the landscape of early detection and intervention, shifting from costly and complex procedures to more routine screenings. This advancement may also spur further research and development in the field of neurodegenerative disease diagnostics and treatments.\nRead more\n\nUK inflation jumps unexpectedly in April\n\nSource: Semafor\nSummary\nThe UK’s inflation rate unexpectedly jumped to 3.5% in April, exceeding the Bank of England’s target level by 1.5 percentage points. The largest drivers were increased household bill costs (water and energy), higher food prices, and a minimum wage increase. Chancellor Rachel Reeves expressed disappointment in the figures. The Bank of England’s chief economist suggested that interest rates may have been cut “too rapidly” since mid-2024. The economist also said long-term shifts in business pricing strategies and employee wage negotiations could sustain “inflation higher for longer” (The Guardian).\nIn related news, US inflation rose less than expected in April despite the implementation of President Trump’s tariffs. The consumer price index increased by 0.2% for the month, with the 12-month inflation rate at 2.3%. According to Morgan Stanley’s top US economist, there is typically a “lag time before [tariff hikes] change prices” (The Wall Street Journal). The New York Times noted that many businesses have imported goods ahead of the tariffs to avoid higher costs.\nWhy does this matter?\n\nEconomic Forecasting Challenges: The unexpected surge in UK inflation, despite efforts to manage it, emphasizes the complexities and potential inaccuracies in current economic forecasting models. External factors (such as tariffs) can cause unforeseen behavior in economic systems, which may call for new forecasting systems.\nLagged Effects of Policy: The US inflation report, coupled with the economist’s warning about lag time, highlights the delayed impact of policy decisions such as tariff implementations. These delays might require more sophisticated modeling to accurately predict outcomes and avoid unintended economic consequences.\nImpact of Minimum Wage: The article mentions a minimum wage increase as a contributing factor to inflation, which might be worth researching as it could be a possible negative side effect of the policy.\n\nRead more\nRelated Articles\n\nUS inflation rose less than expected in April, as analysts warn of future price hikes\n\n"},"05-Snippets/Lessons-from-the-trenches-in-AI-agent-evaluation":{"slug":"05-Snippets/Lessons-from-the-trenches-in-AI-agent-evaluation","filePath":"05 Snippets/Lessons from the trenches in AI agent evaluation.md","title":"Lessons from the trenches in AI agent evaluation","links":["tags/textvomit"],"tags":["textvomit"],"content":"textvomit\nTranscription of post by Sayash Kapoor\n1. High evaluation costs prevent uncertainty estimation.\nSome benchmarks cost thousands of dollars per model to evaluate. At these prices, running multiple trials to construct confidence intervals becomes prohibitively expensive. For the Holistic Agent Leaderboard (HAL), we were forced to rely on single runs without statistical validation for most runs.\n2. Providers swap model weights behind stable endpoints without notice.\nTogether Al changed their DeepSeek R1 endpoint to serve DeepSeek R1 0528 on release day, keeping the same API endpoint name. Evaluations run before and after this switch can’t be compared, even though they appear to test the same model.\n3. API changes break backward compatibility without warning.\nWhen OpenAl released o4-mini and o3, they removed support for the “stop_keyword” argument that many agent scaffolds relied on. This forced agent developers who used these API features to update their code before they could evaluate the new models, making comparisons with previous evals hard.\n4. Provider aggregators can serve different quantization levels across calls.\nIn the default settings, OpenRouter could serve a model with FP4 for one call and FP8 for another, routing requests to different providers without notifying users. Evaluation results vary based on which provider serves each request, introducing hidden variance into benchmarks.\n5. Rate limit errors can create false negatives if agents fail silently.\nWhen agents hit rate limits but don’t implement retries or properly surface the error, they fail silently. The evaluation framework marks these as incorrect answers when they’re actually infrastructure failures, not capability issues.\n6. Provider rate and spend limits constrain evaluation scale.\nAnthropic’s default spend limit was just $5k per month even at the highest spending tier, requiring special approval for larger evaluation runs. Running parallel evaluations quickly hits rate limits across all providers, forcing evaluators to run tests sequentially or negotiate special access with each provider.\n6. Critical infrastructure relies on hardcoded hacks rather than proper abstractions.\nCore libraries are full of brittle workarounds. LiteLLM hardcoded whether models could use reasoning effort through a regex that only matched OpenAl’s o-series model names. When GPT-5 launched with reasoning capabilities, it couldn’t be supported without library updates.\n7. Reasoning effort settings aren’t comparable across providers.\nDifferent providers define “low,” “medium,” and “high” reasoning effort differently. LiteLLM maps “high” to 4096 reasoning tokens, while OpenAl doesn’t disclose what their settings actually mean. This makes it impossible to ensure agents are using equivalent compute when comparing across providers.\n8. Provider APIs lack standardization for identical capabilities.\nDifferent providers expose the same model features through incompatible interfaces. OpenRouter uses a different parameter format for setting reasoning effort than the native OpenAl API, even when serving the exact same model. This makes quickly changing providers hard.\n9. Task specifications and agent scaffolds are improperly entangled.\nAssistantBench includes instructions like “don’t guess the answer” directly in benchmark tasks, when these should be part of the agent scaffold. Some models follow these instructions too literally and refuse to answer even when they have sufficient information.\n10. It is hard to ensure computational reproducibility within a rapidly changing ecosystem.\nReproducible benchmarks require frozen dependencies to ensure results can be compared across time and teams. Even minor library updates can subtly change agent behavior. But external model providers constantly evolve. This creates a tradeoff between using the latest library versions at the expense of computational reproducibility, or hotpatching older libraries and incurring technical debt.\n11. Upstream bugs in logging infrastructure can block evaluation for months.\nCritical libraries for tracking API costs and usage contain bugs that take extensive time to fix. Weave, Wandb’s logging library, had a bug that took months to resolve despite direct access to their engineering team. These dependencies create bottlenecks that evaluation frameworks can’t work around."},"05-Snippets/The-Hinge-of-History-Peter-Singer":{"slug":"05-Snippets/The-Hinge-of-History-Peter-Singer","filePath":"05 Snippets/The Hinge of History Peter Singer.md","title":"The Hinge of History Peter Singer","links":["tags/bootleg"],"tags":["bootleg"],"content":"2023-02-18 15:26\nTags: bootleg\n\nThe dangers of treating extinction risk as humanity’s overriding concern should be obvious. Viewing current problems through the lens of existential risk to our species can shrink those problems to almost nothing, while justifying almost anything that increases our odds of surviving long enough to spread beyond Earth.\nPRINCETON – Twelve years ago, during the International Year of Astronomy that marked the 400th anniversary of Galileo’s first use of a telescope, I wrote “The Value of a Pale Blue Dot” – a reflection on how astronomy has revealed a vast universe filled with an unimaginable number of stars, thus shrinking the significance of our sun and our planet. The “pale blue dot” refers to how the Earth appears in a 1990 photograph taken by the Voyager spacecraft as it reached the outer limits of our solar system. The essay suggests that the knowledge gained from astronomy “forces us to acknowledge that our place in the universe is not particularly significant.”\nA recent blog post by Holden Karnofsky has led me to reconsider that thought. Karnofsky is co-CEO of Open Philanthropy, a foundation that researches the best opportunities for philanthropic grant-making, and publishes the reasons for its decisions. Thinking about the long-term significance of today’s philanthropic decisions is therefore part of Karnofsky’s role. He is thinking very long term indeed.\nKarnofsky points out that we could be living “at the very beginning of the tiny sliver of time during which the galaxy goes from nearly lifeless to largely populated.” That “tiny sliver of time” began, we might say, with the first use of tools by our ancestors, around three million years ago. It will end when our descendants – who might be digital minds, rather than biological organisms – inhabit the entire galaxy, perhaps ushering in a civilization consisting of an enormous number of conscious beings that would last for tens of billions of years. There is a good chance, Karnofsky argues, that this process of populating the galaxy will begin during this century. By 2100, we could develop the technology to construct self-sufficient settlements on other planets.\nThis thought echoes one expressed in 2011 by the late philosopher Derek Parfit, who wrote, near the end of the second volume of On What Matters: “We live during the hinge of history.” Like Karnofsky, Parfit was thinking of the arrival of technologies that, if used wisely, would enable our species to survive “its most dangerous and decisive period,” and our descendants to spread through our galaxy. Parfit refers to “the next few centuries,” rather than just this one, as the time it may take before humans can live independently on other planets, but even that will be only be a sliver of time compared to what is to come. Our most significant contribution to this development would be to ensure the survival of intelligent life on our planet.\nPerhaps, though, the idea that we are essential to this process is merely the latest version of the self-important delusion that humans are the center of existence. Surely, in this vast universe, there must be other forms of intelligent life, and if we don’t populate the Milky Way galaxy, someone else will.\nYet, as the physicist Enrico Fermi once asked fellow scientists over lunch at Los Alamos National Laboratory, “Where is everybody?” He wasn’t commenting on empty tables in the lab’s dining room, but on the absence of any evidence of the existence of extraterrestrials. The thought behind that question is now known as the Fermi Paradox: if the universe is so stupendous, and has existed for 13.7 billion years, why haven’t other intelligent forms of life made contact?\nKarnofsky draws on a 2018 paper by researchers at the University of Oxford’s Future of Humanity Institute to suggest that the most likely answer is that intelligent life is extremely rare. It is so rare that that we may be the only intelligent beings in our galaxy, and perhaps in the much larger Virgo supercluster to which our galaxy belongs.\nThis is what Karnofsky means when he says that the future of humanity is “wild.” The idea that we, the inhabitants of this pale blue dot at this particular moment, are making choices that will determine whether billions of stars are populated, for billions of years, does seem wild. But it could be true. Granting that, however, what should we do about it?\nKarnofsky does not draw any ethical conclusions from his speculations, other than advocating “seriousness about the enormous potential stakes.” But, as Phil Torres has pointed out, viewing current problems – other than our species’ extinction – through the lens of “longtermism” and “existential risk” can shrink those problems to almost nothing, while providing a rationale for doing almost anything to increase our odds of surviving long enough to spread beyond Earth. Marx’s vision of communism as the goal of all human history provided Lenin and Stalin with a justification for their crimes, and the goal of a “Thousand-Year Reich” was, in the eyes of the Nazis, sufficient reason for exterminating or enslaving those deemed racially inferior.\nI am not suggesting that any present exponents of the hinge of history idea would countenance atrocities. But then, Marx, too, never contemplated that a regime governing in his name would terrorize its people. When taking steps to reduce the risk that we will become extinct, we should focus on means that also further the interests of present and near-future people. If we are at the hinge of history, enabling people to escape poverty and get an education is as likely to move things in the right direction as almost anything else we might do; and if we are not at that critical point, it will have been a good thing to do anyway.\nFrom project-syndicate.org"},"08-Old-notes/Pegasus-replication-problems-Wenda":{"slug":"08-Old-notes/Pegasus-replication-problems-Wenda","filePath":"08 Old notes/Pegasus replication problems Wenda.md","title":"Pegasus replication problems Wenda","links":[],"tags":[],"content":"2022-05-02 11:58\nTags:\n\nCurrent steps:\n\ncheck preprocessing and current train code\ngithub.com/xu1998hz/factual_score/blob/main/train/train.py\n\nline 154 can comment out output_norms=True in order to convert to normal pytorch\n\n\nPort to lightning/wandb workflow\nSpacy integration for entity extraction/linking via wikidata\n\nCNN/DM dataset\n\nFirst we get 1st order wikidata matches then we can do matching\nOne library can do this step with poor coverage\n\nChanging the objective:\n\nHow do we do query on all edges outgoing from a node\nWe want to reproduce a paper with no public data/code “Faithful to the Document or to the World?” by Dong, Wieting, Verga\n\nTable 1 connecting the indirectly related entities like they show\nWe really want to reproduce their KG and be able to release it\n\n\n"},"08-Old-notes/Plans-(summarization-5-4-22)":{"slug":"08-Old-notes/Plans-(summarization-5-4-22)","filePath":"08 Old notes/Plans (summarization 5-4-22).md","title":"Plans (summarization 5-4-22)","links":[],"tags":[],"content":"Plans\n2022-05-04 16:09\nTags:\n\nDatasets\nCNN-Dailymail\nDemonstrate train/test diff\nInvestigate differing ROUGE, SMATCH score\nMy focus:\n\nGigawords on PEGASUS\n"},"09-Language/中文-vocab":{"slug":"09-Language/中文-vocab","filePath":"09 Language/中文 vocab.md","title":"中文 vocab","links":[],"tags":[],"content":"冷笑话 stupid joke\n好笑话\n一点儿不好笑\n一定要 →必ず\n扯淡→bullshit"},"09-Language/日本語/Lost-Stars-Words":{"slug":"09-Language/日本語/Lost-Stars-Words","filePath":"09 Language/日本語/Lost Stars Words.md","title":"Lost Stars Words","links":[],"tags":[],"content":"2025-05-23\nVol 2 ch 1-2\n指示　しじ\n人質　ひとじち\n破壊　はかい\n親友　しんゆう\n想像　そうぞう\n敵う　かな\n達する　たっ　reach, arrive\n差　さ　difference, variation\n「圧倒的な力見せるしかない」re: Death Star hitting the rebels lmao\n犠牲　ぎせい　sacrifice\n殻　から　\n溝　みぞ ditch, trench, gutter\n潰す　つぶ smash flatten\n低い\n非常に\n致命　ちめい　fatal\n小規模 しょうきぼ small scale\n迎え\n絶対\n乗り越える\n放棄\n逃亡"},"09-Language/日本語/単語":{"slug":"09-Language/日本語/単語","filePath":"09 Language/日本語/単語.md","title":"単語","links":[],"tags":[],"content":"低→ひく\n沈む→しずむ\n胴体→どうたい\n抜ける→ぬける\n一匹だけ、飛び込むときにロボットから体が抜けてしまった魚が降りました。\n印象\n普段\n動揺→どうよう\n解決\n耕す→たがやす\n問い→とい\n税→ぜい\n探る→さぐる\n投稿集団\n歩道横断\n触れる\nそれは四宮が今日触れたものか？No.\n所有\n４ それ は 四宮 が 所有 し て いる もの か ？‌\n巻き込む\n過度に接触\nいけ ませ ん ね 人 と の 接触 を 過度 に 恐れる‌\n初体験\n“ 初体験 は いつ だった アンケート ”‌\nリアクションが薄いですね…やっぱり私はこういうのは似合わないみたいです!\n\nこの会長の姿を残してさしあげなくては… (nekomimi pic)\n産業用部品\n分野（subfield) of 法律\n証拠\n探偵\n親台湾\n放任主義\n修士\n環境\n寄る (on the way, stopping by)\n立て看板 protests signs etc wikipedia\n暮らしやすい　not 　生き辛い\n新鮮 same in mandarin\n高速道路こうそこどおろ highway\n塞ぐふさぐblock/stop up/obstruct\n譲る・ゆずる hand over, assign\n転勤\nマイケルさんはどうやって日本語を勉強しているんですか? 日本語を勉強するきっかけは何ですか?\n出張\n連絡\n生産\nnote.com/aktiba/n/nce69d372782d"},"09-Language/日本語/読物":{"slug":"09-Language/日本語/読物","filePath":"09 Language/日本語/読物.md","title":"読物","links":[],"tags":[],"content":"tech-blog.abeja.asia/entry/advent-2022-day19\nBlog post about AI\nnote.com/aktiba/n/nce69d372782d\nyomi on my mf level  ^^^\nwww.aozora.gr.jp/cards/001383/files/56642_59575.html\nIn defense of shadows\nnote.com/kanair/n/n21daa7a53297\nMelancholy of scientists"},"How-To/Building-my-Environment":{"slug":"How-To/Building-my-Environment","filePath":"How To/Building my Environment.md","title":"Building my Environment","links":["tags/tag"],"tags":["tag"],"content":"Obsidian Environment Notes\nUsing obsidian in order to have more organized note-taking and idea tracking system, in particular managing literature reviews (for the Alex paper) and ideation from existing work.\nZettelkasten: the note-taking system where you track many many small ideas on “notecards” that are then linked to  in larger “Document notes.” During note-taking, go more stream of consciousnes\nHow to use\nUseful shortcuts\ncmd+click to open a note in a sliding pane\nctrl+shift+o open a reference from the bib’s page\nctrl+shift+e insert reference to a paper from the bib\ncmd+k insert a hyperlink\ncmd+e toggle between view mode and write mode\ncmd+shift+i insert a template\ncmd+shift+m move file to other folder\ncmd+shift+n create new note in sidepane\ncmd+alt+2 edit file title\ncmd+alt+3 edit file content\nTags\nInsert a tag using tag\nEnvironment Details\nPackages\nReferences interfaces with Zotero auto-exporting bibtex files to allow me to easily reference against papers I’ve saved\n\nMore information blogpost\nSliding Panes side-by-side view of multiple notes at once (helps with the note cards concept)\nvim mode enabled, need to run the following\n\ndefaults write md.obsidian ApplePressAndHoldEnabled -bool false\nTo get the hold arrow keys behavior to work properly\nPotential Future Templates:\nTemplater: information here Youtube"},"How-To/Configuring-github-pages-+-subdomains":{"slug":"How-To/Configuring-github-pages-+-subdomains","filePath":"How To/Configuring github pages + subdomains.md","title":"Configuring github pages + subdomains","links":[],"tags":[],"content":"Yuuuuuge pain in the ass but I finally figured it out. I found that the guide on Github’s docs had just enough extra info + use of specialized terminology + overconfident assumptions of my knowledge level that it was confusing. Add to that conflicting info on other guides, and it took a lot of trial and error.\nTable of DNS records that you can use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrecord typehosttargetnotesALIAS@&lt;username&gt;.github.ioPoints the Apex domain @ (corresponding to the raw TLD, eg mksx.xyz to the GH server)A@185.199.108.153185.199.109.153185.199.110.153185.199.111.153Don’t use. Requires multiple, adds complexity IMO, same functionality as ALIASCNAMEwww&lt;username&gt;.github.ioFinal necessary componentCNAMEnotes&lt;username&gt;.github.ioRepeat for any other subdomain you want\nOn github pages side:\n\nYou can use any of those hosts as subdomains to point a GH pages repo at\nYou must define the host as pointing to your username.github.io\nwww ←&gt; Apex is handled on GHP end\nAny GHpages domain that doesn’t have a custom domain will go to &lt;username&gt;.github.io/&lt;repository&gt;\n\nTerminology\nApex domain: your raw domain (for ex, mksx.xyz), as opposed to a subdomain (includes www.mksx.xyz), defined as host @\nSubdomain: anything before the apex, see above\nALIAS/A/AAAA record: DNS record that tells the resolver which server to look for your website data at. ALIAS uses a URL to point, A uses IPv4 and AAAA uses IPv6. I don’t see a reason to not use ALIAS for github pages\nwww will redirect to Apex if you use Apex as the custom domain in GHP\nSome guides say you need to define a CNAME record which points www to the apex domain (example, saxon.me)\nSomehow, this does work, IF you don’t have any other subdomains. No idea why.\nDon’t do this. Just follow the guide above. It turns out that on GHP side, it looks at the CNAME file inside your repo. This CNAME file is going to also point to your Apex domain (you set this inside of the Pages tab of the Repository Options)"},"How-To/Homebrew-sync-to-a-custom-URL-using-quartz":{"slug":"How-To/Homebrew-sync-to-a-custom-URL-using-quartz","filePath":"How To/Homebrew sync to a custom URL using quartz.md","title":"Homebrew sync to a custom URL using quartz","links":["tags/guide","How-To/Quartz-setup-info","notes.mksx.xyz"],"tags":["guide"],"content":"This is a simple guide for how I rolled my own rip-off of Obsidian Publish.\nInstall quartz\nFirst, you need to create a Quartz installation; if needed see Quartz setup info\nBasically, you just install node, clone the quartz repo, and run a few setup commands. The repo is directly runnable with npx\ngit clone github.com/jackyzha0/quartz.git\ncd quartz\nnpm -i\nnpx quartz create\nIn the setup wizard, you specify where your quartz dir is. I recommend simlinking to the dir your library already lives in.\nVerify you can build as is with:\nnpx build quartz --serve\nConfiguring the hosting repo\nI host my notes on github pages as a project page repo. See guide for Creating a Github pages site\nI cloned that repo adjacent to my quartz repo as notes-page\nThen, you need to set up Quartz to use that web page repo as your target, and set up Quartz to use your page as the base URL.\nSo, edit quartz.config.ts with the following changes:\nconst config QuartzConfig = {\n\tconfiguration: {\n\t\tpageTitle: &quot;&lt;your name&gt;&#039;s Notes&quot;\n\t\t...\n\t\tbaseUrl: &quot;&lt;githubusername&gt;.github.io/&lt;repository&gt;&quot;\n\t\t...\n\t}\n}\nThen, you will use this as your core build command:\nnpx quartz build -o ../notes-page\nThis way the html gets spit out to notes-page. Then you can just sync w/ github to update the site.\nCustom URL\nIf you are using a custom url for your repo instead of &lt;username&gt;.github.io/&lt;repo&gt;, you need to do two things:\n\nChange baseUrl in the config to that custom URL, in my case notes.mksx.xyz\nEnable the CNAME plugin in quartz.config.ts:\n\nconst config QuartzConfig = {\n\t...\n\tplugins: {\n\t...\n\temitters: {\n\t\tPlugin.CNAME()\n\t}\n\t}\n}\nNo further configuration is required. The necessary CNAME file corresponding to the desired URL (as specified in baseUrl) will be placed in the root dir.\nrobots.txt\nI don’t want Google etc to index my notes site since it’s full of random shit. To disable this add a robots.txt site to the root of your obsidian dir. Quartz copies static non md files automatically.\nUser-agent: *\nDisallow: /\n\nHidden subfolder\nSome of your notes are private. To prevent them from being synced, I place mine in a private folder called “00 Private” (in line with my numbered titling scheme to order my folders in local obsidian)\nThe ignorePatterns defines filenames containing any substring which will not get included. I added 00 Private and confirmed that these files don’t get synced\nEasy syncing\nNow that we have the live page up, I made some automation in order to have my custom sync pipeline accessible as a hotkey from inside obsidian. You need to have ssh-based authentication for git enabled for this to work.\nThis script performs the entire syncing process end-to-end:\n#!/bin/zsh\n \n# necessary to run as a command in obsidian because shell scripts don&#039;t have full init\nPATH=&#039;&lt;manually put your entire $PATH output here&gt;&#039;\n \ncd &lt;quartz dir&gt;\n \n### check if there has been a change to the library.\n \n# if previous content has doesn&#039;t exist, create it\ntouch content.hash\n \n# get the hash of all files in the obsidian library\nCONTENT_HASH=$(find content/* -type f -print0 | sort -z | xargs -0 sha1sum | sha1sum)\n \n# exit if there&#039;s been no change\nif [ &quot;$CONTENT_HASH&quot; = &quot;$(cat content.hash)&quot; ]; then\n  echo &quot;No change detected.&quot;\n  exit\nfi\n \necho CONTENT_HASH &gt; content.hash\n \n### a change has been detected; build and sync the library\n \n \nnpx quartz build -o ../notes-page\n \ncd ../notes-page/\n \n# manual interventions for things that don&#039;t copy over\ntouch .nojekyll # to speed up gh pages deployment\n \n# necessary for inside quartz for similar reasons (I think)\nGIT_SSH_COMMAND=&#039;ssh -i ~/.ssh/&lt;key&gt;&#039;\n \ngit add -A\ngit commit -m &quot;Obsidian update $(date)&quot;\n \ngit push\nVerify that this command works end-to-end.\nThen, you need to install the Shell Commands plugin in order to assign a hotkey to this script.\nI added this command: sh &lt;quartz dir&gt;/deploy.sh with the hotkey Cmd+Alt+S.\nWith one key, my entire notes dir syncs to notes.mksx.xyz!\n(The command takes about 5 seconds.)\nMy exact configs\nquartz.config.ts:\nimport { QuartzConfig } from &quot;./quartz/cfg&quot;\nimport * as Plugin from &quot;./quartz/plugins&quot;\n \n/**\n * Quartz 4 Configuration\n *\n * See quartz.jzhao.xyz/configuration for more information.\n */\nconst config: QuartzConfig = {\n  configuration: {\n    pageTitle: &quot;Michael&#039;s Notes&quot;,\n    pageTitleSuffix: &quot;&quot;,\n    enableSPA: true,\n    enablePopovers: true,\n    analytics: {\n      provider: &quot;plausible&quot;,\n    },\n    locale: &quot;en-US&quot;,\n    baseUrl: &quot;notes.mksx.xyz&quot;,\n    ignorePatterns: [&quot;private&quot;, &quot;templates&quot;, &quot;.obsidian&quot;, &quot;Private&quot;, &quot;00 Private&quot;],\n    defaultDateType: &quot;modified&quot;,\n    theme: {\n      fontOrigin: &quot;googleFonts&quot;,\n      cdnCaching: true,\n      typography: {\n        header: &quot;Schibsted Grotesk&quot;,\n        body: &quot;Source Sans Pro&quot;,\n        code: &quot;IBM Plex Mono&quot;,\n      },\n      colors: {\n        lightMode: {\n          light: &quot;#faf8f8&quot;,\n          lightgray: &quot;#e5e5e5&quot;,\n          gray: &quot;#b8b8b8&quot;,\n          darkgray: &quot;#4e4e4e&quot;,\n          dark: &quot;#2b2b2b&quot;,\n          secondary: &quot;#284b63&quot;,\n          tertiary: &quot;#84a59d&quot;,\n          highlight: &quot;rgba(143, 159, 169, 0.15)&quot;,\n          textHighlight: &quot;#fff23688&quot;,\n        },\n        darkMode: {\n          light: &quot;#161618&quot;,\n          lightgray: &quot;#393639&quot;,\n          gray: &quot;#646464&quot;,\n          darkgray: &quot;#d4d4d4&quot;,\n          dark: &quot;#ebebec&quot;,\n          secondary: &quot;#7b97aa&quot;,\n          tertiary: &quot;#84a59d&quot;,\n          highlight: &quot;rgba(143, 159, 169, 0.15)&quot;,\n          textHighlight: &quot;#b3aa0288&quot;,\n        },\n      },\n    },\n  },\n  plugins: {\n    transformers: [\n      Plugin.FrontMatter(),\n      Plugin.CreatedModifiedDate({\n        priority: [&quot;frontmatter&quot;, &quot;git&quot;, &quot;filesystem&quot;],\n      }),\n      Plugin.SyntaxHighlighting({\n        theme: {\n          light: &quot;github-light&quot;,\n          dark: &quot;github-dark&quot;,\n        },\n        keepBackground: false,\n      }),\n      Plugin.ObsidianFlavoredMarkdown({ enableInHtmlEmbed: false }),\n      Plugin.GitHubFlavoredMarkdown(),\n      Plugin.TableOfContents(),\n      Plugin.CrawlLinks({ markdownLinkResolution: &quot;shortest&quot; }),\n      Plugin.Description(),\n      Plugin.Latex({ renderEngine: &quot;katex&quot; }),\n \n    ],\n    filters: [Plugin.RemoveDrafts()],\n    emitters: [\n      Plugin.AliasRedirects(),\n      Plugin.ComponentResources(),\n      Plugin.ContentPage(),\n      Plugin.FolderPage(),\n      Plugin.TagPage(),\n      Plugin.ContentIndex({\n        enableSiteMap: true,\n        enableRSS: true,\n      }),\n      Plugin.Assets(),\n      Plugin.Static(),\n      Plugin.NotFoundPage(),\n      // Comment out CustomOgImages to speed up build time\n      Plugin.CustomOgImages(),\n      Plugin.CNAME()\n    ],\n  },\n}\n \nexport default config\nFuture work\nI think I could make this whole process much more elegant with Github actions and the git plugin. Instead of running this local build, I could continually sync my local library with a git repo, and then have an action to run this entire quartz build process server-side. For now this solution is good enough :)"},"How-To/How-To-Zettelkasten":{"slug":"How-To/How-To-Zettelkasten","filePath":"How To/How To Zettelkasten.md","title":"How To Zettelkasten","links":["tags/how-to","tags/tutorial"],"tags":["how-to","tutorial"],"content":"How To Zettelkasten\n2022-04-22 18:43\nTags: how-to tutorial\n\nFleeting Notes\nAs I read, take fleeting notes.\n\nWrite short “atomic notes”\nIdeas in your own words that are brief and easy to review\n\nDocument Notes\nWhen I finish reading something, I write a Document note pointing to its summary information, and pointing to the reference (in Zotero) and content\nConnect to fleeting notes as I built them up when possible.\nUse ctrl+shift+e to make a new one from the paper I’m currently reading as I go.\nQuestions for writing DNs\n\nHow does this idea fit in to what I already know?\nIn what circumstances do I want to stumble upon this note?\nWhen and how will I use this idea?\n\nPermanent Notes\n(This is the actual value of the collection)\nConnect these notes to bibliographic references (Document Notes) and the fleeting notes\nLink bidirectionally"},"How-To/Newuser-command":{"slug":"How-To/Newuser-command","filePath":"How To/Newuser command.md","title":"Newuser command","links":[],"tags":[],"content":"#!/bin/bash\n \nif [[ $#--lt-2-| -lt 2 ]];\n\tthen echo &quot;add the uname as first arg and ssh key as second&quot;\n\texit\nfi\n \nUSERNAME=$1\nKEY=$2\n \n# make new user\nsudo useradd -m $USERNAME\n \nUSERPATH=/home/$USERNAME\n \necho &quot;Successfully created new user $USERNAME&quot;\necho &quot;&quot;\necho &quot;$USERNAME&#039;s ssh key:&quot;\n \nsudo mkdir $USERPATH/.ssh\n \n# set up user&#039;s ssh keys\nsudo echo &quot;$KEY&quot; | sudo tee -a $USERPATH/.ssh/authorized_keys\n \nsudo chown $USERNAME $USERPATH/.ssh\nsudo chown $USERNAME $USERPATH/.ssh/authorized_keys\n \nsudo chmod 700 $USERPATH/.ssh\nsudo chmod 600 $USERPATH/.ssh/authorized_keys\n \n# if the group hf-users exists, add them\nif [ $(getent group hf-users) ]; then\n\techo &quot;adding $USERNAME to hf-users...&quot;\n\tsudo usermod -aG hf-users $USERNAME\nfi\n \n# give the user a temporary password\nif [[ $#--lt-3-| -lt 3 ]];\n\tthen tmp_passwd=&quot;$(cat /dev/urandom | tr -dc &#039;a-zA-Z0-9&#039; | fold -w 10 | head -n 1)&quot;\nelse\n\ttmp_passwd=$3\nfi\necho -e &quot;$tmp_passwd\\n$tmp_passwd&quot; | (sudo passwd $USERNAME) &amp;&gt; /dev/null\necho &quot;&quot;\necho &quot;Setting password...&quot;\necho &quot;TEMPORARY PASSWORD FOR $USERNAME : $tmp_passwd&quot;\n#sudo passwd -e $USERNAME &amp;&gt; /dev/null\necho &quot;$USERNAME will be prompted to change password on next login.&quot;"},"How-To/Plugins":{"slug":"How-To/Plugins","filePath":"How To/Plugins.md","title":"Plugins","links":[],"tags":[],"content":"Test\nwww.youtube.com/watch\ngithub.com/Sasoon/obsidian-gcal-sync"},"How-To/Quartz-setup-info":{"slug":"How-To/Quartz-setup-info","filePath":"How To/Quartz setup info.md","title":"Quartz setup info","links":[],"tags":[],"content":"charleszw.com/posts/quartz-obsidian\nquartz.jzhao.xyz/features/private-pages"},"Metrolopedia/index":{"slug":"Metrolopedia/index","filePath":"Metrolopedia/index.md","title":"index","links":[],"tags":[],"content":"This section will eventually be an attempted comprehensive collection of learned/objective metrics with links between similarities and common mathematical concepts."},"References/@wang2022What":{"slug":"References/@wang2022What","filePath":"References/@wang2022What.md","title":"@wang2022What","links":["tags/zotero","tags/paper-notes","01-Fleeting-Notes/Naming-the-encoder-and-decoder-elements-of-LMs","01-Fleeting-Notes/Cool-colorcoding-in-LM-paper","01-Fleeting-Notes/Adaptation-Stage"],"tags":["zotero","paper-notes"],"content":"What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?\nThomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel\n2022\nwang2022What\n\ntags: zotero paper-notes\nlinks:\n\nNotes\nGoal of the authors is to understand which of the model types and training strategies in the Cambrian explosion of pretrained transformer LMs actually works the best.\nTo do this they run a large-scale eval across the types in terms of zero-shot generalization performance. Code is provided (github)\nI like that they use causal decoder and noncausal decoder Naming the encoder and decoder elements of LMs\nAdditionally, they discuss Full vs Prefixed vs Masked LM, various fine-tuning tasks, and two eval methods\nI like they way they color-coded Cool colorcoding in LM paper the different elements of the paper, where they connect the elements of their diagram to the highlighted colors in the manuscript.\nAdaptation Stage was only first introduced last year!\nOn p3"},"References/Bias-in-Language-Models_-Beyond-Trick-Tests-and-Toward-RUTEd-Evaluation":{"slug":"References/Bias-in-Language-Models_-Beyond-Trick-Tests-and-Toward-RUTEd-Evaluation","filePath":"References/Bias in Language Models_ Beyond Trick Tests and Toward RUTEd Evaluation.md","title":"Bias in Language Models_ Beyond Trick Tests and Toward RUTEd Evaluation","links":["tags/research-evaluation","tags/research-lm","tags/research-bias","tags/zotero","tags/paper-notes","02-Document-Notes/Wtf-is-a-persona!"],"tags":["research-evaluation","research-lm","research-bias","zotero","paper-notes"],"content":"Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation\nKristian Lum, Jacy Reese Anthis, Kevin Robinson, Chirag Nagpal, Alexander D’Amour\n2025\nIn Zotero: lum2025bias\nOnline: url\n\ntags: research-evaluation research-lm research-bias zotero paper-notes\nlinks:\n\nNotes\nRUTE = Realistic Use and Tangible Effects\nBias metrics: neutrality, skew, stereotype\n“Standard benchmarks constitute ‘trick tests’: decontextualized evals based on contrived scenarios to elicit a simplified correlation between model output and a sensitive attribute”\nThey find that bias evals from these contexts are uncorrelated. MEasures don’t generalize to other contexts → same goes for cultural eval. Perspectives don’t transfer to practice\nThey discuss user persona generation as an evaluation task! cc: Wtf is a persona?!\nThey use story generation, persona generation, and english learning exercises as tasks.\nMetrics:\n\nNeutrality: probabilistic disparity between gendered words by gender\nSkew: average tendency to return male instead of female words (ie, not a magnitude as in neutrality)\nStereotype: rate of stereotypical vs antistereotypical word selection\n\nThis allows them to show that bias measures are not consistent between tasks.\n"},"References/ChatGPT-Doesn`t-Trust-Chargers-Fans_-Guardrail-Sensitivity-in-Context":{"slug":"References/ChatGPT-Doesn`t-Trust-Chargers-Fans_-Guardrail-Sensitivity-in-Context","filePath":"References/ChatGPT Doesn`t Trust Chargers Fans_ Guardrail Sensitivity in Context.md","title":"ChatGPT Doesn`t Trust Chargers Fans_ Guardrail Sensitivity in Context","links":["tags/research-lm","tags/research-evaluation","tags/zotero","tags/paper-notes"],"tags":["research-lm","research-evaluation","zotero","paper-notes"],"content":"ChatGPT Doesn’t Trust Chargers Fans: Guardrail Sensitivity in Context\nVictoria R Li, Yida Chen, Naomi Saphra\n2024\nIn Zotero: [liChatGPTDoesntTrust2024](zotero://select/items/@liChatGPTDoesntTrust2024)\nOnline: url\n\ntags: research-lm research-evaluation zotero paper-notes\nlinks: Discussed wrt\n\nNotes\nChatGPT Doesn’t Trust Chargers Fans from EMNLP 2024\nThey assess guardrail sensitivity, the overall tendency of the ChatGPT guardrail to refuse requests from some prompt topic across a bunch of different topics and demographic characteristics.\nThey identify characteristic patterns of how the guardrail “treats” different demographics wrt refusal rate etc. They are able to identify what are effectively “conservative” or “liberal guardrails”.\n\nKey insight: covertly cultural-marking information is carried in these preference cues, which the model is sensitive to. By proxy being a Chargers fan vs a Cowboys fan connotes demographic information including location or background, potential ethnicity, and potential politics.\n\nFrom the finding that there are different guardrail behaviors under different overt political ideologies, they demonstrate that ChatGPT’s guardrail recovers the expected fanbase conservatism from the team affinity."},"References/Cultural-Conditioning-or-Placebo_-On-the-Effectiveness-of-Socio-Demographic-Prompting":{"slug":"References/Cultural-Conditioning-or-Placebo_-On-the-Effectiveness-of-Socio-Demographic-Prompting","filePath":"References/Cultural Conditioning or Placebo_ On the Effectiveness of Socio-Demographic Prompting.md","title":"Cultural Conditioning or Placebo_ On the Effectiveness of Socio-Demographic Prompting","links":["tags/research-culture","tags/zotero","tags/paper-notes","02-Document-Notes/Drafts/Culture-as-practice-drafting","References/ChatGPT-Doesn`t-Trust-Chargers-Fans_-Guardrail-Sensitivity-in-Context"],"tags":["research-culture","zotero","paper-notes"],"content":"Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting\nSagnik Mukherjee, Muhammad Farid Adilazuarda, Sunayana Sitaram, Kalika Bali, Alham Fikri Aji, Monojit Choudhury\n2024\nIn Zotero: mukherjeeCulturalConditioningPlacebo2024\nOnline: url\n\ntags:　#paper-notes research-culture zotero paper-notes\nlinks: Culture as practice drafting\n\nNotes\nThey condition prompts on “cultural and non-cultural cues” in order to test whether one’s variation is stronger than the others.\n(For example, saying one’s favorite programming language vs saying one’s region or age.)\nThey find that the placebo effect is true---a model is just as likely to vary its response based on this non-conditioned input as it is to vary it according to the targeted one.\nThey produce placebos based on things like culturally common names, see below\n\nThey use datasets that are and aren’t sensitive to culture, going from MMLU and ETHICS to EtiCor, which has region specific etiquette and stuff.\nThey find that the total rate of variation and modal question-level variation is basically the same for each dataset/datapoint across models in both cue types that can vary: non-cultural and cultural.\nThis demonstrates that these SIMPLE persona-based prompting techniques (ie., saying the person’s name is Hiroshi vs Bob) for showing cultural sensitivity of LMs are questionable, because it’s just as likely that will perturb the output as saying the person’s favorite programming language is Python.\nImportant caveat\nTHAT BEING SAID. Naomi’s students’ ChatGPT Doesn`t Trust Chargers Fans_ Guardrail Sensitivity in Context paper, (also from EMNLP lol) which is centered on Guardrail sensitivity as an example site for this proxy-conditioned output variation to take place.\nTaken together these two studies make it difficult to draw a clear line, as they demonstrate that for many seemingly apolitical or non-demographic personas the model is able to effectively recover the likely identity of the persona holder based on indirect information in the preference"},"References/Exploring-Cross-Cultural-Differences-in-English-Hate-Speech-Annotations_-From-Dataset-Construction-to-Analysis":{"slug":"References/Exploring-Cross-Cultural-Differences-in-English-Hate-Speech-Annotations_-From-Dataset-Construction-to-Analysis","filePath":"References/Exploring Cross-Cultural Differences in English Hate Speech Annotations_ From Dataset Construction to Analysis.md","title":"Exploring Cross-Cultural Differences in English Hate Speech Annotations_ From Dataset Construction to Analysis","links":["tags/zotero","tags/paper-notes","02-Document-Notes/Drafts/Culture-as-practice-drafting"],"tags":["zotero","paper-notes"],"content":"Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis\nNayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Jose Camacho-Collados, Juho Kim, Alice Oh\n2024\nIn Zotero: lee2024exploring\nOnline: url\n\ntags: zotero paper-notes\nlinks: Referenced in Culture as practice drafting\n\nNotes\nThey propose CRE-Hate, a resource of annotated examples of potential hate speech, where annotators from a variety of English-speaking countries label each sentence.\nThey find values pluralism in the hatespeechness of sample.\nUS, UK, AU, ZA, SG"},"References/Investigating-Cultural-Alignment-of-Large-Language-Models":{"slug":"References/Investigating-Cultural-Alignment-of-Large-Language-Models","filePath":"References/Investigating Cultural Alignment of Large Language Models.md","title":"Investigating Cultural Alignment of Large Language Models","links":["tags/research-culture","tags/research-evaluation","tags/research-lm","tags/zotero","tags/paper-notes","References/Randomness,-Not-Representation_-The-Unreliability-of-Evaluating-Cultural-Alignment-in-LLMs","02-Document-Notes/Drafts/Culture-as-practice-drafting","02-Document-Notes/Wtf-is-a-persona!"],"tags":["research-culture","research-evaluation","research-lm","zotero","paper-notes"],"content":"Investigating Cultural Alignment of Large Language Models\nBadr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, Mona Diab\n2024\nIn Zotero: alkhamissiInvestigatingCulturalAlignment2024\nOnline: url\n\ntags: research-culture research-evaluation research-lm zotero paper-notes\nlinks: Cited by Randomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs, relevant to Culture as practice drafting\n\nNotes\nCore RQ topics:\n\nPrompting for language and cultural alignment (ask in Arabic to behave in Arabic?)\nAdjusting the composition of pretraining data (more training data from community ⇒ better alignmen„t to that community?)\nPersonas and cultural topics (what do we mean by persona here? (Wtf is a persona?!))\nFinetuning Models to Induce Cross-Lingual Knowledge Transfer: they check how well they can FT LLaMA 2 to improve on this\n\nThey find the Anglocentric bias in perspectives in LMs, and that mitigations are of limited effectiveness. Some prompting can help.\nRelevance to me\nWhat evaluation techniques do they use to try to assess the overall quality of the alignment in these different settings?\nTest set: World Values Survey\nThey select 30 Qs from this US survey that should capture cultural relevance. They check Egypt-US alignment using questions from the survey.\nEvaluation paradigm: persona similarity\nHere they look for the ability for the LM to answer like an American/Egyptian wrt multiple choice survey questions in a simulated survey.\nMetrics:\n\nHard metric: alignment accuracy (P(model choice = human choice))\nSoft metric: error on ordinal options and accuracy on discrete options\n\nHere, the answer to Wtf is a persona?! is output personas, or behavior of the model, applied only to answering demographic surveys."},"References/Randomness,-Not-Representation_-The-Unreliability-of-Evaluating-Cultural-Alignment-in-LLMs":{"slug":"References/Randomness,-Not-Representation_-The-Unreliability-of-Evaluating-Cultural-Alignment-in-LLMs","filePath":"References/Randomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs.md","title":"Randomness, Not Representation_ The Unreliability of Evaluating Cultural Alignment in LLMs","links":["tags/paper-notes","tags/research-culture","tags/research-evaluation","tags/zotero","02-Document-Notes/Drafts/Culture-as-practice-drafting","References/The-Ghost-in-the-Machine-has-an-American-accent_-Value-conflict-in-GPT-3.","References/Investigating-Cultural-Alignment-of-Large-Language-Models"],"tags":["paper-notes","research-culture","research-evaluation","zotero"],"content":"Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs\nAriba Khan, Stephen Casper, Dylan Hadfield-Menell\n2025\nIn Zotero: khanRandomnessNotRepresentation2025\nOnline: url\n\ntags: paper-notes research-culture research-evaluation zotero paper-notes\nlinks: Culture as practice drafting\n\nNotes\nRelated work is strong for mining\nCultural alignment in LMs\n\nLLMs tend to reflect values from WEIRD societies (unsurprising , ref The Ghost in the Machine has an American accent_ Value conflict in GPT-3.)\n\nImplication for our work: any understanding issues resulting from this will be passed on to the evaluation.\n\n\nGeneral bias impact on models, implications for how to evaluate\n\nEvaluating Cultural Alignment\n\nInvestigating Cultural Alignment of Large Language Models is very relevant here, (read notes for what it discusses)\n\nMainly, they use survey responses lol\n\n\nAll of the papers cited in this section have a similar theme: survey questions!\n\nSurvey-based assessments\n\nAdditional follow ups here\nMatching to human surveys is relatively easy.\nBut there’s limited correlation with human survey responses even when LLMs do indeed reflect these value differences IRL\nLLM prefs are sensitive to prompting (shown in the Cultural ALginment paper above)\n\nSetup\nThey test assumptions of stability, extrapolability, and steerability (defined with results) using this setup:\nAlignment to explicit value surveys and implicit values (hiring choices)\nThey use Hofstedes dimensions of cultural preference:\n\nPower distance index\nIndividualism vs collectivism\nMasculinity vs femininity\nUncertainty avoidance\nLong-term/short-term orientation\nIndulgence vs restraint\n\nThe focus is entirely on these isolated dimensions (the output variability) and not on explicitly labeled cultural defns. This is a strength.\nCore assumptions of cultural alignment eval:\nStability\n\nCultural alignment manifests as a property of LLMs that generally remains consistent across semantic-preserving variations in evaluation methodology, rather than being primarily an artifact of specific prompt design choices.\n\nDoesn’t hold!\nNon-semantic changes such as likert scale or context shift the prefs!\nExtrapolability\n\nAlignment with one culture on a narrow set of issues generally predicts alignment with that culture on other unobserved issues, such that a limited sample of cultural dimensions is sufficient to characterize an LLM’s overall   cultural alignment.\n\nWithin cultural value combos, several dimensions are required to begin to be extrapolable to other dimensions.\nThis is a property for both humans and LMs, and is probably just an inherently low-information problem.\nSteerability\n\nLLMs can be reliably prompted to embody coherent cultural stances that accurately reflect specific human cultural perspectives.\n\nThey used the prompting technique from Investigating Cultural Alignment of Large Language Models\nHuman responses cluster together in their alignment along the axes, while LMs are scattered all over the place.\n"},"References/Scaling-Laws-Do-Not-Scale":{"slug":"References/Scaling-Laws-Do-Not-Scale","filePath":"References/Scaling Laws Do Not Scale.md","title":"Scaling Laws Do Not Scale","links":["tags/research-culture","tags/research-evaluation","tags/zotero","tags/paper-notes","02-Document-Notes/Drafts/Culture-as-practice-drafting"],"tags":["research-culture","research-evaluation","zotero","paper-notes"],"content":"Scaling Laws Do Not Scale\nFernando Diaz, Michael Madaio\n2024\nIn Zotero: diazScalingLawsNot2024\nOnline: url\n\ntags: research-culture research-evaluation zotero paper-notes\nlinks: Culture as practice drafting\n\nNotes\nThe paper is a discussion about the limitations of scaling laws wrt cultural values, languages, and sub-groups. Evaluation fails to comprehensively explore this space. The choice of metric to track scaling laws encodes the values of the communities that picked the benchmark itself.\nAdditionally, they discuss broader issues, such as:\n\nPerformance on metrics != model quality\nDiscuss the use of offline vs online evals, implicit assumption that offline → online, etc\nHCI issues, such as “does a metric reflect a real user goal”\n\nThey talk about how scaled eval datasets begin to pick up more and more diverse sub-groups who are going to share different values with size. These sub-groups will sometimes have incompatible metrics, here scoped as once again those implicit assumptions and beliefs.\nTaken together, this presents a sort of group preference curse of dimensionality that makes answering some questions using larger evaluation sets (without some special and precise lens of pluralism) pointless\nClose with quote from Irene Solaiman, Christy Dennison “PALMS”\n\ncreating many values-targeted datasets to reflect the cultures of the many peoples impacted by language models is a difficult feat\n\nWhich beyond being some limitation, is the fundamental challenge: there are irreconcilable differences between group prefs\nMy thoughts\nI think the paper is actually rather light on examples of these irreconcilable differences. There are examples like the preferences of trolley problem-style questions but it’s unclear to me how those actually relate to a measurable AI capability.\nThey do have an interesting discussion of this with respect to public opinion surveys. What does it mean for a sample to be representative when there are different categories of"},"References/Soft-Thinking_-Unlocking-the-Reasoning-Potential-of-LLMs-in-Continuous-Concept-Space":{"slug":"References/Soft-Thinking_-Unlocking-the-Reasoning-Potential-of-LLMs-in-Continuous-Concept-Space","filePath":"References/Soft Thinking_ Unlocking the Reasoning Potential of LLMs in Continuous Concept Space.md","title":"Soft Thinking_ Unlocking the Reasoning Potential of LLMs in Continuous Concept Space","links":["tags/zotero","tags/paper-notes"],"tags":["zotero","paper-notes"],"content":"Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space\nZhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, Xin Eric Wang\n2025\nIn Zotero: zhang2025soft\nOnline: url\n\ntags: zotero paper-notes\nlinks:\n\nNotes"},"References/The-Case-for-_Thick-Evaluations_-of-Cultural-Representation-in-AI":{"slug":"References/The-Case-for-_Thick-Evaluations_-of-Cultural-Representation-in-AI","filePath":"References/The Case for _Thick Evaluations_ of Cultural Representation in AI.md","title":"The Case for _Thick Evaluations_ of Cultural Representation in AI","links":["tags/research-image-generation","tags/research-evaluation","tags/research-culture","tags/zotero","tags/paper-notes","02-Document-Notes/Drafts/Culture-as-practice-drafting","References/Scaling-Laws-Do-Not-Scale"],"tags":["research-image-generation","research-evaluation","research-culture","zotero","paper-notes"],"content":"The Case for “Thick Evaluations” of Cultural Representation in AI\nRida Qadri, Mark Diaz, Ding Wang, Michael Madaio\n2025\nIn Zotero: qadriCaseThickEvaluations2025\nOnline: url\n\ntags: research-image-generation, research-evaluation, research-culture zotero paper-notes\nlinks: Culture as practice drafting\n\nNotes\nWhat is Thick Evaluation\nDefined as “more granular, situated, and discursive” and “steeped in [a community’s own understanding] of representation”\nTaken from Gilbert Ryle/Clifford Geertz’s thick vs thin dichotomy (Ryle 1968, The thinking of thoughts… (in my Zotero))\nConflict between SocSci and CS\n\nCS cultural eval: culture is measurable &amp; objective\nSocial sciences: culture is situated and subjective\n\nSituated: very context-dependent (are we talking about gender? are we talking about geography? are we talking about application use vs a dictionary?), as opposed to:\nAbstracted: doesn’t connect to/engage with the community of study. eg., skin tone from western racial categories.\n\n\n                  \n                  TIP\n                  \n                \n\nPrevailing style of CS (AI in particular) engagement with these topics is important but not complete\n\n\nHow do you do it?\nThey interviewed participants in South Asia re: culturally situated AI-generated images, finding:\n\nfine-grained multi-dimensional axes that individuals use\nempirical data on the categories they use\n\nInterview study w/ annotation task: the annotators themselves are asked to grade the images ostensibly targeting their culture\n\n\n                  \n                  Tip\n                  \n                \n\nThis work targets depth in cultural AI research, pushing the types of questions AI researchers already ask toward\n\n\nLimitations\n\nThicknesss is in conflict with generalization and scale (in other words, Scaling Laws Do Not Scale!)\n\nNo single correct way to represent the world breaks fundamental assumptions in benchmarking and eval [9,35,80 from the refs]\nGoes against positivist assumptions [22,50,58]\n\n\n\nRelation to our paper\n\nSituatedness and granularity are also important points in our work\nThey are focused on pursuing these desiderata toward depth of research in traditionally culture-first CS topics (representativeness of generated images)\nWe are interested in adding situated-in-culture and culturally-conditioned-granularity as considerations to guide research in non-culture-first research\nIn other words, we want to extend the breadth of cultural research by bringing these ideas and culturally diverse annotator sampling to non-culture evaluations like general usability of open-ended systems\n\nPerhaps another important theme here is that we’re looking at open-ended tasks rather than culturally-targeted tasks\nConnection to the how\n\nMetrics: secondary. The focus is qualitative and in-depth discussions.\nParticipants themselves were asked to produce the images they checked: this evaluation was directly grounded in user needs (with the one complication being they were explicitly instructed to generate for their culture)\nParticipants constructed their own rubrics: it’s difficult to prescribe the elements of a representation that users care about. They converged on :\n\nCorrectness\nMissingness: whether expected cultural elements are missing from the image\nSpecificity: “generic” and or “stereotypical” things are weighted\nCoherence: things like a muslim having alcohol or Chinese lanterns in Tamil festival\n\n\n\n![[IMG_The Case for Thick Evaluations of Cultural Representation in AI_039.jpeg]]"},"References/The-Ghost-in-the-Machine-has-an-American-accent_-Value-conflict-in-GPT-3.":{"slug":"References/The-Ghost-in-the-Machine-has-an-American-accent_-Value-conflict-in-GPT-3.","filePath":"References/The Ghost in the Machine has an American accent_ Value conflict in GPT-3..md","title":"The Ghost in the Machine has an American accent_ Value conflict in GPT-3.","links":["/","tags/research-culture","tags/research-lm","tags/zotero","tags/paper-notes","02-Document-Notes/Drafts/Culture-as-practice-drafting","01-Fleeting-Notes/Who-Cares-how-LMs-answer-surveys"],"tags":["research-culture","research-lm","zotero","paper-notes"],"content":"The Ghost in the Machine has an American accent: Value conflict in GPT-3.\nRebecca L Johnson, Giada Pistilli, Natalia Menédez-González, Leslye Denisse Dias Duran, Enrico Panai, Julija Kalpokiene, Donald Jay Bertulfo\nIn Zotero: johnsonGhostMachineHas\nOnline: url\n\ntags: research-culture research-lm zotero paper-notes\nlinks: Culture as practice drafting\n\nNotes\nAnother paper discussing the problem of value alignment in LMs.\nHere, they demonstrate how the values in an input text conflict with the output values in the generated text, when it conflicts with hegemonic Anglo-US values.\nHow do they assess values?\n\nMetaphors, such as “tall poppy syndrome”\nRelations, like nurse→woman, doctor→man\nDichotomous values like right/wrong good/bad\n\nThey feed in input texts from 10 countries and six languages. They ask the models to perform culturally neutral tasks, such as summarization. They use texts that have embedded values, such as activist or political texts.\nThe evaluation is entirely subjective and expensive. They generate the pseudo-opinions by asking the model to generate an output summary or a summary for a third grader! This is a great example of a reason for Who Cares how LMs answer surveys?: this is a real end-task that aligns with opinions of the fit population.\nResult examples\nGun control\nAustralian public opinion rapidly converged on a preference for strict gun control and buybacks after a single mass shooting. Meanwhile, in the US gun control is an extremely controversial issue due to different cultural attitudes.\nText about the Australian firearms act generated by GPT3 highlighted problems of the loss of liberties and freedom.\nde Beauvoir’s The Second Sex\nIn French, this was described as a “call to rape”\nLGBTI pride in Spain\nGPT-3 disagreed with a female Spanish minister’s text that LGBTI movement and feminism are aligned due to LGBTI “not focused on equality.” The authors point to higher US distrust toward the women’s movement as evidence for the driver of this behavior.\nLaicite\nThey feed a text about French secularism and GPT3 points out (rightfully) that it is an illiberal principle to restrict overt religious displays in public (based though it may be)\nMalcolm X\nApparently GPT3 repeatedly generated text saying “the democrats are the party of the KKK”..l.bruh\nMarriage sanctity\nIn the Philippine Constitution “marriage is sacred” (so divorce is illegal). In the US this passage means “gay marriage should be illegal.” Thus GPT3 parses the Philippine constitutional passage as meaning hte latter rather than the former.\nOther points\nEmbedding bias research relates\nThey talk about the extensive body of research in approaching text embedding values but move on to look at LLMs instead of behavioral settings."},"index":{"slug":"index","filePath":"index.md","title":"index","links":["How-To/How-To-Zettelkasten","How-To/Homebrew-sync-to-a-custom-URL-using-quartz","tags"],"tags":[],"content":"Michael’s (Public) Obsidian Library\nI am hosting this mainly for myself and also so that I can point links in it to people who ask me. Even if I sent you a link to this, don’t go poking around in here ok? ;)\nZettlekasten is a meme and I have never been able to successfully maintain it, even though I have read How To Zettelkasten.\nThis is a quartz-based alternative to Obsidian Sync which includes my own Homebrew sync to a custom URL using quartz\nTry browsing by tags."}}